#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Seeed Wiki ä¼˜åŒ–é—®ç­”ç³»ç»Ÿ
ä½¿ç”¨é¢„ä¿å­˜çš„ FAISS ç´¢å¼•å’Œ Ollama nomic-embed-text æ¨¡å‹
"""

import json
import os
import pickle
import numpy as np
import faiss
import ollama
import time
import re
import sys
import readline  # æ·»åŠ  readline æ”¯æŒï¼Œæä¾›æ›´å¥½çš„è¾“å…¥ä½“éªŒ
import hashlib
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
import threading


class OptimizedQASystem:
    def __init__(self):
        self.faiss_index = None
        self.faiss_metadata = None
        self.wiki_pages = []
        self.embedding_model = "nomic-embed-text"
        
        # æ€§èƒ½ä¼˜åŒ–ç›¸å…³
        self.embedding_cache = {}  # embedding ç¼“å­˜
        self.answer_cache = {}     # å›ç­”ç¼“å­˜
        self.cache_lock = threading.Lock()  # ç¼“å­˜é”
        self.executor = ThreadPoolExecutor(max_workers=2)  # çº¿ç¨‹æ± 
        
        # æµå¼æ˜¾ç¤ºç›¸å…³
        self.streaming_enabled = True  # æ˜¯å¦å¯ç”¨æµå¼æ˜¾ç¤º
        self.typing_speed = 0.03  # æ‰“å­—é€Ÿåº¦ï¼ˆç§’/å­—ç¬¦ï¼‰
        

        
        # è®¾ç½® readline é…ç½®
        self.setup_readline()
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶
        self.check_data_files()
        
        # æ£€æŸ¥ Ollama æœåŠ¡
        self.check_ollama_service()
        

        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        self.initialize_system()
    
    def setup_readline(self):
        """è®¾ç½® readline é…ç½®ï¼Œæä¾›æ›´å¥½çš„è¾“å…¥ä½“éªŒ"""
        try:
            # è®¾ç½®å†å²æ–‡ä»¶
            histfile = os.path.join(os.path.expanduser("~"), ".seeed_qa_history")
            readline.read_history_file(histfile)
            readline.set_history_length(1000)
            
            # è®¾ç½®è‡ªåŠ¨è¡¥å…¨
            readline.parse_and_bind('tab: complete')
            
            # è®¾ç½®è¾“å…¥æç¤ºç¬¦æ ·å¼
            readline.parse_and_bind('set editing-mode emacs')
            
        except Exception as e:
            print(f"âš ï¸  readline è®¾ç½®å¤±è´¥: {str(e)}")
            print("ğŸ’¡ è¾“å…¥ä½“éªŒå¯èƒ½å—é™ï¼Œä½†åŸºæœ¬åŠŸèƒ½æ­£å¸¸")
    
    def safe_input(self, prompt):
        """å®‰å…¨çš„è¾“å…¥å‡½æ•°ï¼Œæä¾›æ›´å¥½çš„é”™è¯¯å¤„ç†"""
        try:
            # å°è¯•ä½¿ç”¨ readline è¾“å…¥
            user_input = input(prompt)
            return user_input.strip()
        except (EOFError, KeyboardInterrupt):
            print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
            sys.exit(0)
        except Exception as e:
            print(f"\nâŒ è¾“å…¥é”™è¯¯: {str(e)}")
            return ""
    
    def save_history(self):
        """ä¿å­˜è¾“å…¥å†å²"""
        try:
            histfile = os.path.join(os.path.expanduser("~"), ".seeed_qa_history")
            readline.write_history_file(histfile)
        except Exception:
            pass  # å¿½ç•¥å†å²ä¿å­˜é”™è¯¯
    
    def check_data_files(self):
        """æ£€æŸ¥å¿…è¦çš„æ•°æ®æ–‡ä»¶"""
        required_files = [
            "./data_base/faiss_index.bin",
            "./data_base/faiss_metadata.pkl",
            "./data_base/seeed_wiki_embeddings_db.json"
        ]
        
        missing_files = []
        for file in required_files:
            if not os.path.exists(file):
                missing_files.append(file)
        
        if missing_files:
            print("âŒ ç¼ºå°‘å¿…è¦çš„æ•°æ®æ–‡ä»¶:")
            for file in missing_files:
                print(f"   - {file}")
            print("\nğŸ’¡ è¯·å…ˆè¿è¡Œçˆ¬è™«è„šæœ¬è·å–æ•°æ®:")
            print("   python scrape_with_embeddings.py")
            raise FileNotFoundError(f"ç¼ºå°‘æ•°æ®æ–‡ä»¶: {', '.join(missing_files)}")
        
        print("âœ… æ‰€æœ‰å¿…è¦çš„æ•°æ®æ–‡ä»¶å·²æ‰¾åˆ°")
    
    def check_ollama_service(self):
        """æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€"""
        try:
            models = ollama.list()
            print(f"âœ… Ollama æœåŠ¡æ­£å¸¸ï¼Œå¯ç”¨æ¨¡å‹: {len(models['models'])} ä¸ª")
            
            model_names = [model['name'] for model in models['models']]
            print(model_names)
            if 'nomic-embed-text:latest' not in model_names:
                print("âš ï¸  æœªæ‰¾åˆ° nomic-embed-text æ¨¡å‹ï¼Œæ­£åœ¨å®‰è£…...")
                ollama.pull('nomic-embed-text')
                print("âœ… nomic-embed-text æ¨¡å‹å®‰è£…å®Œæˆ")
            else:
                print("âœ… nomic-embed-text æ¨¡å‹å·²å®‰è£…")
                
        except Exception as e:
            print(f"âŒ Ollama æœåŠ¡æ£€æŸ¥å¤±è´¥: {str(e)}")
            raise
    
    def initialize_system(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ä¼˜åŒ–é—®ç­”ç³»ç»Ÿ...")
        
        try:
            # åŠ è½½ FAISS ç´¢å¼•
            print("ğŸ” åŠ è½½ FAISS ç´¢å¼•...")
            if not os.path.exists("./data_base/faiss_index.bin"):
                raise FileNotFoundError("FAISS ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨")
                
            self.faiss_index = faiss.read_index("./data_base/faiss_index.bin")
            if self.faiss_index is None:
                raise Exception("FAISS ç´¢å¼•åŠ è½½å¤±è´¥")
                
            print(f"âœ… FAISS ç´¢å¼•åŠ è½½å®Œæˆ: {self.faiss_index.ntotal} ä¸ªå‘é‡")
            print(f"   ç´¢å¼•ç»´åº¦: {self.faiss_index.d}")
            print(f"   ç´¢å¼•ç±»å‹: {type(self.faiss_index).__name__}")
            
            # åŠ è½½å‘é‡å…ƒæ•°æ®
            print("ğŸ“Š åŠ è½½å‘é‡å…ƒæ•°æ®...")
            if not os.path.exists("./data_base/faiss_metadata.pkl"):
                raise FileNotFoundError("å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
                
            with open("./data_base/faiss_metadata.pkl", 'rb') as f:
                self.faiss_metadata = pickle.load(f)
            
            if not self.faiss_metadata or len(self.faiss_metadata) == 0:
                raise Exception("å…ƒæ•°æ®ä¸ºç©º")
                
            print(f"âœ… å…ƒæ•°æ®åŠ è½½å®Œæˆ: {len(self.faiss_metadata)} æ¡è®°å½•")
            
            # æ£€æŸ¥ç´¢å¼•å’Œå…ƒæ•°æ®çš„ä¸€è‡´æ€§
            if self.faiss_index.ntotal != len(self.faiss_metadata):
                print(f"âš ï¸  è­¦å‘Š: ç´¢å¼•å‘é‡æ•°({self.faiss_index.ntotal})ä¸å…ƒæ•°æ®è®°å½•æ•°({len(self.faiss_metadata)})ä¸åŒ¹é…")
            
            # åŠ è½½ Wiki é¡µé¢æ•°æ®
            print("ğŸ“š åŠ è½½ Wiki é¡µé¢æ•°æ®...")
            if not os.path.exists("./data_base/seeed_wiki_embeddings_db.json"):
                raise FileNotFoundError("Wiki é¡µé¢æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
                
            with open("./data_base/seeed_wiki_embeddings_db.json", 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.wiki_pages = data['pages']
                self.metadata = data['metadata']
            
            if not self.wiki_pages or len(self.wiki_pages) == 0:
                raise Exception("Wiki é¡µé¢æ•°æ®ä¸ºç©º")
                
            print(f"âœ… é¡µé¢æ•°æ®åŠ è½½å®Œæˆ: {len(self.wiki_pages)} ä¸ªé¡µé¢")
            
            # æ£€æŸ¥é¡µé¢æ•°æ®å’Œå…ƒæ•°æ®çš„ä¸€è‡´æ€§
            if len(self.wiki_pages) != len(self.faiss_metadata):
                print(f"âš ï¸  è­¦å‘Š: é¡µé¢æ•°æ®æ•°({len(self.wiki_pages)})ä¸å…ƒæ•°æ®è®°å½•æ•°({len(self.faiss_metadata)})ä¸åŒ¹é…")
            
            # æµ‹è¯• Embedding æ¨¡å‹
            print("ğŸ¤– æµ‹è¯• Embedding æ¨¡å‹...")
            test_embedding = self.generate_embedding("test")
            if test_embedding is None:
                raise Exception("Embedding ç”Ÿæˆå¤±è´¥")
                
            # æ£€æŸ¥ embedding ç»´åº¦æ˜¯å¦ä¸ç´¢å¼•åŒ¹é…
            if test_embedding.shape[0] != self.faiss_index.d:
                raise Exception(f"Embedding ç»´åº¦({test_embedding.shape[0]})ä¸ç´¢å¼•ç»´åº¦({self.faiss_index.d})ä¸åŒ¹é…")
                
            print(f"âœ… Embedding æ¨¡å‹æµ‹è¯•æˆåŠŸ: {len(test_embedding)} ç»´")
            
            print("ğŸ‰ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
            self.show_system_info()
            
            # åŠ è½½ç¼“å­˜
            self.load_cache()
            
        except Exception as e:
            print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            raise
    
    def show_system_info(self):
        """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯"""
        print(f"\nğŸ“Š ç³»ç»Ÿä¿¡æ¯:")
        print(f"   æ€»é¡µé¢æ•°: {len(self.wiki_pages)}")
        print(f"   æ€»å‘é‡æ•°: {self.faiss_index.ntotal}")
        print(f"   å‘é‡ç»´åº¦: {self.metadata['vector_dimension']}")
        print(f"   å†…å®¹ç±»å‹: {self.metadata['content_type']}")
        print(f"   Embedding æ¨¡å‹: {self.metadata['embedding_model']}")
        print(f"   ç´¢å¼•ç±»å‹: {self.metadata['index_type']}")
        print(f"   çˆ¬å–æ—¶é—´: {self.metadata['crawl_time']}")
        print(f"   ç¼“å­˜çŠ¶æ€: Embeddingç¼“å­˜ {len(self.embedding_cache)} é¡¹ï¼Œå›ç­”ç¼“å­˜å·²ç¦ç”¨")
        print(f"   æµå¼æ˜¾ç¤º: {'å¯ç”¨' if self.streaming_enabled else 'ç¦ç”¨'}")
        print(f"   æ‰“å­—é€Ÿåº¦: {self.typing_speed:.3f} ç§’/å­—ç¬¦")

    
    def load_cache(self):
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        try:
            cache_file = "./data_base/cache_data.pkl"
            if os.path.exists(cache_file):
                with open(cache_file, 'rb') as f:
                    cache_data = pickle.load(f)
                    self.embedding_cache = cache_data.get('embedding_cache', {})
                    self.answer_cache = cache_data.get('answer_cache', {})
                print(f"âœ… ç¼“å­˜åŠ è½½å®Œæˆ: Embedding {len(self.embedding_cache)} é¡¹ï¼Œå›ç­” {len(self.answer_cache)} é¡¹")
        except Exception as e:
            print(f"âš ï¸  ç¼“å­˜åŠ è½½å¤±è´¥: {str(e)}")
    
    def save_cache(self):
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        try:
            cache_file = "./data_base/cache_data.pkl"
            cache_data = {
                'embedding_cache': self.embedding_cache,
                'answer_cache': self.answer_cache
            }
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            print(f"âœ… ç¼“å­˜ä¿å­˜å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  ç¼“å­˜ä¿å­˜å¤±è´¥: {str(e)}")
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.cache_lock:
            self.embedding_cache.clear()
            self.answer_cache.clear()
        print("âœ… ç¼“å­˜å·²æ¸…ç©º")
    
    def typewriter_effect(self, text, speed=None):
        """æ‰“å­—æœºæ•ˆæœæ˜¾ç¤ºæ–‡æœ¬"""
        if speed is None:
            speed = self.typing_speed
        
        for char in text:
            print(char, end='', flush=True)
            time.sleep(speed)
        print()  # æ¢è¡Œ
    
    def stream_response(self, response_generator):
        """æµå¼æ˜¾ç¤ºå›ç­”"""
        full_answer = ""
        print("ğŸ’¬ å›ç­”: ", end='', flush=True)
        
        try:
            for chunk in response_generator:
                if 'message' in chunk and 'content' in chunk['message']:
                    content = chunk['message']['content']
                    if content:
                        full_answer += content
                        print(content, end='', flush=True)
                        time.sleep(self.typing_speed)
        except Exception as e:
            print(f"\nâš ï¸  æµå¼æ˜¾ç¤ºé”™è¯¯: {str(e)}")
        
        print()  # æ¢è¡Œ
        return full_answer
    

    

    

    

    
    def generate_embedding(self, text):
        """ä½¿ç”¨ Ollama ç”Ÿæˆæ–‡æœ¬çš„ embedding å‘é‡ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰"""
        if not text or not text.strip():
            print("âŒ è¾“å…¥æ–‡æœ¬ä¸ºç©º")
            return None
            
        # ç”Ÿæˆæ–‡æœ¬çš„å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜é”®
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        
        # æ£€æŸ¥ç¼“å­˜
        with self.cache_lock:
            if text_hash in self.embedding_cache:
                cached_embedding = self.embedding_cache[text_hash]
                if cached_embedding is not None and isinstance(cached_embedding, np.ndarray):
                    return cached_embedding
                else:
                    # æ¸…ç†æ— æ•ˆçš„ç¼“å­˜é¡¹
                    del self.embedding_cache[text_hash]
        
        try:
            print(f"ğŸ” æ­£åœ¨ç”Ÿæˆæ–‡æœ¬çš„ embedding: '{text[:50]}...'")
            response = ollama.embeddings(model=self.embedding_model, prompt=text)
            
            if "embedding" not in response:
                print(f"âŒ Ollama å“åº”æ ¼å¼é”™è¯¯: {response}")
                return None
                
            embedding = response["embedding"]
            
            if not embedding or len(embedding) == 0:
                print("âŒ ç”Ÿæˆçš„ embedding ä¸ºç©º")
                return None
            
            # è½¬æ¢ä¸º numpy æ•°ç»„
            embedding = np.array(embedding, dtype=np.float32)
            
            # æ£€æŸ¥æ•°ç»„æ˜¯å¦æœ‰æ•ˆ
            if np.isnan(embedding).any() or np.isinf(embedding).any():
                print("âŒ embedding åŒ…å« NaN æˆ– Inf å€¼")
                return None
            
            # å½’ä¸€åŒ–
            norm = np.linalg.norm(embedding)
            if norm == 0:
                print("âŒ embedding å‘é‡çš„èŒƒæ•°ä¸º 0")
                return None
                
            embedding = embedding / norm
            
            print(f"âœ… embedding ç”ŸæˆæˆåŠŸ: ç»´åº¦ {len(embedding)}, èŒƒæ•° {np.linalg.norm(embedding):.6f}")
            
            # ç¼“å­˜ç»“æœ
            with self.cache_lock:
                self.embedding_cache[text_hash] = embedding
                # é™åˆ¶ç¼“å­˜å¤§å°ï¼Œé¿å…å†…å­˜æº¢å‡º
                if len(self.embedding_cache) > 1000:
                    # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
                    oldest_key = next(iter(self.embedding_cache))
                    del self.embedding_cache[oldest_key]
            
            return embedding
            
        except Exception as e:
            print(f"âŒ Embedding ç”Ÿæˆå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
    
    def search_knowledge_base(self, query, top_k=20):
        """åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³å†…å®¹ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # æ£€æŸ¥ FAISS ç´¢å¼•æ˜¯å¦æ­£ç¡®åŠ è½½
            if self.faiss_index is None:
                print("âŒ FAISS ç´¢å¼•æœªåŠ è½½")
                return []
            
            # æ£€æŸ¥å…ƒæ•°æ®æ˜¯å¦æ­£ç¡®åŠ è½½
            if self.faiss_metadata is None or len(self.faiss_metadata) == 0:
                print("âŒ å…ƒæ•°æ®æœªåŠ è½½")
                return []
            
            # ç”ŸæˆæŸ¥è¯¢çš„ embedding
            query_embedding = self.generate_embedding(query)
            if query_embedding is None:
                print("âŒ æ— æ³•ç”ŸæˆæŸ¥è¯¢çš„ embedding")
                return []
            
            # ç¡®ä¿ embedding æ˜¯æ­£ç¡®çš„ numpy æ•°ç»„
            if not isinstance(query_embedding, np.ndarray):
                print(f"âŒ embedding ç±»å‹é”™è¯¯: {type(query_embedding)}")
                return []
            
            # æ£€æŸ¥å‘é‡ç»´åº¦
            expected_dim = self.faiss_index.d
            if query_embedding.shape[0] != expected_dim:
                print(f"âŒ å‘é‡ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {expected_dim}, å®é™… {query_embedding.shape[0]}")
                return []
            
            # é‡å¡‘ä¸ºæ­£ç¡®çš„å½¢çŠ¶
            query_embedding = query_embedding.reshape(1, -1).astype(np.float32)
            
            # æ‰§è¡Œ FAISS æœç´¢
            scores, indices = self.faiss_index.search(query_embedding, top_k)
            
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.faiss_metadata):
                    metadata = self.faiss_metadata[idx]
                    page_data = self.wiki_pages[idx]
                    
                    results.append({
                        'rank': i + 1,
                        'score': float(score),
                        'title': metadata['title'],
                        'url': metadata['url'],
                        'content': page_data['content'],
                        'content_length': metadata['content_length'],
                        'timestamp': metadata['timestamp']
                    })
            
            return results
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
    
    def ask_question(self, question):
        """æé—®å¹¶è·å–å›ç­”ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        print(f"\nğŸ¤” ç”¨æˆ·é—®é¢˜: {question}")
        
        # ç¦ç”¨å›ç­”ç¼“å­˜ï¼Œæ¯æ¬¡éƒ½å®æ—¶ç”Ÿæˆ
        # question_hash = hashlib.md5(question.encode('utf-8')).hexdigest()
        # with self.cache_lock:
        #     if question_hash in self.answer_cache:
        #         print("âš¡ ä½¿ç”¨ç¼“å­˜å›ç­”")
        #         print(f"\nğŸ’¬ å›ç­”:")
        #         print(f"{self.answer_cache[question_hash]}")
        #         return
        
        # æœç´¢çŸ¥è¯†åº“
        print("ğŸ” æ­£åœ¨æœç´¢çŸ¥è¯†åº“...")
        start_time = time.time()
        search_results = self.search_knowledge_base(question, top_k=20)  # å‡å°‘æœç´¢æ•°é‡
        search_time = time.time() - start_time
        
        if not search_results:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
            return
        
        print(f"âœ… æœç´¢å®Œæˆï¼Œè€—æ—¶: {search_time:.3f} ç§’")
        print(f"ğŸ“Š æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        # æ™ºèƒ½é€‰æ‹©æœ€ç›¸å…³çš„ç»“æœï¼ˆå‡å°‘åˆ°5ä¸ªï¼‰
        best_results = self.select_best_results(question, search_results, max_results=5)
        
        # æ˜¾ç¤ºæœç´¢ç»“æœ
        print(f"ğŸ” æœç´¢ç»“æœé¢„è§ˆ:")
        for i, result in enumerate(best_results[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"  {i+1}. {result['title']}")
            print(f"     ç›¸å…³åº¦: {result['score']:.3f}")
            print(f"     URL: {result['url']}")
            print()
        
        # ç”Ÿæˆå›ç­”
        print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")
        answer_start_time = time.time()
        answer = self.generate_answer(question, best_results)
        answer_time = time.time() - answer_start_time
        
        # æ˜¾ç¤ºå›ç­”
        print(f"\nğŸ’¬ å›ç­”:")
        print(f"{answer}")
        print(f"\nâ±ï¸  å›ç­”ç”Ÿæˆè€—æ—¶: {answer_time:.3f} ç§’")
        

        
        # ç¦ç”¨å›ç­”ç¼“å­˜ï¼Œä¸ä¿å­˜ç”Ÿæˆçš„å›ç­”
        # with self.cache_lock:
        #     self.answer_cache[question_hash] = answer
        #     # é™åˆ¶ç¼“å­˜å¤§å°
        #     if len(self.answer_cache) > 100:
        #         oldest_key = next(iter(self.answer_cache))
        #         del self.answer_cache[oldest_key]
    
    def select_best_results(self, question, search_results, max_results=10):
        """æ™ºèƒ½é€‰æ‹©æœ€ç›¸å…³çš„ç»“æœ"""
        if not search_results:
            return []
        
        # æå–é—®é¢˜ä¸­çš„å…³é”®è¯
        question_lower = question.lower()
        keywords = []
        
        # ä¸­æ–‡å…³é”®è¯
        chinese_keywords = ['çŸ½é€’', 'ç§‘æŠ€', 'å…¬å¸', 'ä»‹ç»', 'ç®€ä»‹', 'å…³äº', 'ä»€ä¹ˆæ˜¯', 'å¦‚ä½•', 'æ€ä¹ˆ']
        for keyword in chinese_keywords:
            if keyword in question_lower:
                keywords.append(keyword)
        
        # è‹±æ–‡å…³é”®è¯
        english_keywords = ['seeed', 'studio', 'company', 'introduction', 'about', 'what', 'how']
        for keyword in english_keywords:
            if keyword in question_lower:
                keywords.append(keyword)
        
        # è®¡ç®—æ¯ä¸ªç»“æœçš„ç›¸å…³æ€§åˆ†æ•°
        scored_results = []
        for result in search_results:
            score = result['score']
            title = result['title'].lower()
            content = result['content'].lower()
            
            # å…³é”®è¯åŒ¹é…åŠ åˆ†
            keyword_bonus = 0
            for keyword in keywords:
                if keyword in title:
                    keyword_bonus += 0.1
                if keyword in content:
                    keyword_bonus += 0.05
            
            # æ ‡é¢˜åŒ¹é…åŠ åˆ†
            title_bonus = 0
            if any(keyword in title for keyword in keywords):
                title_bonus += 0.05
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            final_score = score + keyword_bonus + title_bonus
            scored_results.append((result, final_score))
        
        # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        scored_results.sort(key=lambda x: x[1], reverse=True)
        
        # è¿”å›å‰Nä¸ªæœ€ä½³ç»“æœ
        best_results = [result for result, score in scored_results[:max_results]]
        
        print(f"ğŸ” æ™ºèƒ½é€‰æ‹©ç»“æœ:")
        print(f"   å…³é”®è¯: {keywords}")
        print(f"   é€‰æ‹©ç»“æœæ•°: {len(best_results)}")
        
        return best_results

    def detect_language(self, text):
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€ - æ”¹è¿›ç‰ˆæœ¬"""
        # æ£€æµ‹ä¸­æ–‡å­—ç¬¦
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        english_chars = re.findall(r'[a-zA-Z]', text)
        
        # è®¡ç®—ä¸­è‹±æ–‡æ¯”ä¾‹
        total_chars = len(text.replace(' ', '').replace('\n', ''))
        if total_chars == 0:
            return 'en'  # é»˜è®¤ä¸ºè‹±æ–‡
            
        chinese_ratio = len(chinese_chars) / total_chars
        english_ratio = len(english_chars) / total_chars
        
        # å¦‚æœä¸­æ–‡å­—ç¬¦è¶…è¿‡10%ï¼Œæˆ–è€…ä¸­æ–‡æ¯”ä¾‹å¤§äºè‹±æ–‡æ¯”ä¾‹ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸­æ–‡
        if chinese_ratio > 0.1 or (chinese_ratio > 0 and chinese_ratio > english_ratio):
            return 'zh'
        elif english_ratio > 0.5:
            return 'en'
        else:
            # å¦‚æœéƒ½ä¸æ˜æ˜¾ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
            chinese_punctuation = re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘]', text)
            if chinese_punctuation:
                return 'zh'
            return 'en'
    
    def generate_answer(self, question, search_results):
        """åŸºäºæœç´¢ç»“æœç”Ÿæˆå›ç­” - ä¼˜åŒ–ç‰ˆæœ¬"""
        if not search_results:
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        # æ£€æµ‹ç”¨æˆ·é—®é¢˜çš„è¯­è¨€
        user_language = self.detect_language(question)
        print(f"ğŸ” æ£€æµ‹åˆ°é—®é¢˜è¯­è¨€: {user_language}")
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä¼˜åŒ–ï¼šé™åˆ¶é•¿åº¦ï¼‰
        context_parts = []
        total_length = 0
        max_context_length = 3000  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
        
        for result in search_results:
            title = result['title']
            content = result['content']
            # ç§»é™¤ [Introduction] å‰ç¼€ï¼Œæ¸…ç†å†…å®¹
            if content.startswith('[Introduction] '):
                content = content[16:]
            
            # æˆªæ–­è¿‡é•¿çš„å†…å®¹
            if len(content) > 500:
                content = content[:500] + "..."
            
            context_part = f"æ–‡æ¡£æ ‡é¢˜: {title}\nå†…å®¹: {content}"
            
            # æ£€æŸ¥æ˜¯å¦ä¼šè¶…å‡ºé•¿åº¦é™åˆ¶
            if total_length + len(context_part) > max_context_length:
                break
                
            context_parts.append(context_part)
            total_length += len(context_part)
        
        context = "\n\n".join(context_parts)
        
        # æ ¹æ®ç”¨æˆ·è¯­è¨€é€‰æ‹© promptï¼Œå¼ºåˆ¶æŒ‡å®šè¾“å‡ºè¯­è¨€
        if user_language == 'zh':
            prompt = f"""è¯·åŸºäºä»¥ä¸‹èµ„æ–™ï¼Œç”¨è‡ªç„¶ã€è¿è´¯çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

é‡è¦è¦æ±‚ï¼š
1. å¿…é¡»ç”¨ä¸­æ–‡å›ç­”ï¼Œä¸èƒ½ä½¿ç”¨è‹±æ–‡
2. ä»‹ç»äº§å“æ—¶è¯´"æˆ‘ä»¬çš„xxxäº§å“..."
3. ä¸¥æ ¼åŸºäºæä¾›çš„èµ„æ–™å›ç­”ï¼Œç»å¯¹ä¸èƒ½ç¼–é€ æˆ–è™šæ„ä»»ä½•ä¿¡æ¯
4. å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰æŸä¸ªå…·ä½“ä¿¡æ¯ï¼ˆå¦‚æˆç«‹æ—¶é—´ã€å…·ä½“æ•°æ®ç­‰ï¼‰ï¼Œç»å¯¹ä¸è¦ç¼–é€ ï¼Œåº”è¯¥è¯´"èµ„æ–™ä¸­æœªæåŠ"
5. è¯­è¨€è¦æµç•…è‡ªç„¶ï¼Œä½“ç°ä¸“ä¸šä¸”äº²åˆ‡çš„ä¼ä¸šå½¢è±¡
6. ä¸è¦åˆ†ç‚¹åˆ†æ®µï¼Œç”¨ä¸€æ®µè¯æ¦‚æ‹¬æ‰€æœ‰ç›¸å…³ä¿¡æ¯
7. ä¸è¦é‡å¤è¯´"æˆ‘ä»¬æ˜¯Seeed Studioçš„AIåŠ©æ‰‹"è¿™æ ·çš„èº«ä»½ä»‹ç»

ç›¸å…³èµ„æ–™:
{context}

ç”¨æˆ·é—®é¢˜: {question}

è¯·ç”¨ä¸€æ®µè¿è´¯çš„ä¸­æ–‡å›ç­”ï¼Œä½¿ç”¨"æˆ‘ä»¬"çš„è¡¨è¾¾æ–¹å¼ï¼Œä¸¥æ ¼åŸºäºèµ„æ–™å†…å®¹ï¼Œä¸ç¼–é€ ä»»ä½•ä¿¡æ¯:"""
        else:
            prompt = f"""Please answer the user's question in natural, coherent English based on the following materials.

Important requirements:
1. Must answer in English, not in Chinese
2. Answer as Seeed Studio's representative, using "we" expressions
3. When introducing products, say "our xxx product..."
4. Strictly base your answer on the provided materials, absolutely do not fabricate or invent any information
5. If specific information (like founding date, specific data, etc.) is not mentioned in the materials, absolutely do not make it up, say "not mentioned in the materials"
6. Make the language fluent and natural, reflecting a professional yet friendly corporate image
7. Don't use bullet points or separate paragraphs, summarize all relevant information in one coherent paragraph
8. Don't repeat identity introductions like "We are Seeed Studio's AI assistant"

Materials:
{context}

User Question: {question}

Please answer using "we" expressions in one coherent English paragraph, strictly based on the materials without fabricating any information:"""
        
        # ä½¿ç”¨ Ollama ç”Ÿæˆè‡ªç„¶è¯­è¨€å›ç­”ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
        try:
            # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´å¿«çš„æ¨¡å‹å’Œæ›´ç®€æ´çš„prompt
            system_prompt = f'ç”¨{user_language}å›ç­”ï¼ŒåŸºäºèµ„æ–™ï¼Œä¸ç¼–é€ ä¿¡æ¯ï¼Œä¸è¦é‡å¤èº«ä»½ä»‹ç»ã€‚'
            
            if self.streaming_enabled:
                # æµå¼ç”Ÿæˆå›ç­”
                response_generator = ollama.chat(
                    model='qwen2.5:3b', 
                    messages=[
                        {'role': 'system', 'content': system_prompt},
                        {'role': 'user', 'content': prompt}
                    ],
                    options={
                        'temperature': 0.7,  # é™ä½éšæœºæ€§ï¼Œæé«˜ä¸€è‡´æ€§
                        'top_p': 0.9,       # é™åˆ¶è¯æ±‡é€‰æ‹©èŒƒå›´
                        'num_predict': 300,  # é™åˆ¶ç”Ÿæˆé•¿åº¦
                    },
                    stream=True  # å¯ç”¨æµå¼è¾“å‡º
                )
                
                answer = self.stream_response(response_generator)
            else:
                # éæµå¼ç”Ÿæˆå›ç­”
                response = ollama.chat(
                    model='qwen2.5:3b', 
                    messages=[
                        {'role': 'system', 'content': system_prompt},
                        {'role': 'user', 'content': prompt}
                    ],
                    options={
                        'temperature': 0.7,
                        'top_p': 0.9,
                        'num_predict': 300,
                    }
                )
                
                answer = response['message']['content'].strip()
                # ä½¿ç”¨æ‰“å­—æœºæ•ˆæœæ˜¾ç¤º
                print("ğŸ’¬ å›ç­”: ", end='', flush=True)
                self.typewriter_effect(answer)
            
            # éªŒè¯å›ç­”è¯­è¨€
            answer_language = self.detect_language(answer)
            if answer_language != user_language:
                print(f"âš ï¸  AIå›ç­”è¯­è¨€ä¸åŒ¹é…ï¼ŒæœŸæœ›{user_language}ï¼Œå®é™…{answer_language}")
                answer = self.generate_manual_answer(question, search_results, user_language)
            
            # å¦‚æœå›ç­”å¤ªçŸ­ï¼Œæ·»åŠ ä¸€äº›è¡¥å……ä¿¡æ¯
            if len(answer) < 50:
                answer = self.generate_manual_answer(question, search_results, user_language)
            
            return answer
            
        except Exception as e:
            print(f"âš ï¸  AI ç”Ÿæˆå›ç­”å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {str(e)}")
            return self.generate_manual_answer(question, search_results, user_language)
    
    def generate_manual_answer(self, question, search_results, language='en'):
        """æ‰‹åŠ¨ç”Ÿæˆå›ç­”ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        # æŒ‰ç›¸å…³åº¦æ’åº
        sorted_results = sorted(search_results, key=lambda x: x['score'], reverse=True)
        
        # æ ¹æ®é—®é¢˜ç±»å‹å’Œè¯­è¨€ç”Ÿæˆä¸åŒçš„å›ç­”
        question_lower = question.lower()
        
        if language == 'zh':
            # ä¸­æ–‡å›ç­”
            if "xiao" in question_lower or "å°" in question_lower:
                answer = "XIAOç³»åˆ—æ˜¯çŸ½é€’ç§‘æŠ€æ¨å‡ºçš„å¾®å‹å¼€å‘æ¿äº§å“çº¿ï¼Œè¿™äº›å¼€å‘æ¿è™½ç„¶ä½“ç§¯å°å·§ï¼Œä½†åŠŸèƒ½å´éå¸¸å¼ºå¤§ã€‚æˆ‘ä»¬é‡‡ç”¨äº†æ ‡å‡†åŒ–çš„è®¾è®¡ç†å¿µï¼Œå…·æœ‰å‡ºè‰²çš„å…¼å®¹æ€§å’Œæ‰©å±•æ€§ï¼Œç‰¹åˆ«é€‚åˆå„ç§åµŒå…¥å¼é¡¹ç›®ã€åŸå‹å¼€å‘å’Œåˆ›å®¢é¡¹ç›®ã€‚XIAOç³»åˆ—äº§å“ä¸ä»…æ”¯æŒArduinoç”Ÿæ€ç³»ç»Ÿï¼Œè¿˜é›†æˆäº†Groveè¿æ¥å™¨ï¼Œè®©æ‚¨å¯ä»¥è½»æ¾è¿æ¥å„ç§ä¼ æ„Ÿå™¨å’Œæ¨¡å—ï¼Œå¤§å¤§ç®€åŒ–äº†ç¡¬ä»¶å¼€å‘çš„å¤æ‚åº¦ã€‚"
            elif "grove" in question_lower:
                answer = "Groveä¼ æ„Ÿå™¨æ¨¡å—ç³»ç»Ÿæ˜¯çŸ½é€’ç§‘æŠ€å¼€å‘çš„ä¸€å¥—æ ‡å‡†åŒ–çš„ç¡¬ä»¶è¿æ¥è§£å†³æ–¹æ¡ˆï¼Œå®ƒå½»åº•æ”¹å˜äº†ä¼ ç»Ÿç¡¬ä»¶å¼€å‘çš„å¤æ‚æµç¨‹ã€‚é€šè¿‡ç»Ÿä¸€çš„è¿æ¥æ¥å£å’Œæ ‡å‡†åŒ–çš„æ¨¡å—è®¾è®¡ï¼ŒGroveç³»ç»Ÿè®©æ‚¨å¯ä»¥åƒæ­ç§¯æœ¨ä¸€æ ·è½»æ¾åœ°å°†å„ç§ä¼ æ„Ÿå™¨ã€æ‰§è¡Œå™¨å’Œé€šä¿¡æ¨¡å—è¿æ¥åˆ°å¼€å‘æ¿ä¸Šã€‚è¿™ç§è®¾è®¡ä¸ä»…å¤§å¤§é™ä½äº†ç¡¬ä»¶å¼€å‘çš„å…¥é—¨é—¨æ§›ï¼Œè¿˜æé«˜äº†é¡¹ç›®çš„å¯é æ€§å’Œå¯ç»´æŠ¤æ€§ï¼Œç‰¹åˆ«é€‚åˆåˆå­¦è€…å’Œå¿«é€ŸåŸå‹å¼€å‘ã€‚"
            elif "sensecap" in question_lower:
                answer = "SenseCAPæ˜¯çŸ½é€’ç§‘æŠ€ä¸“é—¨ä¸ºç¯å¢ƒç›‘æµ‹å’Œç‰©è”ç½‘åº”ç”¨æ‰“é€ çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆï¼Œå®ƒé›†æˆäº†é«˜ç²¾åº¦çš„ä¼ æ„Ÿå™¨æŠ€æœ¯ã€å…ˆè¿›çš„æ•°æ®é‡‡é›†ç³»ç»Ÿå’Œå¼ºå¤§çš„äº‘ç«¯ç®¡ç†å¹³å°ã€‚è¿™å¥—ç³»ç»Ÿèƒ½å¤Ÿå®æ—¶ç›‘æµ‹å„ç§ç¯å¢ƒå‚æ•°ï¼Œå¦‚æ¸©åº¦ã€æ¹¿åº¦ã€ç©ºæ°”è´¨é‡ã€å…‰ç…§å¼ºåº¦ç­‰ï¼Œå¹¶å°†æ•°æ®é€šè¿‡æ— çº¿ç½‘ç»œä¼ è¾“åˆ°äº‘ç«¯è¿›è¡Œåˆ†æå’Œç®¡ç†ã€‚SenseCAPç‰¹åˆ«é€‚ç”¨äºæ™ºæ…§å†œä¸šã€ç¯å¢ƒç›‘æµ‹ã€å·¥ä¸šç‰©è”ç½‘ç­‰åœºæ™¯ï¼Œä¸ºç”¨æˆ·æä¾›å¯é ã€å‡†ç¡®çš„ç¯å¢ƒæ•°æ®æ”¯æŒã€‚"
            elif "edge computing" in question_lower or "è¾¹ç¼˜è®¡ç®—" in question_lower:
                answer = "è¾¹ç¼˜AIè®¡ç®—ä»£è¡¨äº†äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸€ä¸ªé‡è¦å‘å±•æ–¹å‘ï¼Œå®ƒå°†AIåº”ç”¨ä»äº‘ç«¯è¿ç§»åˆ°æœ¬åœ°è®¾å¤‡ä¸Šè¿è¡Œï¼Œå®ç°äº†æ›´å¿«çš„å“åº”é€Ÿåº¦å’Œæ›´å¥½çš„éšç§ä¿æŠ¤ã€‚é€šè¿‡reComputerç­‰åŸºäºNVIDIA Jetsonå¹³å°çš„è®¾å¤‡ï¼Œè¾¹ç¼˜è®¡ç®—èƒ½å¤Ÿåœ¨æœ¬åœ°å¤„ç†å„ç§AIä»»åŠ¡ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€å›¾åƒå¤„ç†ã€è‡ªç„¶è¯­è¨€ç†è§£ç­‰ï¼Œè€Œæ— éœ€ä¾èµ–ç½‘ç»œè¿æ¥ã€‚è¿™ç§æŠ€æœ¯ç‰¹åˆ«é€‚åˆéœ€è¦å®æ—¶å¤„ç†ã€ä½å»¶è¿Ÿå“åº”çš„åº”ç”¨åœºæ™¯ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰é¢†åŸŸã€‚"
            elif "recomputer" in question_lower:
                answer = "reComputerç³»åˆ—æ˜¯çŸ½é€’ç§‘æŠ€åŸºäºNVIDIA Jetsonå¹³å°å¼€å‘çš„é«˜æ€§èƒ½è¾¹ç¼˜è®¡ç®—è®¾å¤‡ï¼Œå®ƒä¸“é—¨ä¸ºAIå’Œè¾¹ç¼˜è®¡ç®—åº”ç”¨è€Œè®¾è®¡ã€‚è¿™äº›è®¾å¤‡é›†æˆäº†å¼ºå¤§çš„GPUè®¡ç®—èƒ½åŠ›ï¼Œæ”¯æŒå„ç§ä¸»æµçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼Œèƒ½å¤Ÿè¿è¡Œå¤æ‚çš„AIæ¨¡å‹å’Œç®—æ³•ã€‚reComputerç³»åˆ—äº§å“ä¸ä»…æ€§èƒ½å¼ºåŠ²ï¼Œè¿˜å…·æœ‰è‰¯å¥½çš„æ•£çƒ­è®¾è®¡å’Œä¸°å¯Œçš„æ¥å£é…ç½®ï¼Œç‰¹åˆ«é€‚åˆéœ€è¦æœ¬åœ°AIå¤„ç†èƒ½åŠ›çš„åº”ç”¨åœºæ™¯ï¼Œå¦‚æœºå™¨äººã€æ— äººæœºã€æ™ºèƒ½æ‘„åƒå¤´ç­‰ã€‚"
            else:
                answer = None
            
            if answer:
                # ä½¿ç”¨æ‰“å­—æœºæ•ˆæœæ˜¾ç¤º
                if self.streaming_enabled:
                    print("ğŸ’¬ å›ç­”: ", end='', flush=True)
                    self.typewriter_effect(answer)
                return answer
            
            else:
                # é€šç”¨ä¸­æ–‡å›ç­”
                top_result = sorted_results[0]
                title = top_result['title']
                content = top_result['content']
                score = top_result['score']
                
                if content.startswith('[Introduction] '):
                    content = content[16:]
                
                answer = f"æ ¹æ®æœç´¢ç»“æœï¼Œ{title} æä¾›äº†ä¸æ‚¨é—®é¢˜æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚{content[:300]}... è¿™ä¸ªç»“æœçš„ç›¸å…³åº¦è¯„åˆ†ä¸º {score:.3f}ï¼Œè¡¨æ˜å®ƒåŒ…å«äº†æ‚¨éœ€è¦çš„é‡è¦ä¿¡æ¯ã€‚å¦‚æœæ‚¨éœ€è¦æ›´è¯¦ç»†çš„äº†è§£ï¼Œå¯ä»¥è®¿é—®ç›¸å…³çš„ Wiki é¡µé¢è·å–å®Œæ•´çš„æŠ€æœ¯è§„æ ¼å’Œä½¿ç”¨è¯´æ˜ã€‚"
                
                # ä½¿ç”¨æ‰“å­—æœºæ•ˆæœæ˜¾ç¤º
                if self.streaming_enabled:
                    print("ğŸ’¬ å›ç­”: ", end='', flush=True)
                    self.typewriter_effect(answer)
                
                return answer
        
        else:
            # è‹±æ–‡å›ç­”
            if "xiao" in question_lower:
                answer = "XIAO series is a line of micro development boards that we launched at Seeed Studio. These boards are compact in size but powerful in functionality, featuring our standardized design philosophy with excellent compatibility and expandability. They are particularly suitable for various embedded projects, prototyping, and maker projects. XIAO series products not only support the Arduino ecosystem but also integrate Grove connectors, allowing you to easily connect various sensors and modules, greatly simplifying the complexity of hardware development."
            elif "grove" in question_lower:
                answer = "Grove sensor module system is a standardized hardware connection solution that we developed at Seeed Studio, revolutionizing the complex process of traditional hardware development. Through unified connection interfaces and standardized module design, Grove system allows you to easily connect various sensors, actuators, and communication modules to development boards like building blocks. This design not only greatly reduces the entry barrier for hardware development but also improves project reliability and maintainability, making it particularly suitable for beginners and rapid prototyping."
            elif "sensecap" in question_lower:
                answer = "SenseCAP is a one-stop solution that we specifically designed at Seeed Studio for environmental monitoring and IoT applications. It integrates high-precision sensor technology, advanced data acquisition systems, and powerful cloud management platforms. The system can monitor various environmental parameters in real-time, such as temperature, humidity, air quality, light intensity, etc., and transmit data to the cloud for analysis and management through wireless networks. SenseCAP is particularly suitable for smart agriculture, environmental monitoring, industrial IoT, and other scenarios, providing users with reliable and accurate environmental data support."
            elif "edge computing" in question_lower:
                answer = "Edge AI computing represents an important development direction in artificial intelligence technology, moving AI applications from the cloud to local devices for operation, achieving faster response speeds and better privacy protection. Through devices like reComputer based on the NVIDIA Jetson platform, edge computing can process various AI tasks locally, such as speech recognition, image processing, natural language understanding, etc., without relying on network connections. This technology is particularly suitable for application scenarios that require real-time processing and low-latency responses, such as autonomous driving, intelligent monitoring, and industrial automation."
            elif "recomputer" in question_lower:
                answer = "reComputer series is a high-performance edge computing device that we developed at Seeed Studio based on the NVIDIA Jetson platform, specifically designed for AI and edge computing applications. These devices integrate powerful GPU computing capabilities and support various mainstream deep learning frameworks such as TensorFlow and PyTorch, enabling the operation of complex AI models and algorithms. reComputer series products are not only powerful in performance but also feature good thermal design and rich interface configurations, making them particularly suitable for application scenarios that require local AI processing capabilities, such as robotics, drones, and smart cameras."
            else:
                answer = None
            
            if answer:
                # ä½¿ç”¨æ‰“å­—æœºæ•ˆæœæ˜¾ç¤º
                if self.streaming_enabled:
                    print("ğŸ’¬ å›ç­”: ", end='', flush=True)
                    self.typewriter_effect(answer)
                return answer
            
            else:
                # é€šç”¨è‹±æ–‡å›ç­”
                top_result = sorted_results[0]
                title = top_result['title']
                content = top_result['content']
                score = top_result['score']
                
                if content.startswith('[Introduction] '):
                    content = content[16:]
                
                answer = f"Based on the search results, {title} provides the most relevant information for your question. {content[:300]}... This result has a relevance score of {score:.3f}, indicating that it contains important information you need. If you need more detailed information, you can visit the relevant Wiki page for complete technical specifications and usage instructions."
                
                # ä½¿ç”¨æ‰“å­—æœºæ•ˆæœæ˜¾ç¤º
                if self.streaming_enabled:
                    print("ğŸ’¬ å›ç­”: ", end='', flush=True)
                    self.typewriter_effect(answer)
                
                return answer
    
    def run(self):
        """è¿è¡Œé—®ç­”ç³»ç»Ÿ"""
        print("ğŸ¤– Seeed Studioï¼ˆçŸ½é€’ç§‘æŠ€ï¼‰ä¸“å±æ™ºèƒ½åŠ©æ‰‹")
        print("=" * 50)
        print("æ¬¢è¿ä½¿ç”¨æˆ‘ä»¬çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼")
        print("æˆ‘æ˜¯Seeed Studioçš„ä¸“å±AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡")
        print("=" * 50)
        
        sample_questions = [
            "ä»‹ç»ä¸€ä¸‹XIAOç³»åˆ—äº§å“",
            "Groveä¼ æ„Ÿå™¨æ¨¡å—æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
            "SenseCAPçš„åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",
            "Edge Computingæ˜¯ä»€ä¹ˆï¼Ÿ",
            "reComputeræœ‰ä»€ä¹ˆç‰¹è‰²ï¼Ÿ"
        ]
        
        print(f"\nğŸ’¡ ç¤ºä¾‹é—®é¢˜:")
        for i, question in enumerate(sample_questions, 1):
            print(f"   {i}. {question}")
        
        print(f"\nğŸ’¬ ç°åœ¨å¯ä»¥å¼€å§‹æé—®äº†ï¼")
        print("ğŸ’¡ è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©ï¼Œ'quit' é€€å‡º")
        print("ğŸ’¡ æ”¯æŒæ–¹å‘é”®ã€é€€æ ¼é”®ç­‰ç¼–è¾‘åŠŸèƒ½")
        print("-" * 50)
        
        try:
            while True:
                try:
                    query = self.safe_input("\nğŸ¤” è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ")
                    
                    if not query:
                        continue
                    
                    if query.lower() in ['quit', 'exit', 'q']:
                        print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                        break
                    elif query.lower() == 'help':
                        self.show_help()
                        continue
                    elif query.lower() == 'info':
                        self.show_system_info()
                        continue
                    elif query.lower() == 'debug':
                        self.show_debug_info()
                        continue
                    elif query.lower() == 'sample':
                        print("ğŸ’¡ ç¤ºä¾‹é—®é¢˜:")
                        for i, question in enumerate(sample_questions, 1):
                            print(f"   {i}. {question}")
                        continue
                    elif query.lower() == 'clear':
                        self.clear_cache()
                        continue
                    elif query.lower() == 'save':
                        self.save_cache()
                        continue
                    elif query.lower() == 'stream':
                        self.streaming_enabled = not self.streaming_enabled
                        status = "å¯ç”¨" if self.streaming_enabled else "ç¦ç”¨"
                        print(f"âœ… æµå¼æ˜¾ç¤ºå·²{status}")
                        continue
                    elif query.lower() == 'speed':
                        print(f"å½“å‰æ‰“å­—é€Ÿåº¦: {self.typing_speed:.3f} ç§’/å­—ç¬¦")
                        try:
                            new_speed = float(self.safe_input("è¯·è¾“å…¥æ–°çš„æ‰“å­—é€Ÿåº¦ (0.01-0.1): "))
                            if 0.01 <= new_speed <= 0.1:
                                self.typing_speed = new_speed
                                print(f"âœ… æ‰“å­—é€Ÿåº¦å·²è®¾ç½®ä¸º: {new_speed:.3f} ç§’/å­—ç¬¦")
                            else:
                                print("âŒ é€Ÿåº¦èŒƒå›´åº”åœ¨ 0.01-0.1 ä¹‹é—´")
                        except ValueError:
                            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                        continue

                    
                    if query.isdigit() and 1 <= int(query) <= len(sample_questions):
                        query = sample_questions[int(query) - 1]
                        print(f"ğŸ” é€‰æ‹©çš„é—®é¢˜: {query}")
                    
                    self.ask_question(query)
                    
                except KeyboardInterrupt:
                    print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
                    break
                except Exception as e:
                    print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
                    continue
                    
        finally:
            # ä¿å­˜è¾“å…¥å†å²å’Œç¼“å­˜
            self.save_history()
            self.save_cache()
    
    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print("   - ç›´æ¥è¾“å…¥é—®é¢˜")
        print("   - è¾“å…¥ 'help' æ˜¾ç¤ºå¸®åŠ©")
        print("   - è¾“å…¥ 'info' æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯")
        print("   - è¾“å…¥ 'sample' æ˜¾ç¤ºç¤ºä¾‹é—®é¢˜")
        print("   - è¾“å…¥ 'debug' æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯")
        print("   - è¾“å…¥ 'clear' æ¸…ç©ºç¼“å­˜")
        print("   - è¾“å…¥ 'save' ä¿å­˜ç¼“å­˜")
        print("   - è¾“å…¥ 'stream' åˆ‡æ¢æµå¼æ˜¾ç¤º")
        print("   - è¾“å…¥ 'speed' è°ƒæ•´æ‰“å­—é€Ÿåº¦")

        print("   - è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç¨‹åº")
        print("\nâŒ¨ï¸  è¾“å…¥åŠŸèƒ½:")
        print("   - æ”¯æŒæ–¹å‘é”®ç§»åŠ¨å…‰æ ‡")
        print("   - æ”¯æŒé€€æ ¼é”®åˆ é™¤å­—ç¬¦")
        print("   - æ”¯æŒ Ctrl+A é€‰æ‹©å…¨éƒ¨")
        print("   - æ”¯æŒ Ctrl+U åˆ é™¤æ•´è¡Œ")
        print("   - æ”¯æŒ Tab é”®è‡ªåŠ¨è¡¥å…¨")
        print("\nğŸš€ ç³»ç»Ÿç‰¹æ€§:")
        print("   - ä½¿ç”¨é¢„ä¿å­˜çš„ FAISS ç´¢å¼•ï¼Œå¯åŠ¨å¿«é€Ÿ")
        print("   - åŸºäº Ollama nomic-embed-text æ¨¡å‹")
        print("   - æ™ºèƒ½ç¼“å­˜æœºåˆ¶ï¼Œé‡å¤é—®é¢˜ç§’ç­”")
        print("   - ä¼˜åŒ–çš„æœç´¢ç®—æ³•ï¼Œå“åº”æ›´å¿«")
        print("   - æµå¼å›ç­”æ˜¾ç¤ºï¼Œæ‰“å­—æœºæ•ˆæœ")
        print("   - å®æ—¶ç”Ÿæˆå›ç­”ï¼Œä¸ä¿å­˜ç¼“å­˜")
        print("   - åŸºäºè‹±æ–‡ Wiki å†…å®¹ï¼Œè´¨é‡é«˜")
    
    def show_debug_info(self):
        """æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯"""
        print("\nğŸ” è°ƒè¯•ä¿¡æ¯:")
        print(f"   FAISS ç´¢å¼•: {'å·²åŠ è½½' if self.faiss_index else 'æœªåŠ è½½'}")
        if self.faiss_index:
            print(f"     ç´¢å¼•ç±»å‹: {type(self.faiss_index).__name__}")
            print(f"     å‘é‡æ•°é‡: {self.faiss_index.ntotal}")
            print(f"     å‘é‡ç»´åº¦: {self.faiss_index.d}")
            print(f"     ç´¢å¼•çŠ¶æ€: {'æ­£å¸¸' if self.faiss_index.ntotal > 0 else 'å¼‚å¸¸'}")
        
        print(f"   å…ƒæ•°æ®: {'å·²åŠ è½½' if self.faiss_metadata else 'æœªåŠ è½½'}")
        if self.faiss_metadata:
            print(f"     è®°å½•æ•°é‡: {len(self.faiss_metadata)}")
            print(f"     ç¬¬ä¸€æ¡è®°å½•: {list(self.faiss_metadata[0].keys()) if self.faiss_metadata else 'æ— '}")
        
        print(f"   Wiki é¡µé¢: {'å·²åŠ è½½' if self.wiki_pages else 'æœªåŠ è½½'}")
        if self.wiki_pages:
            print(f"     é¡µé¢æ•°é‡: {len(self.wiki_pages)}")
            print(f"     ç¬¬ä¸€æ¡é¡µé¢: {list(self.wiki_pages[0].keys()) if self.wiki_pages else 'æ— '}")
        
        print(f"   Embedding æ¨¡å‹: {self.embedding_model}")
        print(f"   Embedding ç¼“å­˜: {len(self.embedding_cache)} é¡¹")
        print(f"   å›ç­”ç¼“å­˜: {len(self.answer_cache)} é¡¹")
        
        # æµ‹è¯• embedding ç”Ÿæˆ
        print("\nğŸ§ª æµ‹è¯• Embedding ç”Ÿæˆ...")
        try:
            test_text = "test"
            test_embedding = self.generate_embedding(test_text)
            if test_embedding is not None:
                print(f"   âœ… æµ‹è¯•æˆåŠŸ: ç»´åº¦ {len(test_embedding)}, ç±»å‹ {type(test_embedding)}")
                print(f"      èŒƒæ•°: {np.linalg.norm(test_embedding):.6f}")
                print(f"      æ•°æ®ç±»å‹: {test_embedding.dtype}")
                print(f"      å½¢çŠ¶: {test_embedding.shape}")
            else:
                print("   âŒ æµ‹è¯•å¤±è´¥: è¿”å› None")
        except Exception as e:
            print(f"   âŒ æµ‹è¯•å¼‚å¸¸: {str(e)}")
        
        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        print("\nğŸ” æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥:")
        if self.faiss_index and self.faiss_metadata and self.wiki_pages:
            index_count = self.faiss_index.ntotal
            metadata_count = len(self.faiss_metadata)
            pages_count = len(self.wiki_pages)
            
            print(f"   FAISS ç´¢å¼•å‘é‡æ•°: {index_count}")
            print(f"   å…ƒæ•°æ®è®°å½•æ•°: {metadata_count}")
            print(f"   Wiki é¡µé¢æ•°: {pages_count}")
            
            if index_count == metadata_count == pages_count:
                print("   âœ… æ•°æ®ä¸€è‡´æ€§: å®Œå…¨åŒ¹é…")
            else:
                print("   âš ï¸  æ•°æ®ä¸€è‡´æ€§: ä¸åŒ¹é…")
                if index_count != metadata_count:
                    print(f"      âš ï¸  ç´¢å¼•ä¸å…ƒæ•°æ®ä¸åŒ¹é…: {index_count} vs {metadata_count}")
                if metadata_count != pages_count:
                    print(f"      âš ï¸  å…ƒæ•°æ®ä¸é¡µé¢ä¸åŒ¹é…: {metadata_count} vs {pages_count}")
        else:
            print("   âŒ æ— æ³•æ£€æŸ¥: æ•°æ®æœªå®Œå…¨åŠ è½½")

def main():
    """ä¸»å‡½æ•°"""
    try:
        qa_system = OptimizedQASystem()
        qa_system.run()
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {str(e)}")
        print("è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶å’Œ Ollama æœåŠ¡")

if __name__ == "__main__":
    main()

