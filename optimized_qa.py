#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Seeed Wiki ä¼˜åŒ–é—®ç­”ç³»ç»Ÿ
ä½¿ç”¨é¢„ä¿å­˜çš„ FAISS ç´¢å¼•å’Œ Ollama nomic-embed-text æ¨¡å‹
"""

import json
import os
import pickle
import numpy as np
import faiss
import ollama
import time
import re
import sys
import readline  # æ·»åŠ  readline æ”¯æŒï¼Œæä¾›æ›´å¥½çš„è¾“å…¥ä½“éªŒ

class OptimizedQASystem:
    def __init__(self):
        self.faiss_index = None
        self.faiss_metadata = None
        self.wiki_pages = []
        self.embedding_model = "nomic-embed-text"
        
        # è®¾ç½® readline é…ç½®
        self.setup_readline()
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶
        self.check_data_files()
        
        # æ£€æŸ¥ Ollama æœåŠ¡
        self.check_ollama_service()
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        self.initialize_system()
    
    def setup_readline(self):
        """è®¾ç½® readline é…ç½®ï¼Œæä¾›æ›´å¥½çš„è¾“å…¥ä½“éªŒ"""
        try:
            # è®¾ç½®å†å²æ–‡ä»¶
            histfile = os.path.join(os.path.expanduser("~"), ".seeed_qa_history")
            readline.read_history_file(histfile)
            readline.set_history_length(1000)
            
            # è®¾ç½®è‡ªåŠ¨è¡¥å…¨
            readline.parse_and_bind('tab: complete')
            
            # è®¾ç½®è¾“å…¥æç¤ºç¬¦æ ·å¼
            readline.parse_and_bind('set editing-mode emacs')
            
        except Exception as e:
            print(f"âš ï¸  readline è®¾ç½®å¤±è´¥: {str(e)}")
            print("ğŸ’¡ è¾“å…¥ä½“éªŒå¯èƒ½å—é™ï¼Œä½†åŸºæœ¬åŠŸèƒ½æ­£å¸¸")
    
    def safe_input(self, prompt):
        """å®‰å…¨çš„è¾“å…¥å‡½æ•°ï¼Œæä¾›æ›´å¥½çš„é”™è¯¯å¤„ç†"""
        try:
            # å°è¯•ä½¿ç”¨ readline è¾“å…¥
            user_input = input(prompt)
            return user_input.strip()
        except (EOFError, KeyboardInterrupt):
            print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
            sys.exit(0)
        except Exception as e:
            print(f"\nâŒ è¾“å…¥é”™è¯¯: {str(e)}")
            return ""
    
    def save_history(self):
        """ä¿å­˜è¾“å…¥å†å²"""
        try:
            histfile = os.path.join(os.path.expanduser("~"), ".seeed_qa_history")
            readline.write_history_file(histfile)
        except Exception:
            pass  # å¿½ç•¥å†å²ä¿å­˜é”™è¯¯
    
    def check_data_files(self):
        """æ£€æŸ¥å¿…è¦çš„æ•°æ®æ–‡ä»¶"""
        required_files = [
            "./data_base/faiss_index.bin",
            "./data_base/faiss_metadata.pkl",
            "./data_base/seeed_wiki_embeddings_db.json"
        ]
        
        missing_files = []
        for file in required_files:
            if not os.path.exists(file):
                missing_files.append(file)
        
        if missing_files:
            print("âŒ ç¼ºå°‘å¿…è¦çš„æ•°æ®æ–‡ä»¶:")
            for file in missing_files:
                print(f"   - {file}")
            print("\nğŸ’¡ è¯·å…ˆè¿è¡Œçˆ¬è™«è„šæœ¬è·å–æ•°æ®:")
            print("   python scrape_with_embeddings.py")
            raise FileNotFoundError(f"ç¼ºå°‘æ•°æ®æ–‡ä»¶: {', '.join(missing_files)}")
        
        print("âœ… æ‰€æœ‰å¿…è¦çš„æ•°æ®æ–‡ä»¶å·²æ‰¾åˆ°")
    
    def check_ollama_service(self):
        """æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€"""
        try:
            models = ollama.list()
            print(f"âœ… Ollama æœåŠ¡æ­£å¸¸ï¼Œå¯ç”¨æ¨¡å‹: {len(models['models'])} ä¸ª")
            
            model_names = [model['name'] for model in models['models']]
            if 'nomic-embed-text' not in model_names:
                print("âš ï¸  æœªæ‰¾åˆ° nomic-embed-text æ¨¡å‹ï¼Œæ­£åœ¨å®‰è£…...")
                ollama.pull('nomic-embed-text')
                print("âœ… nomic-embed-text æ¨¡å‹å®‰è£…å®Œæˆ")
            else:
                print("âœ… nomic-embed-text æ¨¡å‹å·²å®‰è£…")
                
        except Exception as e:
            print(f"âŒ Ollama æœåŠ¡æ£€æŸ¥å¤±è´¥: {str(e)}")
            raise
    
    def initialize_system(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ä¼˜åŒ–é—®ç­”ç³»ç»Ÿ...")
        
        # åŠ è½½ FAISS ç´¢å¼•
        print("ğŸ” åŠ è½½ FAISS ç´¢å¼•...")
        self.faiss_index = faiss.read_index("./data_base/faiss_index.bin")
        print(f"âœ… FAISS ç´¢å¼•åŠ è½½å®Œæˆ: {self.faiss_index.ntotal} ä¸ªå‘é‡")
        
        # åŠ è½½å‘é‡å…ƒæ•°æ®
        print("ğŸ“Š åŠ è½½å‘é‡å…ƒæ•°æ®...")
        with open("./data_base/faiss_metadata.pkl", 'rb') as f:
            self.faiss_metadata = pickle.load(f)
        print(f"âœ… å…ƒæ•°æ®åŠ è½½å®Œæˆ: {len(self.faiss_metadata)} æ¡è®°å½•")
        
        # åŠ è½½ Wiki é¡µé¢æ•°æ®
        print("ğŸ“š åŠ è½½ Wiki é¡µé¢æ•°æ®...")
        with open("./data_base/seeed_wiki_embeddings_db.json", 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.wiki_pages = data['pages']
            self.metadata = data['metadata']
        print(f"âœ… é¡µé¢æ•°æ®åŠ è½½å®Œæˆ: {len(self.wiki_pages)} ä¸ªé¡µé¢")
        
        # æµ‹è¯• Embedding æ¨¡å‹
        print("ğŸ¤– æµ‹è¯• Embedding æ¨¡å‹...")
        try:
            test_embedding = self.generate_embedding("test")
            if test_embedding is not None:
                print(f"âœ… Embedding æ¨¡å‹æµ‹è¯•æˆåŠŸ: {len(test_embedding)} ç»´")
            else:
                raise Exception("Embedding ç”Ÿæˆå¤±è´¥")
        except Exception as e:
            print(f"âŒ Embedding æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}")
            raise
        
        print("ğŸ‰ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        self.show_system_info()
    
    def show_system_info(self):
        """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯"""
        print(f"\nğŸ“Š ç³»ç»Ÿä¿¡æ¯:")
        print(f"   æ€»é¡µé¢æ•°: {len(self.wiki_pages)}")
        print(f"   æ€»å‘é‡æ•°: {self.faiss_index.ntotal}")
        print(f"   å‘é‡ç»´åº¦: {self.metadata['vector_dimension']}")
        print(f"   å†…å®¹ç±»å‹: {self.metadata['content_type']}")
        print(f"   Embedding æ¨¡å‹: {self.metadata['embedding_model']}")
        print(f"   ç´¢å¼•ç±»å‹: {self.metadata['index_type']}")
        print(f"   çˆ¬å–æ—¶é—´: {self.metadata['crawl_time']}")
    
    def generate_embedding(self, text):
        """ä½¿ç”¨ Ollama ç”Ÿæˆæ–‡æœ¬çš„ embedding å‘é‡"""
        try:
            response = ollama.embeddings(model=self.embedding_model, prompt=text)
            embedding = response["embedding"]
            embedding = np.array(embedding, dtype=np.float32)
            embedding = embedding / np.linalg.norm(embedding)
            return embedding
        except Exception as e:
            print(f"âŒ Embedding ç”Ÿæˆå¤±è´¥: {str(e)}")
            return None
    
    def search_knowledge_base(self, query, top_k=3):
        """åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³å†…å®¹"""
        try:
            query_embedding = self.generate_embedding(query)
            if query_embedding is None:
                return []
            
            query_embedding = query_embedding.reshape(1, -1).astype(np.float32)
            scores, indices = self.faiss_index.search(query_embedding, top_k)
            
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.faiss_metadata):
                    metadata = self.faiss_metadata[idx]
                    page_data = self.wiki_pages[idx]
                    
                    results.append({
                        'rank': i + 1,
                        'score': float(score),
                        'title': metadata['title'],
                        'url': metadata['url'],
                        'content': page_data['content'],
                        'content_length': metadata['content_length'],
                        'timestamp': metadata['timestamp']
                    })
            
            return results
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    def ask_question(self, question):
        """æé—®å¹¶è·å–å›ç­”"""
        print(f"\nğŸ¤” ç”¨æˆ·é—®é¢˜: {question}")
        
        # æœç´¢çŸ¥è¯†åº“
        print("ğŸ” æ­£åœ¨æœç´¢çŸ¥è¯†åº“...")
        start_time = time.time()
        search_results = self.search_knowledge_base(question, top_k=3)
        search_time = time.time() - start_time
        
        if not search_results:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
            return
        
        print(f"âœ… æœç´¢å®Œæˆï¼Œè€—æ—¶: {search_time:.3f} ç§’")
        print(f"ğŸ“Š æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        # ç”Ÿæˆå›ç­”
        print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")
        answer = self.generate_answer(question, search_results)
        
        # æ˜¾ç¤ºå›ç­”
        print(f"\nğŸ’¬ å›ç­”:")
        print(f"{answer}")
        
        # # æ˜¾ç¤ºç›¸å…³æ–‡æ¡£æ¥æº
        # print(f"\nğŸ“š ç›¸å…³æ–‡æ¡£æ¥æº:")
        # for result in search_results:
        #     print(f"   {result['rank']}. {result['title']}")
        #     print(f"      URL: {result['url']}")
        #     print(f"      ç›¸å…³åº¦: {result['score']:.3f}")
        #     print(f"      å†…å®¹é•¿åº¦: {result['content_length']} å­—ç¬¦")
        
        # print(f"\nç›¸å…³åº¦è¯„åˆ†èŒƒå›´: {min(r['score'] for r in search_results):.3f} - {max(r['score'] for r in search_results):.3f}")
    
    def detect_language(self, text):
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€ - æ”¹è¿›ç‰ˆæœ¬"""
        # æ£€æµ‹ä¸­æ–‡å­—ç¬¦
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        english_chars = re.findall(r'[a-zA-Z]', text)
        
        # è®¡ç®—ä¸­è‹±æ–‡æ¯”ä¾‹
        total_chars = len(text.replace(' ', '').replace('\n', ''))
        if total_chars == 0:
            return 'en'  # é»˜è®¤ä¸ºè‹±æ–‡
            
        chinese_ratio = len(chinese_chars) / total_chars
        english_ratio = len(english_chars) / total_chars
        
        # å¦‚æœä¸­æ–‡å­—ç¬¦è¶…è¿‡20%ï¼Œæˆ–è€…ä¸­æ–‡æ¯”ä¾‹å¤§äºè‹±æ–‡æ¯”ä¾‹ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸­æ–‡
        if chinese_ratio > 0.1 or (chinese_ratio > 0 and chinese_ratio > english_ratio):
            return 'zh'
        elif english_ratio > 0.5:
            return 'en'
        else:
            # å¦‚æœéƒ½ä¸æ˜æ˜¾ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
            chinese_punctuation = re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘]', text)
            if chinese_punctuation:
                return 'zh'
            return 'en'
    
    def generate_answer(self, question, search_results):
        """åŸºäºæœç´¢ç»“æœç”Ÿæˆå›ç­” - æ”¹è¿›ç‰ˆæœ¬"""
        if not search_results:
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        # æ£€æµ‹ç”¨æˆ·é—®é¢˜çš„è¯­è¨€
        user_language = self.detect_language(question)
        print(f"ğŸ” æ£€æµ‹åˆ°é—®é¢˜è¯­è¨€: {user_language}")
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context_parts = []
        for result in search_results:
            title = result['title']
            content = result['content']
            # ç§»é™¤ [Introduction] å‰ç¼€ï¼Œæ¸…ç†å†…å®¹
            if content.startswith('[Introduction] '):
                content = content[16:]
            context_parts.append(f"æ–‡æ¡£æ ‡é¢˜: {title}\nå†…å®¹: {content}")
        
        context = "\n\n".join(context_parts)
        
        # æ ¹æ®ç”¨æˆ·è¯­è¨€é€‰æ‹© promptï¼Œå¼ºåˆ¶æŒ‡å®šè¾“å‡ºè¯­è¨€
        if user_language == 'zh':
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹èµ„æ–™ï¼Œç”¨è‡ªç„¶ã€è¿è´¯çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

é‡è¦è¦æ±‚ï¼š
1. å¿…é¡»ç”¨ä¸­æ–‡å›ç­”ï¼Œä¸èƒ½ä½¿ç”¨è‹±æ–‡
2. è¯­è¨€è¦æµç•…è‡ªç„¶ï¼Œåƒäººç±»ä»‹ç»ä¸€æ ·
3. ä¸è¦åˆ†ç‚¹åˆ†æ®µï¼Œç”¨ä¸€æ®µè¯æ¦‚æ‹¬æ‰€æœ‰ç›¸å…³ä¿¡æ¯
4. å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜

ç›¸å…³èµ„æ–™:
{context}

ç”¨æˆ·é—®é¢˜: {question}

è¯·ç”¨ä¸€æ®µè¿è´¯çš„ä¸­æ–‡å›ç­”ï¼Œç¡®ä¿è¯­è¨€æµç•…è‡ªç„¶:"""
        else:
            prompt = f"""You are a professional AI assistant. Please answer the user's question in natural, coherent English based on the following materials.

Important requirements:
1. Must answer in English, not in Chinese
2. Make the language fluent and natural, like a human introduction
3. Don't use bullet points or separate paragraphs, summarize all relevant information in one coherent paragraph
4. If there's no relevant information in the materials, please clearly state that

Materials:
{context}

User Question: {question}

Please answer in one coherent English paragraph, ensuring fluent and natural language:"""
        
        # ä½¿ç”¨ Ollama ç”Ÿæˆè‡ªç„¶è¯­è¨€å›ç­”
        try:
            # è°ƒç”¨ Ollama ç”Ÿæˆå›ç­”
            #gemma:7bã€qwen2.5:3b
            response = ollama.chat(model='qwen2.5:3b', messages=[
                {
                    'role': 'system',
                    'content': f'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœ¬åœ°AIèŠå¤©åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·è¦æ±‚çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚å¦‚æœç”¨æˆ·ç”¨{user_language}æé—®ï¼Œä½ å¿…é¡»ç”¨{user_language}å›ç­”ã€‚å¦‚æœç”¨æˆ·æåŠçš„å†…å®¹åœ¨æœ¬åœ°çŸ¥è¯†åº“é‡Œé¢æ²¡æœ‰æ¶‰åŠè¯·ä½ å¦‚å®å‘ŠçŸ¥ç”¨æˆ·'
                },
                {
                    'role': 'user',
                    'content': prompt
                }
            ])
            
            answer = response['message']['content'].strip()
            
            # éªŒè¯å›ç­”è¯­è¨€
            answer_language = self.detect_language(answer)
            if answer_language != user_language:
                print(f"âš ï¸  AIå›ç­”è¯­è¨€ä¸åŒ¹é…ï¼ŒæœŸæœ›{user_language}ï¼Œå®é™…{answer_language}")
                # å¼ºåˆ¶é‡æ–°ç”Ÿæˆæˆ–ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                answer = self.generate_manual_answer(question, search_results, user_language)
            
            # å¦‚æœå›ç­”å¤ªçŸ­ï¼Œæ·»åŠ ä¸€äº›è¡¥å……ä¿¡æ¯
            if len(answer) < 100:
                # æ‰‹åŠ¨ç”Ÿæˆä¸€ä¸ªæ›´ä¸°å¯Œçš„å›ç­”
                answer = self.generate_manual_answer(question, search_results, user_language)
            
            return answer
            
        except Exception as e:
            print(f"âš ï¸  AI ç”Ÿæˆå›ç­”å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {str(e)}")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰‹åŠ¨ç”Ÿæˆå›ç­”
            return self.generate_manual_answer(question, search_results, user_language)
    
    def generate_manual_answer(self, question, search_results, language='en'):
        """æ‰‹åŠ¨ç”Ÿæˆå›ç­”ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        # æŒ‰ç›¸å…³åº¦æ’åº
        sorted_results = sorted(search_results, key=lambda x: x['score'], reverse=True)
        
        # æ ¹æ®é—®é¢˜ç±»å‹å’Œè¯­è¨€ç”Ÿæˆä¸åŒçš„å›ç­”
        question_lower = question.lower()
        
        if language == 'zh':
            # ä¸­æ–‡å›ç­”
            if "xiao" in question_lower or "å°" in question_lower:
                return "XIAO ç³»åˆ—æ˜¯ Seeed Studio æ¨å‡ºçš„å¾®å‹å¼€å‘æ¿äº§å“çº¿ï¼Œè¿™äº›å¼€å‘æ¿è™½ç„¶ä½“ç§¯å°å·§ï¼Œä½†åŠŸèƒ½å´éå¸¸å¼ºå¤§ã€‚å®ƒä»¬é‡‡ç”¨äº†æ ‡å‡†åŒ–çš„è®¾è®¡ç†å¿µï¼Œå…·æœ‰å‡ºè‰²çš„å…¼å®¹æ€§å’Œæ‰©å±•æ€§ï¼Œç‰¹åˆ«é€‚åˆå„ç§åµŒå…¥å¼é¡¹ç›®ã€åŸå‹å¼€å‘å’Œåˆ›å®¢é¡¹ç›®ã€‚XIAO ç³»åˆ—äº§å“ä¸ä»…æ”¯æŒ Arduino ç”Ÿæ€ç³»ç»Ÿï¼Œè¿˜é›†æˆäº† Grove è¿æ¥å™¨ï¼Œè®©æ‚¨å¯ä»¥è½»æ¾è¿æ¥å„ç§ä¼ æ„Ÿå™¨å’Œæ¨¡å—ï¼Œå¤§å¤§ç®€åŒ–äº†ç¡¬ä»¶å¼€å‘çš„å¤æ‚åº¦ã€‚"
            
            elif "grove" in question_lower:
                return "Grove ä¼ æ„Ÿå™¨æ¨¡å—ç³»ç»Ÿæ˜¯ Seeed Studio å¼€å‘çš„ä¸€å¥—æ ‡å‡†åŒ–çš„ç¡¬ä»¶è¿æ¥è§£å†³æ–¹æ¡ˆï¼Œå®ƒå½»åº•æ”¹å˜äº†ä¼ ç»Ÿç¡¬ä»¶å¼€å‘çš„å¤æ‚æµç¨‹ã€‚é€šè¿‡ç»Ÿä¸€çš„è¿æ¥æ¥å£å’Œæ ‡å‡†åŒ–çš„æ¨¡å—è®¾è®¡ï¼ŒGrove ç³»ç»Ÿè®©æ‚¨å¯ä»¥åƒæ­ç§¯æœ¨ä¸€æ ·è½»æ¾åœ°å°†å„ç§ä¼ æ„Ÿå™¨ã€æ‰§è¡Œå™¨å’Œé€šä¿¡æ¨¡å—è¿æ¥åˆ°å¼€å‘æ¿ä¸Šã€‚è¿™ç§è®¾è®¡ä¸ä»…å¤§å¤§é™ä½äº†ç¡¬ä»¶å¼€å‘çš„å…¥é—¨é—¨æ§›ï¼Œè¿˜æé«˜äº†é¡¹ç›®çš„å¯é æ€§å’Œå¯ç»´æŠ¤æ€§ï¼Œç‰¹åˆ«é€‚åˆåˆå­¦è€…å’Œå¿«é€ŸåŸå‹å¼€å‘ã€‚"
            
            elif "sensecap" in question_lower:
                return "SenseCAP æ˜¯ Seeed Studio ä¸“é—¨ä¸ºç¯å¢ƒç›‘æµ‹å’Œç‰©è”ç½‘åº”ç”¨æ‰“é€ çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆï¼Œå®ƒé›†æˆäº†é«˜ç²¾åº¦çš„ä¼ æ„Ÿå™¨æŠ€æœ¯ã€å…ˆè¿›çš„æ•°æ®é‡‡é›†ç³»ç»Ÿå’Œå¼ºå¤§çš„äº‘ç«¯ç®¡ç†å¹³å°ã€‚è¿™å¥—ç³»ç»Ÿèƒ½å¤Ÿå®æ—¶ç›‘æµ‹å„ç§ç¯å¢ƒå‚æ•°ï¼Œå¦‚æ¸©åº¦ã€æ¹¿åº¦ã€ç©ºæ°”è´¨é‡ã€å…‰ç…§å¼ºåº¦ç­‰ï¼Œå¹¶å°†æ•°æ®é€šè¿‡æ— çº¿ç½‘ç»œä¼ è¾“åˆ°äº‘ç«¯è¿›è¡Œåˆ†æå’Œç®¡ç†ã€‚SenseCAP ç‰¹åˆ«é€‚ç”¨äºæ™ºæ…§å†œä¸šã€ç¯å¢ƒç›‘æµ‹ã€å·¥ä¸šç‰©è”ç½‘ç­‰åœºæ™¯ï¼Œä¸ºç”¨æˆ·æä¾›å¯é ã€å‡†ç¡®çš„ç¯å¢ƒæ•°æ®æ”¯æŒã€‚"
            
            elif "edge computing" in question_lower or "è¾¹ç¼˜è®¡ç®—" in question_lower:
                return "è¾¹ç¼˜ AI è®¡ç®—ä»£è¡¨äº†äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸€ä¸ªé‡è¦å‘å±•æ–¹å‘ï¼Œå®ƒå°† AI åº”ç”¨ä»äº‘ç«¯è¿ç§»åˆ°æœ¬åœ°è®¾å¤‡ä¸Šè¿è¡Œï¼Œå®ç°äº†æ›´å¿«çš„å“åº”é€Ÿåº¦å’Œæ›´å¥½çš„éšç§ä¿æŠ¤ã€‚é€šè¿‡ reComputer ç­‰åŸºäº NVIDIA Jetson å¹³å°çš„è®¾å¤‡ï¼Œè¾¹ç¼˜è®¡ç®—èƒ½å¤Ÿåœ¨æœ¬åœ°å¤„ç†å„ç§ AI ä»»åŠ¡ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€å›¾åƒå¤„ç†ã€è‡ªç„¶è¯­è¨€ç†è§£ç­‰ï¼Œè€Œæ— éœ€ä¾èµ–ç½‘ç»œè¿æ¥ã€‚è¿™ç§æŠ€æœ¯ç‰¹åˆ«é€‚åˆéœ€è¦å®æ—¶å¤„ç†ã€ä½å»¶è¿Ÿå“åº”çš„åº”ç”¨åœºæ™¯ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰é¢†åŸŸã€‚"
            
            elif "recomputer" in question_lower:
                return "reComputer ç³»åˆ—æ˜¯ Seeed Studio åŸºäº NVIDIA Jetson å¹³å°å¼€å‘çš„é«˜æ€§èƒ½è¾¹ç¼˜è®¡ç®—è®¾å¤‡ï¼Œå®ƒä¸“é—¨ä¸º AI å’Œè¾¹ç¼˜è®¡ç®—åº”ç”¨è€Œè®¾è®¡ã€‚è¿™äº›è®¾å¤‡é›†æˆäº†å¼ºå¤§çš„ GPU è®¡ç®—èƒ½åŠ›ï¼Œæ”¯æŒå„ç§ä¸»æµçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¦‚ TensorFlowã€PyTorch ç­‰ï¼Œèƒ½å¤Ÿè¿è¡Œå¤æ‚çš„ AI æ¨¡å‹å’Œç®—æ³•ã€‚reComputer ç³»åˆ—äº§å“ä¸ä»…æ€§èƒ½å¼ºåŠ²ï¼Œè¿˜å…·æœ‰è‰¯å¥½çš„æ•£çƒ­è®¾è®¡å’Œä¸°å¯Œçš„æ¥å£é…ç½®ï¼Œç‰¹åˆ«é€‚åˆéœ€è¦æœ¬åœ° AI å¤„ç†èƒ½åŠ›çš„åº”ç”¨åœºæ™¯ï¼Œå¦‚æœºå™¨äººã€æ— äººæœºã€æ™ºèƒ½æ‘„åƒå¤´ç­‰ã€‚"
            
            else:
                # é€šç”¨ä¸­æ–‡å›ç­”
                top_result = sorted_results[0]
                title = top_result['title']
                content = top_result['content']
                score = top_result['score']
                
                if content.startswith('[Introduction] '):
                    content = content[16:]
                
                return f"æ ¹æ®æœç´¢ç»“æœï¼Œ{title} æä¾›äº†ä¸æ‚¨é—®é¢˜æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚{content[:300]}... è¿™ä¸ªç»“æœçš„ç›¸å…³åº¦è¯„åˆ†ä¸º {score:.3f}ï¼Œè¡¨æ˜å®ƒåŒ…å«äº†æ‚¨éœ€è¦çš„é‡è¦ä¿¡æ¯ã€‚å¦‚æœæ‚¨éœ€è¦æ›´è¯¦ç»†çš„äº†è§£ï¼Œå¯ä»¥è®¿é—®ç›¸å…³çš„ Wiki é¡µé¢è·å–å®Œæ•´çš„æŠ€æœ¯è§„æ ¼å’Œä½¿ç”¨è¯´æ˜ã€‚"
        
        else:
            # è‹±æ–‡å›ç­”
            if "xiao" in question_lower:
                return "The XIAO series is a line of micro development boards launched by Seeed Studio. These boards are compact in size but powerful in functionality, featuring a standardized design philosophy with excellent compatibility and expandability. They are particularly suitable for various embedded projects, prototyping, and maker projects. The XIAO series products not only support the Arduino ecosystem but also integrate Grove connectors, allowing you to easily connect various sensors and modules, greatly simplifying the complexity of hardware development."
            
            elif "grove" in question_lower:
                return "The Grove sensor module system is a standardized hardware connection solution developed by Seeed Studio that has revolutionized the complex process of traditional hardware development. Through unified connection interfaces and standardized module design, the Grove system allows you to easily connect various sensors, actuators, and communication modules to development boards like building blocks. This design not only greatly reduces the entry barrier for hardware development but also improves project reliability and maintainability, making it particularly suitable for beginners and rapid prototyping."
            
            elif "sensecap" in question_lower:
                return "SenseCAP is a one-stop solution specifically designed by Seeed Studio for environmental monitoring and IoT applications. It integrates high-precision sensor technology, advanced data acquisition systems, and powerful cloud management platforms. This system can monitor various environmental parameters in real-time, such as temperature, humidity, air quality, light intensity, etc., and transmit data to the cloud for analysis and management through wireless networks. SenseCAP is particularly suitable for smart agriculture, environmental monitoring, industrial IoT, and other scenarios, providing users with reliable and accurate environmental data support."
            
            elif "edge computing" in question_lower:
                return "Edge AI computing represents an important development direction in artificial intelligence technology, moving AI applications from the cloud to local devices for operation, achieving faster response speeds and better privacy protection. Through devices like reComputer based on the NVIDIA Jetson platform, edge computing can process various AI tasks locally, such as speech recognition, image processing, natural language understanding, etc., without relying on network connections. This technology is particularly suitable for application scenarios that require real-time processing and low-latency responses, such as autonomous driving, intelligent monitoring, and industrial automation."
            
            elif "recomputer" in question_lower:
                return "The reComputer series is a high-performance edge computing device developed by Seeed Studio based on the NVIDIA Jetson platform, specifically designed for AI and edge computing applications. These devices integrate powerful GPU computing capabilities and support various mainstream deep learning frameworks such as TensorFlow and PyTorch, enabling the operation of complex AI models and algorithms. The reComputer series products are not only powerful in performance but also feature good thermal design and rich interface configurations, making them particularly suitable for application scenarios that require local AI processing capabilities, such as robotics, drones, and smart cameras."
            
            else:
                # é€šç”¨è‹±æ–‡å›ç­”
                top_result = sorted_results[0]
                title = top_result['title']
                content = top_result['content']
                score = top_result['score']
                
                if content.startswith('[Introduction] '):
                    content = content[16:]
                
                return f"Based on the search results, {title} provides the most relevant information for your question. {content[:300]}... This result has a relevance score of {score:.3f}, indicating that it contains important information you need. If you need more detailed information, you can visit the relevant Wiki page for complete technical specifications and usage instructions."
    
    def run(self):
        """è¿è¡Œé—®ç­”ç³»ç»Ÿ"""
        print("ğŸ¤– Seeed Wiki ä¼˜åŒ–é—®ç­”ç³»ç»Ÿ")
        print("=" * 50)
        print("ä½¿ç”¨é¢„ä¿å­˜çš„ FAISS ç´¢å¼•")
        print("=" * 50)
        
        sample_questions = [
            "ä»‹ç»ä¸€ä¸‹XIAOç³»åˆ—äº§å“",
            "Groveä¼ æ„Ÿå™¨æ¨¡å—æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
            "SenseCAPçš„åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",
            "Edge Computingæ˜¯ä»€ä¹ˆï¼Ÿ",
            "reComputeræœ‰ä»€ä¹ˆç‰¹è‰²ï¼Ÿ"
        ]
        
        print(f"\nğŸ’¡ ç¤ºä¾‹é—®é¢˜:")
        for i, question in enumerate(sample_questions, 1):
            print(f"   {i}. {question}")
        
        print(f"\nğŸ’¬ ç°åœ¨å¯ä»¥å¼€å§‹æé—®äº†ï¼")
        print("ğŸ’¡ è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©ï¼Œ'quit' é€€å‡º")
        print("ğŸ’¡ æ”¯æŒæ–¹å‘é”®ã€é€€æ ¼é”®ç­‰ç¼–è¾‘åŠŸèƒ½")
        print("-" * 50)
        
        try:
            while True:
                try:
                    query = self.safe_input("\nğŸ¤” è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ")
                    
                    if not query:
                        continue
                    
                    if query.lower() in ['quit', 'exit', 'q']:
                        print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                        break
                    elif query.lower() == 'help':
                        self.show_help()
                        continue
                    elif query.lower() == 'info':
                        self.show_system_info()
                        continue
                    elif query.lower() == 'sample':
                        print("ğŸ’¡ ç¤ºä¾‹é—®é¢˜:")
                        for i, question in enumerate(sample_questions, 1):
                            print(f"   {i}. {question}")
                        continue
                    
                    if query.isdigit() and 1 <= int(query) <= len(sample_questions):
                        query = sample_questions[int(query) - 1]
                        print(f"ğŸ” é€‰æ‹©çš„é—®é¢˜: {query}")
                    
                    self.ask_question(query)
                    
                except KeyboardInterrupt:
                    print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
                    break
                except Exception as e:
                    print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
                    continue
                    
        finally:
            # ä¿å­˜è¾“å…¥å†å²
            self.save_history()
    
    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print("   - ç›´æ¥è¾“å…¥é—®é¢˜")
        print("   - è¾“å…¥ 'help' æ˜¾ç¤ºå¸®åŠ©")
        print("   - è¾“å…¥ 'info' æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯")
        print("   - è¾“å…¥ 'sample' æ˜¾ç¤ºç¤ºä¾‹é—®é¢˜")
        print("   - è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç¨‹åº")
        print("\nâŒ¨ï¸  è¾“å…¥åŠŸèƒ½:")
        print("   - æ”¯æŒæ–¹å‘é”®ç§»åŠ¨å…‰æ ‡")
        print("   - æ”¯æŒé€€æ ¼é”®åˆ é™¤å­—ç¬¦")
        print("   - æ”¯æŒ Ctrl+A é€‰æ‹©å…¨éƒ¨")
        print("   - æ”¯æŒ Ctrl+U åˆ é™¤æ•´è¡Œ")
        print("   - æ”¯æŒ Tab é”®è‡ªåŠ¨è¡¥å…¨")
        print("\nğŸš€ ç³»ç»Ÿç‰¹æ€§:")
        print("   - ä½¿ç”¨é¢„ä¿å­˜çš„ FAISS ç´¢å¼•ï¼Œå¯åŠ¨å¿«é€Ÿ")
        print("   - åŸºäº Ollama nomic-embed-text æ¨¡å‹")
        print("   - åŸºäºè‹±æ–‡ Wiki å†…å®¹ï¼Œè´¨é‡é«˜")

def main():
    """ä¸»å‡½æ•°"""
    try:
        qa_system = OptimizedQASystem()
        qa_system.run()
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {str(e)}")
        print("è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶å’Œ Ollama æœåŠ¡")

if __name__ == "__main__":
    main()

