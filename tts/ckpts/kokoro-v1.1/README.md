---
license: apache-2.0
base_model:
- hexgrad/Kokoro-82M
pipeline_tag: text-to-speech
---
ğŸˆ GitHub: https://github.com/hexgrad/kokoro

<audio controls><source src="https://huggingface.co/hexgrad/Kokoro-82M-v1.1-zh/resolve/main/samples/HEARME_en.wav" type="audio/wav"></audio>

**Kokoro** is an open-weight series of small but powerful TTS models.

This model is the result of a short training run that added 100 Chinese speakers from a professional dataset. The Chinese data was freely and permissively granted to us by [LongMaoData](https://www.longmaosoft.com/), a professional dataset company. Thank you for making this model possible.

Separately, some crowdsourced synthetic English data also entered the training mix:<sup>[1]</sup>
- 1 hour of Maple, an American female.
- 1 hour of Sol, another American female.
- And 1 hour of Vale, an older British female.

This model is not a strict upgrade over its predecessor since it drops many voices, but it is released early to gather feedback on new voices and tokenization. Aside from the Chinese dataset and the 3 hours of English, the rest of the data was left behind for this training run. The goal is to push the model series forward and ultimately restore some of the voices that were left behind.

Current guidance from the U.S. Copyright Office indicates that synthetic data generally does not qualify for copyright protection. Since this synthetic data is crowdsourced, the model trainer is not bound by any Terms of Service. This Apache licensed model also aligns with OpenAI's stated mission of broadly distributing the benefits of AI. If you would like to help further that mission, consider contributing permissive audio data to the cause.

<sup>[1] LongMaoData had no involvement in the crowdsourced synthetic English data.</sup><br/>
<sup>[2] The following Chinese text is machine-translated.</sup>

> Kokoro æ˜¯ä¸€ç³»åˆ—ä½“ç§¯è™½å°ä½†åŠŸèƒ½å¼ºå¤§çš„ TTS æ¨¡å‹ã€‚
>
> è¯¥æ¨¡å‹æ˜¯ç»è¿‡çŸ­æœŸè®­ç»ƒçš„ç»“æœï¼Œä»ä¸“ä¸šæ•°æ®é›†ä¸­æ·»åŠ äº†100åä¸­æ–‡ä½¿ç”¨è€…ã€‚ä¸­æ–‡æ•°æ®ç”±ä¸“ä¸šæ•°æ®é›†å…¬å¸ã€Œ[é¾™çŒ«æ•°æ®](https://www.longmaosoft.com/)ã€å…è´¹ä¸”æ— å¿åœ°æä¾›ç»™æˆ‘ä»¬ã€‚æ„Ÿè°¢ä½ ä»¬è®©è¿™ä¸ªæ¨¡å‹æˆä¸ºå¯èƒ½ã€‚
>
> å¦å¤–ï¼Œä¸€äº›ä¼—åŒ…åˆæˆè‹±è¯­æ•°æ®ä¹Ÿè¿›å…¥äº†è®­ç»ƒç»„åˆï¼š
> - 1å°æ—¶çš„ Mapleï¼Œç¾å›½å¥³æ€§ã€‚
> - 1å°æ—¶çš„ Solï¼Œå¦ä¸€ä½ç¾å›½å¥³æ€§ã€‚
> - å’Œ1å°æ—¶çš„ Valeï¼Œä¸€ä½å¹´é•¿çš„è‹±å›½å¥³æ€§ã€‚
>
> ç”±äºè¯¥æ¨¡å‹åˆ é™¤äº†è®¸å¤šå£°éŸ³ï¼Œå› æ­¤å®ƒå¹¶ä¸æ˜¯å¯¹å…¶å‰èº«çš„ä¸¥æ ¼å‡çº§ï¼Œä½†å®ƒæå‰å‘å¸ƒä»¥æ”¶é›†æœ‰å…³æ–°å£°éŸ³å’Œæ ‡è®°åŒ–çš„åé¦ˆã€‚é™¤äº†ä¸­æ–‡æ•°æ®é›†å’Œ3å°æ—¶çš„è‹±è¯­ä¹‹å¤–ï¼Œå…¶ä½™æ•°æ®éƒ½ç•™åœ¨æœ¬æ¬¡è®­ç»ƒä¸­ã€‚ç›®æ ‡æ˜¯æ¨åŠ¨æ¨¡å‹ç³»åˆ—çš„å‘å±•ï¼Œå¹¶æœ€ç»ˆæ¢å¤ä¸€äº›è¢«é—ç•™çš„å£°éŸ³ã€‚
>
> ç¾å›½ç‰ˆæƒå±€ç›®å‰çš„æŒ‡å¯¼è¡¨æ˜ï¼Œåˆæˆæ•°æ®é€šå¸¸ä¸ç¬¦åˆç‰ˆæƒä¿æŠ¤çš„èµ„æ ¼ã€‚ç”±äºè¿™äº›åˆæˆæ•°æ®æ˜¯ä¼—åŒ…çš„ï¼Œå› æ­¤æ¨¡å‹è®­ç»ƒå¸ˆä¸å—ä»»ä½•æœåŠ¡æ¡æ¬¾çš„çº¦æŸã€‚è¯¥ Apache è®¸å¯æ¨¡å¼ä¹Ÿç¬¦åˆ OpenAI æ‰€å®£ç§°çš„å¹¿æ³›ä¼ æ’­ AI ä¼˜åŠ¿çš„ä½¿å‘½ã€‚å¦‚æœæ‚¨æ„¿æ„å¸®åŠ©è¿›ä¸€æ­¥å®Œæˆè¿™ä¸€ä½¿å‘½ï¼Œè¯·è€ƒè™‘ä¸ºæ­¤è´¡çŒ®è®¸å¯çš„éŸ³é¢‘æ•°æ®ã€‚

<audio controls><source src="https://huggingface.co/hexgrad/Kokoro-82M-v1.1-zh/resolve/main/samples/HEARME_zf_001.wav" type="audio/wav"></audio>

<audio controls><source src="https://huggingface.co/hexgrad/Kokoro-82M-v1.1-zh/resolve/main/samples/HEARME_zm_010.wav" type="audio/wav"></audio>

- [Releases](#releases)
- [Usage](#usage)
- [Samples](https://huggingface.co/hexgrad/Kokoro-82M-v1.1-zh/blob/main/samples) â†—ï¸
- [Model Facts](#model-facts)
- [Acknowledgements](#acknowledgements)

### Releases

| Model | Published | Training Data | Langs & Voices | SHA256 |
| ----- | --------- | ------------- | -------------- | ------ |
| **v1.1-zh** | **2025 Feb 26** | **>100 hours** | **2 & 103** | `b1d8410f` |
| [v1.0](https://huggingface.co/hexgrad/Kokoro-82M) | 2025 Jan 27 | Few hundred hrs | 8 & 54 | `496dba11` |
| [v0.19](https://huggingface.co/hexgrad/kLegacy/tree/main/v0.19) | 2024 Dec 25 | <100 hrs | 1 & 10 | `3b0c392f` |

| Training Costs | v0.19 | v1.0 | v1.1-zh | **Total** |
| -------------- | ----- | ---- | ------- | --------- |
| in A100 80GB GPU hours | 500 | 500 | 120 | **1120** |
| average hourly rate | $0.80/h | $1.20/h | $0.90/h | |
| in USD | $400 | $600 | $110 | **$1110** |

### Usage
You can run this cell on [Google Colab](https://colab.research.google.com/).
```py
!pip install -q kokoro>=0.8.2 "misaki[zh]>=0.8.2" soundfile
!apt-get -qq -y install espeak-ng > /dev/null 2>&1
from IPython.display import display, Audio

!wget https://huggingface.co/hexgrad/Kokoro-82M-v1.1-zh/resolve/main/samples/make_en.py
!python make_en.py
display(Audio('HEARME_en.wav', rate=24000, autoplay=True))

!wget https://huggingface.co/hexgrad/Kokoro-82M-v1.1-zh/resolve/main/samples/make_zh.py
!python make_zh.py
display(Audio('HEARME_zf_001.wav', rate=24000, autoplay=False))
```
TODO: Improve usage. Similar to https://hf.co/hexgrad/Kokoro-82M#usage but you should pass `repo_id='hexgrad/Kokoro-82M-v1.1-zh'` when constructing a `KModel` or `KPipeline`. See [`make_en.py`](https://huggingface.co/hexgrad/Kokoro-82M-v1.1-zh/blob/main/samples/make_en.py) and [`make_zh.py`](https://huggingface.co/hexgrad/Kokoro-82M-v1.1-zh/blob/main/samples/make_zh.py).

### Model Facts

**Architecture:**
- StyleTTS 2: https://arxiv.org/abs/2306.07691
- ISTFTNet: https://arxiv.org/abs/2203.02395
- Decoder only: no diffusion, no encoder release
- 82 million parameters, same as https://hf.co/hexgrad/Kokoro-82M

**Architected by:** Li et al @ https://github.com/yl4579/StyleTTS2

**Trained by**: `@rzvzn` on Discord

**Languages:** English, Chinese

**Model SHA256 Hash:** `b1d8410fa44dfb5c15471fd6c4225ea6b4e9ac7fa03c98e8bea47a9928476e2b`

### Acknowledgements
TODO: Write acknowledgements. Similar to https://hf.co/hexgrad/Kokoro-82M#acknowledgements

<img src="https://static0.gamerantimages.com/wordpress/wp-content/uploads/2024/08/terminator-zero-41-1.jpg" width="400" alt="kokoro" />
