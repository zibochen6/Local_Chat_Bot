#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼ºåˆ¶æ£€æŸ¥è„šæœ¬ - æ£€æŸ¥æ‰€æœ‰é¡µé¢å¹¶æ›´æ–°ç¼ºå¤±çš„å†…å®¹
ç”¨äºå¤„ç†ä¸­æ–­åçš„æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
"""

import os
import json
import time
from datetime import datetime
from scrape_with_embeddings import OptimizedWikiScraper

def force_check_all_pages():
    """å¼ºåˆ¶æ£€æŸ¥æ‰€æœ‰é¡µé¢å¹¶æ›´æ–°ç¼ºå¤±çš„å†…å®¹"""
    print("ğŸ”§ å¼€å§‹å¼ºåˆ¶æ£€æŸ¥æ‰€æœ‰é¡µé¢...")
    print("ğŸ“Š åŠŸèƒ½è¯´æ˜:")
    print("   - å¿½ç•¥24å°æ—¶æ›´æ–°é™åˆ¶")
    print("   - æ£€æŸ¥æ‰€æœ‰é¡µé¢çš„å®Œæ•´æ€§")
    print("   - æ›´æ–°ç¼ºå¤±æˆ–æŸåçš„é¡µé¢")
    print("   - é‡æ–°ç”Ÿæˆç¼ºå¤±çš„å‘é‡")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = OptimizedWikiScraper()
    
    # æ£€æŸ¥ç°æœ‰æ•°æ®
    print(f"\nğŸ“‚ ç°æœ‰æ•°æ®ç»Ÿè®¡:")
    print(f"   - é¡µé¢æ•°é‡: {len(scraper.all_content)}")
    print(f"   - å‘é‡æ•°é‡: {len([v for v in scraper.faiss_vectors if v is not None])}")
    print(f"   - URLå“ˆå¸Œæ•°é‡: {len(scraper.url_hashes)}")
    
    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    print(f"\nğŸ” æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")
    
    # æ£€æŸ¥å‘é‡å’Œé¡µé¢æ•°é‡æ˜¯å¦åŒ¹é…
    valid_vectors = [v for v in scraper.faiss_vectors if v is not None]
    if len(valid_vectors) != len(scraper.all_content):
        print(f"âš ï¸  å‘é‡æ•°é‡ä¸åŒ¹é…: é¡µé¢ {len(scraper.all_content)}, å‘é‡ {len(valid_vectors)}")
        print("ğŸ”„ éœ€è¦é‡æ–°ç”Ÿæˆç¼ºå¤±çš„å‘é‡")
        need_rebuild = True
    else:
        print("âœ… å‘é‡æ•°é‡åŒ¹é…")
        need_rebuild = False
    
    # æ£€æŸ¥URLå“ˆå¸Œå®Œæ•´æ€§
    missing_hashes = []
    for page in scraper.all_content:
        url = page.get('url')
        if url and url not in scraper.url_hashes:
            missing_hashes.append(url)
    
    if missing_hashes:
        print(f"âš ï¸  å‘ç° {len(missing_hashes)} ä¸ªé¡µé¢ç¼ºå°‘å“ˆå¸Œå€¼")
        print("ğŸ”„ éœ€è¦é‡æ–°è®¡ç®—å“ˆå¸Œå€¼")
        need_rebuild = True
    else:
        print("âœ… URLå“ˆå¸Œå€¼å®Œæ•´")
    
    # æ£€æŸ¥é¡µé¢å†…å®¹å®Œæ•´æ€§
    incomplete_pages = []
    for page in scraper.all_content:
        if not page.get('content') or len(page.get('content', '')) < 50:
            incomplete_pages.append(page.get('url'))
    
    if incomplete_pages:
        print(f"âš ï¸  å‘ç° {len(incomplete_pages)} ä¸ªé¡µé¢å†…å®¹ä¸å®Œæ•´")
        print("ğŸ”„ éœ€è¦é‡æ–°çˆ¬å–è¿™äº›é¡µé¢")
        need_rebuild = True
    else:
        print("âœ… é¡µé¢å†…å®¹å®Œæ•´")
    
    if not need_rebuild:
        print("\nâœ… æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡ï¼Œæ— éœ€é‡å»º")
        print("ğŸ’¡ å¦‚éœ€å¼ºåˆ¶æ›´æ–°ï¼Œè¯·ä½¿ç”¨ --force-check å‚æ•°")
        return
    
    print(f"\nğŸ”„ å¼€å§‹å¼ºåˆ¶æ›´æ–°...")
    
    # å¼ºåˆ¶è¿è¡Œå¢é‡æ›´æ–°
    try:
        scraper.run_incremental_update(force_check=True)
        print("\nâœ… å¼ºåˆ¶æ›´æ–°å®Œæˆ")
    except Exception as e:
        print(f"\nâŒ å¼ºåˆ¶æ›´æ–°å¤±è´¥: {str(e)}")
        return
    
    # æ›´æ–°åçš„ç»Ÿè®¡
    print(f"\nğŸ“Š æ›´æ–°åç»Ÿè®¡:")
    print(f"   - é¡µé¢æ•°é‡: {len(scraper.all_content)}")
    print(f"   - å‘é‡æ•°é‡: {len([v for v in scraper.faiss_vectors if v is not None])}")
    print(f"   - URLå“ˆå¸Œæ•°é‡: {len(scraper.url_hashes)}")
    
    # è¯­è¨€ç»Ÿè®¡
    chinese_pages = len([p for p in scraper.all_content if p.get('language') == 'ä¸­æ–‡'])
    english_pages = len([p for p in scraper.all_content if p.get('language') == 'English'])
    
    print(f"   - ä¸­æ–‡é¡µé¢: {chinese_pages}")
    print(f"   - è‹±æ–‡é¡µé¢: {english_pages}")
    
    print(f"\nğŸ‰ å¼ºåˆ¶æ£€æŸ¥å®Œæˆï¼")
    print(f"ğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨ä¼˜åŒ–ç‰ˆé—®ç­”ç³»ç»Ÿäº†")

def check_specific_pages():
    """æ£€æŸ¥ç‰¹å®šé¡µé¢çš„å®Œæ•´æ€§"""
    print("ğŸ” æ£€æŸ¥ç‰¹å®šé¡µé¢å®Œæ•´æ€§...")
    
    scraper = OptimizedWikiScraper()
    
    # æ£€æŸ¥é‡è¦é¡µé¢æ˜¯å¦å­˜åœ¨
    important_urls = [
        "https://wiki.seeedstudio.com/Getting_Started/",
        "https://wiki.seeedstudio.com/XIAO/",
        "https://wiki.seeedstudio.com/Grove/",
        "https://wiki.seeedstudio.com/SenseCAP/",
        "https://wiki.seeedstudio.com/reComputer/",
        "https://wiki.seeedstudio.com/zh/Getting_Started/",
        "https://wiki.seeedstudio.com/zh/XIAO/",
    ]
    
    missing_pages = []
    for url in important_urls:
        found = False
        for page in scraper.all_content:
            if page.get('url') == url:
                found = True
                content_length = len(page.get('content', ''))
                if content_length < 50:
                    print(f"âš ï¸  {url}: å†…å®¹è¿‡çŸ­ ({content_length} å­—ç¬¦)")
                else:
                    print(f"âœ… {url}: æ­£å¸¸ ({content_length} å­—ç¬¦)")
                break
        
        if not found:
            missing_pages.append(url)
            print(f"âŒ {url}: ç¼ºå¤±")
    
    if missing_pages:
        print(f"\nâš ï¸  å‘ç° {len(missing_pages)} ä¸ªé‡è¦é¡µé¢ç¼ºå¤±")
        print("å»ºè®®è¿è¡Œå¼ºåˆ¶æ£€æŸ¥æ¥è¡¥å……è¿™äº›é¡µé¢")
    else:
        print(f"\nâœ… æ‰€æœ‰é‡è¦é¡µé¢éƒ½å­˜åœ¨")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¼ºåˆ¶æ£€æŸ¥è„šæœ¬')
    parser.add_argument('--check-only', action='store_true', 
                       help='ä»…æ£€æŸ¥ï¼Œä¸æ‰§è¡Œæ›´æ–°')
    parser.add_argument('--check-specific', action='store_true',
                       help='æ£€æŸ¥ç‰¹å®šé‡è¦é¡µé¢')
    
    args = parser.parse_args()
    
    if args.check_specific:
        check_specific_pages()
    elif args.check_only:
        print("ğŸ” ä»…æ£€æŸ¥æ¨¡å¼...")
        scraper = OptimizedWikiScraper()
        print(f"é¡µé¢æ•°é‡: {len(scraper.all_content)}")
        print(f"å‘é‡æ•°é‡: {len([v for v in scraper.faiss_vectors if v is not None])}")
        print(f"URLå“ˆå¸Œæ•°é‡: {len(scraper.url_hashes)}")
    else:
        force_check_all_pages()

if __name__ == "__main__":
    main()
