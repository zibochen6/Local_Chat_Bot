import sys
import subprocess
import time
import os
import wave
import numpy as np
import signal
import tempfile
import threading
from collections import deque

# Signal handler for clean exit
def signal_handler(sig, frame):
    print('\nğŸ›‘ æ­£åœ¨é€€å‡º...')
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)

import sherpa_ncnn


def create_recognizer():
    # Please replace the model files if needed.
    # See https://k2-fsa.github.io/sherpa/ncnn/pretrained_models/index.html
    # for download links.
    print("ğŸ”„ æ­£åœ¨åŠ è½½è¯­éŸ³è¯†åˆ«æ¨¡å‹...")
    recognizer = sherpa_ncnn.Recognizer(
        tokens="/home/seeed/Local_Chat_Bot/audio/sherpa-ncnn/models/tokens.txt",
        encoder_param="/home/seeed/Local_Chat_Bot/audio/sherpa-ncnn/models/encoder_jit_trace-pnnx.ncnn.param",
        encoder_bin="/home/seeed/Local_Chat_Bot/audio/sherpa-ncnn/models/encoder_jit_trace-pnnx.ncnn.bin",
        decoder_param="/home/seeed/Local_Chat_Bot/audio/sherpa-ncnn/models/decoder_jit_trace-pnnx.ncnn.param",
        decoder_bin="/home/seeed/Local_Chat_Bot/audio/sherpa-ncnn/models/decoder_jit_trace-pnnx.ncnn.bin",
        joiner_param="/home/seeed/Local_Chat_Bot/audio/sherpa-ncnn/models/joiner_jit_trace-pnnx.ncnn.param",
        joiner_bin="/home/seeed/Local_Chat_Bot/audio/sherpa-ncnn/models/joiner_jit_trace-pnnx.ncnn.bin",
        num_threads=4,
    )
    print("âœ… è¯­éŸ³è¯†åˆ«æ¨¡å‹åŠ è½½å®Œæˆ")
    return recognizer


class SmartVoiceBot:
    def __init__(self):
        self.recognizer = create_recognizer()
        self.wake_word = "ä½ å¥½"
        self.is_listening = False
        self.conversation_buffer = deque(maxlen=20)  # ä¿å­˜æœ€è¿‘20æ®µéŸ³é¢‘
        self.silence_count = 0
        self.max_silence = 3  # 3æ¬¡é™é»˜åç»“æŸå¯¹è¯ (çº¦3ç§’)
        self.min_conversation_length = 1  # è‡³å°‘1ä¸ªéŸ³é¢‘æ®µå°±å¯ä»¥è€ƒè™‘ç»“æŸ
        self.audio_buffer = []  # éŸ³é¢‘ç¼“å†²åŒºï¼Œç”¨äºç´¯ç§¯éŸ³é¢‘
        self.has_speech = False  # æ ‡è®°æ˜¯å¦æ£€æµ‹åˆ°è¯­éŸ³
        self.last_speech_time = 0  # æœ€åä¸€æ¬¡æ£€æµ‹åˆ°è¯­éŸ³çš„æ—¶é—´
        self.has_meaningful_content = False  # æ˜¯å¦å·²ç»è·å¾—æœ‰æ„ä¹‰çš„å†…å®¹
        
    def record_chunk(self, duration=1):
        """å½•åˆ¶éŸ³é¢‘ç‰‡æ®µ"""
        temp_file = f"/tmp/voice_chunk_{int(time.time() * 1000)}.wav"
        
        try:
            cmd = [
                "arecord", 
                "-D", "sysdefault",
                "-f", "S16_LE", 
                "-r", "16000",
                "-c", "1",
                "-d", str(duration),
                temp_file
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=duration + 1)
            
            if result.returncode != 0:
                return None, 0
                
            # Load audio
            with wave.open(temp_file, 'rb') as f:
                frames = f.readframes(f.getnframes())
                audio_data = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32768.0
            
            audio_level = np.abs(audio_data).max()
            return audio_data, audio_level
            
        except Exception as e:
            return None, 0
        finally:
            if os.path.exists(temp_file):
                os.remove(temp_file)
    
    def recognize_audio(self, audio_data):
        """è¯†åˆ«éŸ³é¢‘æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # éŸ³é¢‘é¢„å¤„ç†
            if len(audio_data) < 16000:  # å°‘äº1ç§’çš„éŸ³é¢‘ï¼Œè¯†åˆ«æ•ˆæœå¯èƒ½ä¸å¥½
                return ""
            
            # æ£€æŸ¥éŸ³é¢‘ç”µå¹³
            audio_max = np.abs(audio_data).max()
            if audio_max < 0.01:  # éŸ³é¢‘å¤ªå°ï¼Œå¯èƒ½æ˜¯å™ªéŸ³
                return ""
            
            # éŸ³é¢‘å½’ä¸€åŒ– - æ›´ä¿å®ˆçš„å½’ä¸€åŒ–
            if audio_max > 0:
                # å½’ä¸€åŒ–åˆ°åˆç†èŒƒå›´ï¼Œé¿å…è¿‡åº¦æ”¾å¤§
                if audio_max < 0.1:
                    audio_data = audio_data / audio_max * 0.1
                elif audio_max > 0.8:
                    audio_data = audio_data / audio_max * 0.8
                # å¦‚æœåœ¨åˆç†èŒƒå›´å†…ï¼Œä¿æŒåŸæ ·
            
            # Create fresh recognizer
            model_dir = "/home/seeed/Local_Chat_Bot/audio/sherpa-ncnn/models"
            recognizer = sherpa_ncnn.Recognizer(
                tokens=f"{model_dir}/tokens.txt",
                encoder_param=f"{model_dir}/encoder_jit_trace-pnnx.ncnn.param",
                encoder_bin=f"{model_dir}/encoder_jit_trace-pnnx.ncnn.bin",
                decoder_param=f"{model_dir}/decoder_jit_trace-pnnx.ncnn.param",
                decoder_bin=f"{model_dir}/decoder_jit_trace-pnnx.ncnn.bin",
                joiner_param=f"{model_dir}/joiner_jit_trace-pnnx.ncnn.param",
                joiner_bin=f"{model_dir}/joiner_jit_trace-pnnx.ncnn.bin",
                num_threads=4,
            )
            
            # ç›´æ¥å¤„ç†å®Œæ•´éŸ³é¢‘ï¼Œä¸åˆ†æ®µ
            recognizer.accept_waveform(16000, audio_data)
            
            # Add silence to finalize
            silence = np.zeros(int(0.8 * 16000), dtype=np.float32)
            recognizer.accept_waveform(16000, silence)
            recognizer.input_finished()
            
            result = recognizer.text.strip()
            
            # åå¤„ç†ï¼šå»é™¤æ˜æ˜¾çš„è¯†åˆ«é”™è¯¯
            if len(result) < 1:  # ç©ºç»“æœ
                return ""
                
            return result
            
        except Exception as e:
            print(f"è¯†åˆ«é”™è¯¯: {e}")
            return ""
    
    def detect_wake_word(self, text):
        """æ£€æµ‹å”¤é†’è¯"""
        return self.wake_word in text
    
    def is_conversation_end(self):
        """æ™ºèƒ½åˆ¤æ–­å¯¹è¯æ˜¯å¦ç»“æŸ"""
        current_time = time.time()
        
        # å¦‚æœå·²ç»æœ‰æœ‰æ„ä¹‰çš„å†…å®¹ï¼Œä½¿ç”¨æ›´çŸ­çš„ç­‰å¾…æ—¶é—´
        if self.has_meaningful_content:
            # æœ‰å†…å®¹åï¼Œ2ç§’é™é»˜å°±ç»“æŸ
            if self.silence_count >= 2:
                print(f"ğŸ¯ å·²è·å¾—å®Œæ•´å†…å®¹ï¼Œ{self.silence_count}ç§’é™é»˜åç»“æŸå¯¹è¯")
                return True
        else:
            # æ²¡æœ‰å†…å®¹æ—¶ï¼Œç­‰å¾…ç¨é•¿ä¸€äº›
            if self.silence_count >= self.max_silence:
                print(f"ğŸ”‡ æ£€æµ‹åˆ°è¿ç»­{self.silence_count}æ¬¡é™é»˜ï¼Œå‡†å¤‡ç»“æŸå¯¹è¯...")
                return True
            
        # æ£€æŸ¥ç»“æŸè¯
        if len(self.conversation_buffer) > 0:
            recent_text = " ".join([item["text"] for item in list(self.conversation_buffer)[-2:]])
            end_phrases = ["è°¢è°¢", "å†è§", "å¥½çš„", "çŸ¥é“äº†", "æ˜ç™½äº†", "å®Œæ¯•", "ç»“æŸ", "æ²¡äº†", "å°±è¿™æ ·"]
            
            for phrase in end_phrases:
                if phrase in recent_text:
                    print(f"ğŸ”š æ£€æµ‹åˆ°ç»“æŸè¯ '{phrase}'")
                    return True
        
        # å¦‚æœè¶…è¿‡15ç§’æ²¡æœ‰æ–°çš„æœ‰æ•ˆè¯­éŸ³ï¼Œå¼ºåˆ¶ç»“æŸ
        if self.last_speech_time > 0 and (current_time - self.last_speech_time) > 15:
            print("â° è¶…æ—¶å¼ºåˆ¶ç»“æŸå¯¹è¯")
            return True
                
        return False
    
    def continuous_listen(self):
        """è¿ç»­ç›‘å¬æ¨¡å¼"""
        print("ğŸ”„ ç›‘å¬å”¤é†’è¯ä¸­...")
        
        while True:
            audio_data, audio_level = self.record_chunk(duration=1)
            
            if audio_data is not None and audio_level > 0.02:
                text = self.recognize_audio(audio_data)
                
                if text and self.detect_wake_word(text):
                    print(f"ğŸ¯ æ£€æµ‹åˆ°å”¤é†’è¯: '{text}'")
                    print("ğŸ‘‚ å¼€å§‹å€¾å¬æ‚¨çš„é—®é¢˜...")
                    self.start_conversation()
                    print("ğŸ”„ é‡æ–°ç›‘å¬å”¤é†’è¯ä¸­...")
            
            time.sleep(0.1)
    
    def start_conversation(self):
        """å¼€å§‹å¯¹è¯æ¨¡å¼ - æ™ºèƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        self.is_listening = True
        self.conversation_buffer.clear()
        self.silence_count = 0
        self.audio_buffer = []
        self.has_speech = False
        self.last_speech_time = 0
        self.has_meaningful_content = False
        
        full_conversation = []
        continuous_audio = []  # è¿ç»­éŸ³é¢‘ç‰‡æ®µ
        speech_detected = False
        
        print("ğŸ’¬ å¯¹è¯å¼€å§‹ (æ™ºèƒ½å¿«é€Ÿç»“æŸ)")
        print("-" * 40)
        
        while self.is_listening:
            audio_data, audio_level = self.record_chunk(duration=1)
            
            if audio_data is not None:
                if audio_level > 0.02:  # æœ‰å£°éŸ³ (æé«˜é˜ˆå€¼ï¼Œå‡å°‘å™ªéŸ³å¹²æ‰°)
                    continuous_audio.append(audio_data)
                    speech_detected = True
                    self.silence_count = 0  # é‡ç½®é™é»˜è®¡æ•°
                    self.last_speech_time = time.time()  # æ›´æ–°æœ€åè¯­éŸ³æ—¶é—´
                    print("ğŸµ", end="", flush=True)  # æ˜¾ç¤ºå½•éŸ³è¿›åº¦
                    
                else:
                    # é™é»˜
                    if speech_detected and len(continuous_audio) > 1:  # éœ€è¦è‡³å°‘2æ®µéŸ³é¢‘
                        # æœ‰è¯­éŸ³åçš„é™é»˜ï¼Œå¤„ç†ç´¯ç§¯çš„éŸ³é¢‘
                        combined_audio = np.concatenate(continuous_audio)
                        
                        # æ˜¾ç¤ºéŸ³é¢‘ä¿¡æ¯ç”¨äºè°ƒè¯•
                        audio_level = np.abs(combined_audio).max()
                        print(f"\nğŸ“Š å¤„ç†éŸ³é¢‘: {len(combined_audio)} é‡‡æ ·ç‚¹ ({len(combined_audio)/16000:.1f}ç§’), ç”µå¹³:{audio_level:.3f}")
                        
                        text = self.recognize_audio(combined_audio)
                        
                        if text:
                            print(f"ğŸ—£ï¸  '{text}'")
                            full_conversation.append(text)
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ„ä¹‰çš„å†…å®¹
                            if self.is_meaningful_content(text):
                                self.has_meaningful_content = True
                                print("âœ¨ æ£€æµ‹åˆ°å®Œæ•´é—®é¢˜")
                            
                            self.conversation_buffer.append({
                                "text": text,
                                "level": audio_level,
                                "timestamp": time.time()
                            })
                        else:
                            print("ğŸ”‡ æ­¤æ®µéŸ³é¢‘æœªè¯†åˆ«åˆ°æ–‡æœ¬")
                        
                        # æ¸…ç©ºéŸ³é¢‘ç¼“å†²
                        continuous_audio = []
                        speech_detected = False
                    
                    self.silence_count += 1
                    print(".", end="", flush=True)  # æ˜¾ç¤ºé™é»˜è¿›åº¦
                    
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»“æŸå¯¹è¯
                if self.is_conversation_end():
                    # å¤„ç†å‰©ä½™çš„éŸ³é¢‘
                    if len(continuous_audio) > 0:
                        combined_audio = np.concatenate(continuous_audio)
                        text = self.recognize_audio(combined_audio)
                        if text:
                            print(f"\nğŸ—£ï¸  '{text}'")
                            full_conversation.append(text)
                    
                    self.end_conversation(full_conversation)
                    break
            
            time.sleep(0.1)
    
    def is_meaningful_content(self, text):
        """åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ„ä¹‰çš„å®Œæ•´å†…å®¹"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç–‘é—®è¯ï¼Œå¯èƒ½æ˜¯å®Œæ•´é—®é¢˜
        question_words = ["ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å“ªé‡Œ", "å“ªä¸ª", "å‡ ", "å¤šå°‘", "è°", "å—", "å‘¢", "?", "ï¼Ÿ"]
        for word in question_words:
            if word in text:
                return True
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„çŸ­è¯­æˆ–å¥å­ (é•¿åº¦å¤§äº2ä¸ªå­—ç¬¦)
        if len(text.strip()) >= 3:
            return True
            
        return False
    
    def end_conversation(self, full_conversation):
        """ç»“æŸå¯¹è¯"""
        self.is_listening = False
        
        print("\n" + "-" * 40)
        print("ğŸ”š å¯¹è¯ç»“æŸ")
        
        if full_conversation:
            complete_question = " ".join(full_conversation)
            print(f"ğŸ“ å®Œæ•´é—®é¢˜: '{complete_question}'")
            print(f"ğŸ’­ è¿™é‡Œå¯ä»¥è°ƒç”¨AIæ¨¡å‹å›ç­”é—®é¢˜...")
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ AIå›ç­”é€»è¾‘
            # response = call_ai_model(complete_question)
            # print(f"ğŸ¤– AIå›ç­”: {response}")
        else:
            print("ğŸ˜… æ²¡æœ‰æ£€æµ‹åˆ°å®Œæ•´é—®é¢˜")
        
        print("ğŸ”„ è¿”å›å¾…æœºçŠ¶æ€ï¼Œç­‰å¾…å”¤é†’è¯...\n")

def main():
    print("ğŸ¤– æ™ºèƒ½è¯­éŸ³å¯¹è¯æœºå™¨äºº")
    print("ğŸ’¡ å”¤é†’è¯: 'ä½ å¥½'")
    print("ğŸ¯ åŠŸèƒ½: æ™ºèƒ½æ£€æµ‹å¯¹è¯å¼€å§‹å’Œç»“æŸ")
    
    # Create recognizer to test
    print("ğŸ“± æ­£åœ¨åŠ è½½è¯­éŸ³è¯†åˆ«æ¨¡å‹...")
    test_recognizer = create_recognizer()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # Initialize voice bot
    bot = SmartVoiceBot()
    
    print("\nğŸ™ï¸ å¼€å§‹ç›‘å¬... (æŒ‰ Ctrl+C é€€å‡º)")
    print("ğŸ’¬ è¯·è¯´'ä½ å¥½'æ¥å”¤é†’æœºå™¨äºº")
    print("=" * 50)
    
    try:
        bot.continuous_listen()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·é€€å‡º")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()




if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç¨‹åºé€€å‡º")
    except Exception as e:
        print(f"âŒ ç¨‹åºé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()