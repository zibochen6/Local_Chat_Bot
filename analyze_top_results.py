#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æå‰å‡ ä¸ªé«˜æ’åé¡µé¢çš„å†…å®¹
"""

import json
import pickle
import numpy as np
import faiss
import ollama

def analyze_top_results():
    """åˆ†æå‰å‡ ä¸ªé«˜æ’åé¡µé¢çš„å†…å®¹"""
    print("ğŸ” åˆ†æå‰å‡ ä¸ªé«˜æ’åé¡µé¢çš„å†…å®¹...")
    
    # åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    
    # åŠ è½½FAISSç´¢å¼•
    faiss_index = faiss.read_index("./data_base/faiss_index.bin")
    print(f"âœ… FAISSç´¢å¼•åŠ è½½å®Œæˆ: {faiss_index.ntotal} ä¸ªå‘é‡")
    
    # åŠ è½½å…ƒæ•°æ®
    with open("./data_base/faiss_metadata.pkl", 'rb') as f:
        faiss_metadata = pickle.load(f)
    print(f"âœ… å…ƒæ•°æ®åŠ è½½å®Œæˆ: {len(faiss_metadata)} æ¡è®°å½•")
    
    # åŠ è½½é¡µé¢æ•°æ®
    with open("./data_base/seeed_wiki_embeddings_db.json", 'r', encoding='utf-8') as f:
        data = json.load(f)
        wiki_pages = data.get('pages', [])
    print(f"âœ… é¡µé¢æ•°æ®åŠ è½½å®Œæˆ: {len(wiki_pages)} ä¸ªé¡µé¢")
    
    # æ‰¾åˆ°çŸ½é€’ç§‘æŠ€é¡µé¢
    target_url = 'https://wiki.seeedstudio.com/cn/Getting_Started/'
    target_idx = None
    for i, page in enumerate(wiki_pages):
        if page.get('url') == target_url:
            target_idx = i
            break
    
    if target_idx is None:
        print("âŒ æœªæ‰¾åˆ°çŸ½é€’ç§‘æŠ€é¡µé¢")
        return
    
    print(f"âœ… æ‰¾åˆ°çŸ½é€’ç§‘æŠ€é¡µé¢ï¼Œç´¢å¼•: {target_idx}")
    
    # æµ‹è¯•æŸ¥è¯¢
    query = "ä»‹ç»ä¸€ä¸‹çŸ½é€’ç§‘æŠ€"
    print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: '{query}'")
    
    try:
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        response = ollama.embeddings(model='nomic-embed-text', prompt=query)
        query_embedding = np.array(response['embedding'], dtype=np.float32)
        query_embedding = query_embedding / np.linalg.norm(query_embedding)
        
        # æœç´¢å‰100ä¸ªç»“æœ
        query_embedding_reshaped = query_embedding.reshape(1, -1)
        scores, indices = faiss_index.search(query_embedding_reshaped, 100)
        
        # æŸ¥æ‰¾ç›®æ ‡é¡µé¢åœ¨æœç´¢ç»“æœä¸­çš„ä½ç½®
        target_rank = None
        for i, idx in enumerate(indices[0]):
            if idx == target_idx:
                target_rank = i + 1
                break
        
        if target_rank:
            print(f"âœ… ç›®æ ‡é¡µé¢æ’å: {target_rank}")
            print(f"âœ… ç›®æ ‡é¡µé¢ç›¸å…³åº¦: {scores[0][target_rank-1]:.6f}")
        else:
            print("âŒ ç›®æ ‡é¡µé¢åœ¨å‰100ä¸ªç»“æœä¸­æœªæ‰¾åˆ°")
            return
        
        # åˆ†æå‰10ä¸ªç»“æœ
        print(f"\nğŸ“Š åˆ†æå‰10ä¸ªç»“æœ:")
        for i in range(10):
            idx = indices[0][i]
            score = scores[0][i]
            if idx < len(wiki_pages):
                page = wiki_pages[idx]
                title = page.get('title', 'N/A')
                url = page.get('url', 'N/A')
                content = page.get('content', '')
                
                print(f"\n{i+1}. ç›¸å…³åº¦: {score:.6f}")
                print(f"   æ ‡é¢˜: {title}")
                print(f"   URL: {url}")
                print(f"   å†…å®¹é¢„è§ˆ: {content[:300]}...")
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                keywords = ['çŸ½é€’', 'ç§‘æŠ€', 'å…¬å¸', 'ä»‹ç»', 'ç®€ä»‹', 'å…³äº']
                found_keywords = []
                for keyword in keywords:
                    if keyword in content:
                        found_keywords.append(keyword)
                
                if found_keywords:
                    print(f"   åŒ…å«å…³é”®è¯: {', '.join(found_keywords)}")
                else:
                    print(f"   æœªåŒ…å«å¸¸è§å…³é”®è¯")
        
        # åˆ†æçŸ½é€’ç§‘æŠ€é¡µé¢
        print(f"\nğŸ“„ åˆ†æçŸ½é€’ç§‘æŠ€é¡µé¢:")
        target_page = wiki_pages[target_idx]
        target_title = target_page.get('title', 'N/A')
        target_url = target_page.get('url', 'N/A')
        target_content = target_page.get('content', '')
        
        print(f"   æ ‡é¢˜: {target_title}")
        print(f"   URL: {target_url}")
        print(f"   å†…å®¹é¢„è§ˆ: {target_content[:500]}...")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
        keywords = ['çŸ½é€’', 'ç§‘æŠ€', 'å…¬å¸', 'ä»‹ç»', 'ç®€ä»‹', 'å…³äº']
        found_keywords = []
        for keyword in keywords:
            if keyword in target_content:
                found_keywords.append(keyword)
        
        if found_keywords:
            print(f"   åŒ…å«å…³é”®è¯: {', '.join(found_keywords)}")
        else:
            print(f"   æœªåŒ…å«å¸¸è§å…³é”®è¯")
        
        # åˆ†ææŸ¥è¯¢å‘é‡
        print(f"\nğŸ” åˆ†ææŸ¥è¯¢å‘é‡:")
        print(f"   æŸ¥è¯¢: '{query}'")
        print(f"   æŸ¥è¯¢å‘é‡èŒƒæ•°: {np.linalg.norm(query_embedding):.6f}")
        
        # æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦åŒ…å«å…³é”®è¯
        query_keywords = []
        for keyword in keywords:
            if keyword in query:
                query_keywords.append(keyword)
        
        if query_keywords:
            print(f"   æŸ¥è¯¢åŒ…å«å…³é”®è¯: {', '.join(query_keywords)}")
        else:
            print(f"   æŸ¥è¯¢æœªåŒ…å«å¸¸è§å…³é”®è¯")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("\nğŸ‰ åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    analyze_top_results()
