#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆéŸ³é¢‘è½¬æ–‡å­—è„šæœ¬
ä½¿ç”¨ OpenAI Whisper å¤„ç†æœ¬åœ° demo.wav æ–‡ä»¶
æ”¯æŒ GPU åŠ é€Ÿæ¨ç†
"""

import os
import whisper
import torch

def check_gpu_support():
    """æ£€æŸ¥ GPU æ”¯æŒæƒ…å†µ"""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸš€ GPU åŠ é€Ÿå¯ç”¨: {gpu_name}")
        print(f"ğŸ’¾ GPU æ˜¾å­˜: {gpu_memory:.1f} GB")
        return True
    else:
        print("âš ï¸  GPU ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU æ¨¡å¼")
        return False

def main():
    # æŒ‡å®šæœ¬åœ°æµ‹è¯•éŸ³é¢‘æ–‡ä»¶
    filename = "demo.wav"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(filename):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ '{filename}'")
        print("è¯·ç¡®ä¿ demo.wav æ–‡ä»¶å­˜åœ¨äºå½“å‰ç›®å½•ä¸­")
        return
    
    print(f"ğŸ“ æ­£åœ¨å¤„ç†éŸ³é¢‘æ–‡ä»¶: {filename}")
    
    # æ£€æŸ¥ GPU æ”¯æŒ
    use_gpu = check_gpu_support()
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ¤– æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹...")
    try:
        if use_gpu:
            # GPU æ¨¡å¼ï¼šä½¿ç”¨ medium æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
            print("ğŸ”¥ ä½¿ç”¨ GPU åŠ é€Ÿæ¨¡å¼")
            #medium
            model = whisper.load_model("base")
            # å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU
            model = model.to("cuda")
        else:
            # CPU æ¨¡å¼ï¼šä½¿ç”¨ base æ¨¡å‹ä»¥èŠ‚çœå†…å­˜
            print("ğŸ’» ä½¿ç”¨ CPU æ¨¡å¼")
            model = whisper.load_model("base")
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return
    
    # è½¬æ¢éŸ³é¢‘
    print("ğŸ”„ æ­£åœ¨è½¬æ¢è¯­éŸ³...")
    try:
        # è®¾ç½®æ¨ç†å‚æ•°
        if use_gpu:
            # GPU æ¨¡å¼ï¼šä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°ä»¥æé«˜æ€§èƒ½
            result = model.transcribe(filename, fp16=True)
        else:
            # CPU æ¨¡å¼ï¼šä½¿ç”¨é»˜è®¤è®¾ç½®
            result = model.transcribe(filename)
        
        print(f"ğŸ” æ£€æµ‹è¯­è¨€: {result['language']}")
        print(f"ğŸ“ è½¬æ¢ç»“æœ:")
        print("-" * 50)
        print(result["text"])
        print("-" * 50)
        print("âœ… è¯­éŸ³è½¬æ–‡å­—å®Œæˆ")
        
        # å¦‚æœæœ‰åˆ†æ®µä¿¡æ¯ï¼Œæ˜¾ç¤ºè¯¦ç»†æ—¶é—´æˆ³
        if 'segments' in result:
            print("\nğŸ“Š è¯¦ç»†åˆ†æ®µä¿¡æ¯:")
            for segment in result['segments']:
                start = segment['start']
                end = segment['end']
                text = segment['text']
                print(f"[{start:.2f}s -> {end:.2f}s] {text}")
        
        # æ˜¾ç¤ºæ€§èƒ½ä¿¡æ¯
        if use_gpu:
            gpu_memory_used = torch.cuda.memory_allocated() / 1024**3
            gpu_memory_cached = torch.cuda.memory_reserved() / 1024**3
            print(f"\nâš¡ GPU æ€§èƒ½ä¿¡æ¯:")
            print(f"   æ˜¾å­˜ä½¿ç”¨: {gpu_memory_used:.2f} GB")
            print(f"   æ˜¾å­˜ç¼“å­˜: {gpu_memory_cached:.2f} GB")
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {str(e)}")
    finally:
        # æ¸…ç† GPU å†…å­˜
        if use_gpu:
            torch.cuda.empty_cache()

if __name__ == "__main__":
    main()
