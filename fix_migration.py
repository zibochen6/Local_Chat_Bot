#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¿ç§»é—®é¢˜ä¿®å¤è„šæœ¬
è§£å†³æ–°è®¾å¤‡ä¸Šè¿è¡Œ optimized_qa.py çš„é—®é¢˜
"""

import os
import subprocess
import sys
import json

def run_command(command, description):
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºç»“æœ"""
    print(f"\nğŸ”§ {description}...")
    print(f"   æ‰§è¡Œå‘½ä»¤: {command}")
    
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True)
        if result.returncode == 0:
            print(f"   âœ… æˆåŠŸ: {result.stdout.strip()}")
            return True
        else:
            print(f"   âŒ å¤±è´¥: {result.stderr.strip()}")
            return False
    except Exception as e:
        print(f"   âŒ æ‰§è¡Œå¼‚å¸¸: {str(e)}")
        return False

def check_ollama_status():
    """æ£€æŸ¥ Ollama çŠ¶æ€"""
    print("ğŸ” æ£€æŸ¥ Ollama çŠ¶æ€...")
    
    # æ£€æŸ¥ Ollama æ˜¯å¦å®‰è£…
    if not run_command("which ollama", "æ£€æŸ¥ Ollama å®‰è£…"):
        print("âŒ Ollama æœªå®‰è£…")
        return False
    
    # æ£€æŸ¥ Ollama ç‰ˆæœ¬
    if not run_command("ollama --version", "æ£€æŸ¥ Ollama ç‰ˆæœ¬"):
        print("âŒ Ollama ç‰ˆæœ¬æ£€æŸ¥å¤±è´¥")
        return False
    
    # æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€
    if not run_command("ollama list", "æ£€æŸ¥ Ollama æœåŠ¡"):
        print("âŒ Ollama æœåŠ¡æœªè¿è¡Œ")
        return False
    
    return True

def install_embedding_models():
    """å®‰è£… embedding æ¨¡å‹"""
    print("\nğŸ”§ å®‰è£… embedding æ¨¡å‹...")
    
    models_to_install = [
        "nomic-embed-text",
        "all-MiniLM-L6-v2", 
        "text-embedding-ada-002"
    ]
    
    installed_models = []
    
    for model in models_to_install:
        print(f"\nğŸ” å®‰è£…æ¨¡å‹: {model}")
        if run_command(f"ollama pull {model}", f"å®‰è£… {model}"):
            installed_models.append(model)
        else:
            print(f"âš ï¸  {model} å®‰è£…å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª...")
    
    return installed_models

def verify_models():
    """éªŒè¯æ¨¡å‹å¯ç”¨æ€§"""
    print("\nğŸ” éªŒè¯æ¨¡å‹å¯ç”¨æ€§...")
    
    try:
        import ollama
        import numpy as np
        
        models = ollama.list()
        model_names = [model['name'] for model in models['models']]
        
        print(f"   å¯ç”¨æ¨¡å‹: {model_names}")
        
        # æµ‹è¯• embedding æ¨¡å‹
        working_models = []
        for model_name in model_names:
            if 'embed' in model_name.lower() or 'nomic' in model_name.lower():
                try:
                    response = ollama.embeddings(model=model_name, prompt="test")
                    if "embedding" in response and len(response["embedding"]) > 0:
                        embedding = np.array(response["embedding"], dtype=np.float32)
                        norm = np.linalg.norm(embedding)
                        if norm > 0:
                            working_models.append(model_name)
                            print(f"   âœ… {model_name}: èŒƒæ•° {norm:.6f}")
                        else:
                            print(f"   âŒ {model_name}: èŒƒæ•°ä¸º 0")
                    else:
                        print(f"   âŒ {model_name}: å“åº”æ ¼å¼é”™è¯¯")
                except Exception as e:
                    print(f"   âŒ {model_name}: {str(e)}")
        
        return working_models
        
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘å¿…è¦åº“: {str(e)}")
        return []
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {str(e)}")
        return []

def check_data_files():
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶"""
    print("\nğŸ” æ£€æŸ¥æ•°æ®æ–‡ä»¶...")
    
    required_files = [
        "./data_base/faiss_index.bin",
        "./data_base/faiss_metadata.pkl",
        "./data_base/seeed_wiki_embeddings_db.json"
    ]
    
    missing_files = []
    for file in required_files:
        if os.path.exists(file):
            size = os.path.getsize(file)
            print(f"   âœ… {file}: {size} å­—èŠ‚")
        else:
            print(f"   âŒ {file}: æ–‡ä»¶ä¸å­˜åœ¨")
            missing_files.append(file)
    
    if missing_files:
        print(f"\nâš ï¸  ç¼ºå°‘æ–‡ä»¶: {missing_files}")
        return False
    
    return True

def create_backup_config():
    """åˆ›å»ºå¤‡ç”¨é…ç½®"""
    print("\nğŸ”§ åˆ›å»ºå¤‡ç”¨é…ç½®...")
    
    config = {
        "embedding_model": "nomic-embed-text:latest",
        "fallback_models": [
            "all-MiniLM-L6-v2",
            "text-embedding-ada-002"
        ],
        "max_retries": 3,
        "debug_mode": True
    }
    
    try:
        with open("migration_config.json", "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print("   âœ… é…ç½®æ–‡ä»¶åˆ›å»ºæˆåŠŸ: migration_config.json")
        return True
    except Exception as e:
        print(f"   âŒ é…ç½®æ–‡ä»¶åˆ›å»ºå¤±è´¥: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è¿ç§»é—®é¢˜ä¿®å¤å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥ Ollama çŠ¶æ€
    if not check_ollama_status():
        print("\nğŸ’¡ è¯·å…ˆå®‰è£…å¹¶å¯åŠ¨ Ollama:")
        print("   1. å®‰è£…: curl -fsSL https://ollama.ai/install.sh | sh")
        print("   2. å¯åŠ¨: ollama serve")
        return
    
    # å®‰è£… embedding æ¨¡å‹
    installed_models = install_embedding_models()
    
    if not installed_models:
        print("âŒ æ²¡æœ‰æˆåŠŸå®‰è£…ä»»ä½• embedding æ¨¡å‹")
        return
    
    # éªŒè¯æ¨¡å‹
    working_models = verify_models()
    
    if not working_models:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„ embedding æ¨¡å‹")
        return
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not check_data_files():
        print("âŒ æ•°æ®æ–‡ä»¶ä¸å®Œæ•´")
        return
    
    # åˆ›å»ºé…ç½®
    create_backup_config()
    
    print(f"\nğŸ‰ ä¿®å¤å®Œæˆï¼")
    print(f"âœ… å¯ç”¨æ¨¡å‹: {working_models}")
    print(f"\nğŸ’¡ ç°åœ¨å¯ä»¥å°è¯•è¿è¡Œ:")
    print(f"   python optimized_qa.py")
    print(f"\nğŸ’¡ å¦‚æœä»æœ‰é—®é¢˜ï¼Œè¯·è¿è¡Œè¯Šæ–­å·¥å…·:")
    print(f"   python test_ollama.py")

if __name__ == "__main__":
    main()
