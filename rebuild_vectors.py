#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡å»ºå‘é‡æ•°æ®è„šæœ¬
ä¸“é—¨ç”¨äºä¿®å¤æŸåçš„å‘é‡æ•°æ®
"""

import os
import json
import pickle
import numpy as np
import faiss
import ollama
from datetime import datetime

def rebuild_vectors():
    """é‡å»ºæ‰€æœ‰å‘é‡æ•°æ®"""
    print("ğŸ”§ å¼€å§‹é‡å»ºå‘é‡æ•°æ®...")
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_dir = "./data_base"
    db_file = f"{data_dir}/seeed_wiki_embeddings_db.json"
    faiss_index_file = f"{data_dir}/faiss_index.bin"
    faiss_metadata_file = f"{data_dir}/faiss_metadata.pkl"
    
    # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶
    if not os.path.exists(db_file):
        print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    # åŠ è½½é¡µé¢æ•°æ®
    print("ğŸ“‚ åŠ è½½é¡µé¢æ•°æ®...")
    with open(db_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
        pages = data.get('pages', [])
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(pages)} ä¸ªé¡µé¢")
    
    # æ£€æŸ¥ Ollama æœåŠ¡
    print("ğŸ” æ£€æŸ¥ Ollama æœåŠ¡...")
    try:
        models = ollama.list()
        if 'nomic-embed-text' not in [m['name'] for m in models['models']]:
            print("ğŸ“¥ å®‰è£… nomic-embed-text æ¨¡å‹...")
            ollama.pull('nomic-embed-text')
        print("âœ… Ollama æœåŠ¡æ­£å¸¸")
    except Exception as e:
        print(f"âŒ Ollama æœåŠ¡æ£€æŸ¥å¤±è´¥: {e}")
        return
    
    # æµ‹è¯• Embedding ç”Ÿæˆ
    print("ğŸ§  æµ‹è¯• Embedding ç”Ÿæˆ...")
    try:
        test_response = ollama.embeddings(model='nomic-embed-text', prompt='test')
        dimension = len(test_response['embedding'])
        print(f"âœ… Embedding æµ‹è¯•æˆåŠŸï¼Œç»´åº¦: {dimension}")
    except Exception as e:
        print(f"âŒ Embedding æµ‹è¯•å¤±è´¥: {e}")
        return
    
    # é‡å»ºå‘é‡æ•°æ®
    print("ğŸ”„ å¼€å§‹é‡å»ºå‘é‡æ•°æ®...")
    vectors = []
    metadata = []
    failed_pages = []
    
    for i, page in enumerate(pages):
        try:
            content = page.get('content', '')
            if not content or len(content) < 10:
                print(f"âš ï¸  é¡µé¢ {i+1}/{len(pages)}: å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡")
                failed_pages.append(page.get('url', f'page_{i}'))
                continue
            
            # ç”Ÿæˆ Embedding
            response = ollama.embeddings(model='nomic-embed-text', prompt=content)
            embedding = np.array(response['embedding'], dtype=np.float32)
            
            # å½’ä¸€åŒ–å‘é‡
            embedding = embedding / np.linalg.norm(embedding)
            
            vectors.append(embedding)
            metadata.append({
                'title': page.get('title', ''),
                'url': page.get('url', ''),
                'content_length': len(content),
                'timestamp': datetime.now().isoformat(),
                'language': page.get('language', 'Unknown')
            })
            
            if (i + 1) % 100 == 0:
                print(f"âœ… å·²å¤„ç† {i+1}/{len(pages)} ä¸ªé¡µé¢")
                
        except Exception as e:
            print(f"âŒ é¡µé¢ {i+1}/{len(pages)} å¤„ç†å¤±è´¥: {e}")
            failed_pages.append(page.get('url', f'page_{i}'))
    
    print(f"\nğŸ“Š å‘é‡é‡å»ºå®Œæˆ:")
    print(f"   - æˆåŠŸ: {len(vectors)} ä¸ª")
    print(f"   - å¤±è´¥: {len(failed_pages)} ä¸ª")
    
    if not vectors:
        print("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•å‘é‡")
        return
    
    # æ„å»º FAISS ç´¢å¼•
    print("ğŸ” æ„å»º FAISS ç´¢å¼•...")
    try:
        vectors_array = np.array(vectors, dtype=np.float32)
        faiss_index = faiss.IndexFlatIP(dimension)
        faiss_index.add(vectors_array)
        
        print(f"âœ… FAISS ç´¢å¼•æ„å»ºå®Œæˆ: {faiss_index.ntotal} ä¸ªå‘é‡")
    except Exception as e:
        print(f"âŒ FAISS ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
        return
    
    # ä¿å­˜æ•°æ®
    print("ğŸ’¾ ä¿å­˜æ•°æ®...")
    try:
        # ä¿å­˜ FAISS ç´¢å¼•
        faiss.write_index(faiss_index, faiss_index_file)
        print(f"âœ… FAISS ç´¢å¼•å·²ä¿å­˜: {faiss_index_file}")
        
        # ä¿å­˜å…ƒæ•°æ®
        with open(faiss_metadata_file, 'wb') as f:
            pickle.dump(metadata, f)
        print(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {faiss_metadata_file}")
        
        # æ›´æ–°æ•°æ®åº“æ–‡ä»¶
        data['metadata'].update({
            'total_vectors': len(vectors),
            'vector_dimension': dimension,
            'last_update': datetime.now().isoformat(),
            'rebuild_time': datetime.now().isoformat()
        })
        
        with open(db_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ•°æ®åº“å·²æ›´æ–°: {db_file}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
        return
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ é‡å»ºç»Ÿè®¡:")
    print(f"   - æ€»é¡µé¢æ•°: {len(pages)}")
    print(f"   - æˆåŠŸå‘é‡: {len(vectors)}")
    print(f"   - å¤±è´¥é¡µé¢: {len(failed_pages)}")
    print(f"   - å‘é‡ç»´åº¦: {dimension}")
    
    if failed_pages:
        print(f"\nâš ï¸  å¤±è´¥çš„é¡µé¢:")
        for url in failed_pages[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"   - {url}")
        if len(failed_pages) > 10:
            print(f"   ... è¿˜æœ‰ {len(failed_pages) - 10} ä¸ª")
    
    print(f"\nğŸ‰ å‘é‡é‡å»ºå®Œæˆï¼")
    print(f"ğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨ä¼˜åŒ–ç‰ˆé—®ç­”ç³»ç»Ÿäº†")

def check_vectors():
    """æ£€æŸ¥å‘é‡æ•°æ®çŠ¶æ€"""
    print("ğŸ” æ£€æŸ¥å‘é‡æ•°æ®çŠ¶æ€...")
    
    data_dir = "./data_base"
    db_file = f"{data_dir}/seeed_wiki_embeddings_db.json"
    faiss_index_file = f"{data_dir}/faiss_index.bin"
    faiss_metadata_file = f"{data_dir}/faiss_metadata.pkl"
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
    print("ğŸ“ æ–‡ä»¶æ£€æŸ¥:")
    print(f"   æ•°æ®åº“æ–‡ä»¶: {'âœ…' if os.path.exists(db_file) else 'âŒ'}")
    print(f"   FAISSç´¢å¼•: {'âœ…' if os.path.exists(faiss_index_file) else 'âŒ'}")
    print(f"   å…ƒæ•°æ®æ–‡ä»¶: {'âœ…' if os.path.exists(faiss_metadata_file) else 'âŒ'}")
    
    if not os.path.exists(db_file):
        print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    # åŠ è½½æ•°æ®
    try:
        with open(db_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            pages = data.get('pages', [])
            metadata = data.get('metadata', {})
        
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   é¡µé¢æ•°é‡: {len(pages)}")
        print(f"   å‘é‡æ•°é‡: {metadata.get('total_vectors', 'N/A')}")
        print(f"   å‘é‡ç»´åº¦: {metadata.get('vector_dimension', 'N/A')}")
        print(f"   æœ€åæ›´æ–°: {metadata.get('last_update', 'N/A')}")
        
        # æ£€æŸ¥ FAISS ç´¢å¼•
        if os.path.exists(faiss_index_file):
            try:
                faiss_index = faiss.read_index(faiss_index_file)
                print(f"   FAISSç´¢å¼•: {faiss_index.ntotal} ä¸ªå‘é‡")
            except Exception as e:
                print(f"   FAISSç´¢å¼•: âŒ æŸå ({e})")
        else:
            print("   FAISSç´¢å¼•: âŒ ä¸å­˜åœ¨")
        
        # æ£€æŸ¥å…ƒæ•°æ®
        if os.path.exists(faiss_metadata_file):
            try:
                with open(faiss_metadata_file, 'rb') as f:
                    metadata_list = pickle.load(f)
                print(f"   å…ƒæ•°æ®: {len(metadata_list)} æ¡è®°å½•")
            except Exception as e:
                print(f"   å…ƒæ•°æ®: âŒ æŸå ({e})")
        else:
            print("   å…ƒæ•°æ®: âŒ ä¸å­˜åœ¨")
            
    except Exception as e:
        print(f"âŒ æ•°æ®æ£€æŸ¥å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='é‡å»ºå‘é‡æ•°æ®')
    parser.add_argument('--check', action='store_true', help='ä»…æ£€æŸ¥çŠ¶æ€')
    parser.add_argument('--rebuild', action='store_true', help='é‡å»ºå‘é‡æ•°æ®')
    
    args = parser.parse_args()
    
    if args.check:
        check_vectors()
    elif args.rebuild:
        rebuild_vectors()
    else:
        print("è¯·æŒ‡å®šæ“ä½œ:")
        print("  --check   æ£€æŸ¥å‘é‡æ•°æ®çŠ¶æ€")
        print("  --rebuild é‡å»ºå‘é‡æ•°æ®")

if __name__ == "__main__":
    main()
