#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†æˆè¯­éŸ³è¯†åˆ«çš„æ™ºèƒ½å¯¹è¯åŠ©æ‰‹
ç»“åˆsherpa-ncnnè¯­éŸ³è¯†åˆ«å’ŒKokoro TTSï¼Œå®ç°å®Œæ•´çš„è¯­éŸ³å¯¹è¯åŠŸèƒ½
æ”¯æŒå”¤é†’è¯æ£€æµ‹ã€è¿ç»­å¯¹è¯ã€æ™ºèƒ½ç»“æŸåˆ¤æ–­å’Œä½å»¶è¿Ÿå“åº”
"""

import json
import os
import pickle
import numpy as np
import faiss
import ollama
import time
import re
import sys
import threading
import queue
import torch
import gc
import subprocess
import wave
import tempfile
import signal
from collections import deque
from concurrent.futures import ThreadPoolExecutor
from kokoro import KPipeline
import sherpa_ncnn


class IntegratedVoiceAssistant:
    def __init__(self):
        # è¯­éŸ³è¯†åˆ«ç»„ä»¶
        self.voice_recognizer = None
        self.wake_word = "ä½ å¥½"
        self.conversation_buffer = deque(maxlen=20)
        self.silence_count = 0
        self.max_silence = 3
        self.last_speech_time = 0
        self.has_meaningful_content = False
        self.is_listening_for_wake = True
        self.is_in_conversation = False
        
        # é—®ç­”ç³»ç»Ÿç»„ä»¶
        self.faiss_index = None
        self.faiss_metadata = None
        self.wiki_pages = []
        self.embedding_model = "nomic-embed-text"
        self.embedding_cache = {}
        self.cache_lock = threading.Lock()
        
        # TTSç»„ä»¶
        self.tts_pipeline_zh = None
        self.tts_pipeline_en = None
        self.tts_device = None
        self.tts_available = False
        self.voice_zf_tensor = None  # ä¸­æ–‡éŸ³è‰²
        self.voice_af_tensor = None  # è‹±æ–‡éŸ³è‰²
        
        # çº¿ç¨‹ç®¡ç†
        self.executor = ThreadPoolExecutor(max_workers=6)
        self.running = False
        
        # ä»»åŠ¡é˜Ÿåˆ—
        self.voice_queue = queue.Queue()
        self.llm_queue = queue.Queue()
        self.tts_queue = queue.Queue()
        self.audio_queue = queue.Queue()
        
        # å·¥ä½œçº¿ç¨‹
        self.voice_thread = None
        self.llm_thread = None
        self.tts_thread = None
        self.audio_thread = None
        
        # çŠ¶æ€æ ‡å¿—
        self.voice_processing = False
        self.llm_processing = False
        self.tts_processing = False
        self.audio_processing = False
        self.is_speaking = False  # æœºå™¨äººæ˜¯å¦æ­£åœ¨è¯´è¯
        
        # éŸ³é¢‘ç³»ç»Ÿ
        self.audio_initialized = False
        self.temp_audio_files = []  # ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶ç®¡ç†
        
        # ä¿¡å·å¤„ç†
        signal.signal(signal.SIGINT, self.signal_handler)
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        self.initialize_system()
    
    def signal_handler(self, sig, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        print('\nğŸ›‘ æ­£åœ¨é€€å‡º...')
        self.running = False
        sys.exit(0)
    
    def initialize_system(self):
        """åˆå§‹åŒ–æ•´ä¸ªç³»ç»Ÿ"""
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–é›†æˆè¯­éŸ³å¯¹è¯åŠ©æ‰‹...")
        
        try:
            # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
            self.initialize_knowledge_base()
            self.initialize_voice_recognition()
            self.initialize_tts()
            
            # å¯åŠ¨å·¥ä½œçº¿ç¨‹
            self.start_worker_threads()
            
            print("ğŸ‰ é›†æˆè¯­éŸ³å¯¹è¯åŠ©æ‰‹åˆå§‹åŒ–å®Œæˆï¼")
            self.show_system_info()
            
        except Exception as e:
            print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            raise
    
    def initialize_knowledge_base(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        print("ğŸ“š åˆå§‹åŒ–çŸ¥è¯†åº“...")
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶
        required_files = [
            "./data_base/faiss_index.bin",
            "./data_base/faiss_metadata.pkl",
            "./data_base/seeed_wiki_embeddings_db.json"
        ]
        
        for file in required_files:
            if not os.path.exists(file):
                print(f"âŒ ç¼ºå°‘æ•°æ®æ–‡ä»¶: {file}")
                raise FileNotFoundError(f"ç¼ºå°‘æ•°æ®æ–‡ä»¶: {file}")
        
        # æ£€æŸ¥OllamaæœåŠ¡
        try:
            models = ollama.list()
            print(f"âœ… OllamaæœåŠ¡æ­£å¸¸ï¼Œå¯ç”¨æ¨¡å‹: {len(models.models)} ä¸ª")
        except Exception as e:
            print(f"âŒ OllamaæœåŠ¡æ£€æŸ¥å¤±è´¥: {str(e)}")
            raise
        
        # åŠ è½½FAISSç´¢å¼•
        print("ğŸ” åŠ è½½FAISSç´¢å¼•...")
        self.faiss_index = faiss.read_index("./data_base/faiss_index.bin")
        print(f"âœ… FAISSç´¢å¼•åŠ è½½å®Œæˆ: {self.faiss_index.ntotal} ä¸ªå‘é‡")
        
        # åŠ è½½å…ƒæ•°æ®
        print("ğŸ“Š åŠ è½½å…ƒæ•°æ®...")
        with open("./data_base/faiss_metadata.pkl", 'rb') as f:
            self.faiss_metadata = pickle.load(f)
        print(f"âœ… å…ƒæ•°æ®åŠ è½½å®Œæˆ: {len(self.faiss_metadata)} æ¡è®°å½•")
        
        # åŠ è½½Wikié¡µé¢æ•°æ®
        print("ğŸ“– åŠ è½½Wikié¡µé¢æ•°æ®...")
        with open("./data_base/seeed_wiki_embeddings_db.json", 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.wiki_pages = data['pages']
            self.metadata = data['metadata']
        print(f"âœ… é¡µé¢æ•°æ®åŠ è½½å®Œæˆ: {len(self.wiki_pages)} ä¸ªé¡µé¢")
    
    def initialize_voice_recognition(self):
        """åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«"""
        print("ğŸ¤ åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ...")
        
        model_dir = "/home/seeed/Local_Chat_Bot/audio/sherpa-ncnn/models"
        
        try:
            self.voice_recognizer = sherpa_ncnn.Recognizer(
                tokens=f"{model_dir}/tokens.txt",
                encoder_param=f"{model_dir}/encoder_jit_trace-pnnx.ncnn.param",
                encoder_bin=f"{model_dir}/encoder_jit_trace-pnnx.ncnn.bin",
                decoder_param=f"{model_dir}/decoder_jit_trace-pnnx.ncnn.param",
                decoder_bin=f"{model_dir}/decoder_jit_trace-pnnx.ncnn.bin",
                joiner_param=f"{model_dir}/joiner_jit_trace-pnnx.ncnn.param",
                joiner_bin=f"{model_dir}/joiner_jit_trace-pnnx.ncnn.bin",
                num_threads=4,
            )
            print("âœ… è¯­éŸ³è¯†åˆ«æ¨¡å‹åŠ è½½å®Œæˆ")
            print(f"ğŸ“± æ¨¡å‹é‡‡æ ·ç‡: {self.voice_recognizer.sample_rate} Hz")
        except Exception as e:
            print(f"âŒ è¯­éŸ³è¯†åˆ«åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def initialize_tts(self):
        """åˆå§‹åŒ–TTSç³»ç»Ÿ"""
        print("ğŸµ åˆå§‹åŒ–TTSç³»ç»Ÿ...")
        
        try:
            # æ£€æŸ¥GPUå¯ç”¨æ€§
            self.tts_device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"ğŸµ ä½¿ç”¨è®¾å¤‡: {self.tts_device}")
            
            # ä¸å†ä½¿ç”¨pygameï¼Œä½¿ç”¨ç³»ç»ŸéŸ³é¢‘æ’­æ”¾å™¨
            self.audio_initialized = True
            print("âœ… éŸ³é¢‘ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ (ä½¿ç”¨ç³»ç»Ÿæ’­æ”¾å™¨)")
            
            # åŠ è½½éŸ³è‰²æ–‡ä»¶
            print("ğŸ¶ åŠ è½½éŸ³è‰²æ–‡ä»¶...")
            voice_zf_path = 'ckpts/kokoro-v1.1/voices/zf_001.pt'
            voice_af_path = 'ckpts/kokoro-v1.1/voices/af_heart.pt'
            
            if os.path.exists(voice_zf_path):
                self.voice_zf_tensor = torch.load(voice_zf_path, weights_only=True)
                print("âœ… åŠ è½½ä¸­æ–‡éŸ³è‰²: zf_001")
            
            if os.path.exists(voice_af_path):
                self.voice_af_tensor = torch.load(voice_af_path, weights_only=True)
                print("âœ… åŠ è½½è‹±æ–‡éŸ³è‰²: af_heart")
            
            # å®šä¹‰è‹±æ–‡å›è°ƒå‡½æ•°
            def en_callable(text):
                """è‹±æ–‡å›è°ƒå‡½æ•°"""
                try:
                    selected_voice = self.voice_af_tensor if self.voice_af_tensor is not None else self.voice_zf_tensor
                    result = next(self.tts_pipeline_en(text, voice=selected_voice))
                    return result.phonemes
                except Exception as e:
                    return text
            
            # åˆå§‹åŒ–TTSæ¨¡å‹
            print("ğŸ”§ åŠ è½½TTSæ¨¡å‹...")
            self.tts_pipeline_en = KPipeline(lang_code='a', device=self.tts_device)
            self.tts_pipeline_zh = KPipeline(lang_code='z', device=self.tts_device, en_callable=en_callable)
            
            # æ¨¡å‹é¢„çƒ­
            print("ğŸ”¥ TTSæ¨¡å‹é¢„çƒ­...")
            warmup_voice = self.voice_zf_tensor if self.voice_zf_tensor is not None else 'af_heart'
            generator = self.tts_pipeline_zh("ä½ å¥½", voice=warmup_voice)
            for gs, ps, audio in generator:
                break
            
            self.tts_available = True
            print("âœ… TTSç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ TTSåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.tts_available = False
            raise
    
    def start_worker_threads(self):
        """å¯åŠ¨å·¥ä½œçº¿ç¨‹"""
        print("ğŸ§µ å¯åŠ¨å·¥ä½œçº¿ç¨‹...")
        
        self.running = True
        
        # è¯­éŸ³è¯†åˆ«çº¿ç¨‹
        self.voice_thread = threading.Thread(target=self.voice_worker, daemon=True)
        self.voice_thread.start()
        print("âœ… è¯­éŸ³è¯†åˆ«çº¿ç¨‹å·²å¯åŠ¨")
        
        # LLMå¤„ç†çº¿ç¨‹
        self.llm_thread = threading.Thread(target=self.llm_worker, daemon=True)
        self.llm_thread.start()
        print("âœ… LLMå¤„ç†çº¿ç¨‹å·²å¯åŠ¨")
        
        # TTSå¤„ç†çº¿ç¨‹
        self.tts_thread = threading.Thread(target=self.tts_worker, daemon=True)
        self.tts_thread.start()
        print("âœ… TTSå¤„ç†çº¿ç¨‹å·²å¯åŠ¨")
        
        # éŸ³é¢‘æ’­æ”¾çº¿ç¨‹
        self.audio_thread = threading.Thread(target=self.audio_worker, daemon=True)
        self.audio_thread.start()
        print("âœ… éŸ³é¢‘æ’­æ”¾çº¿ç¨‹å·²å¯åŠ¨")
    
    def voice_worker(self):
        """è¯­éŸ³è¯†åˆ«å·¥ä½œçº¿ç¨‹"""
        print("ğŸ¤ è¯­éŸ³è¯†åˆ«çº¿ç¨‹å¼€å§‹è¿è¡Œ...")
        
        # å¯åŠ¨è¿ç»­ç›‘å¬
        self.continuous_voice_listen()
    
    def continuous_voice_listen(self):
        """è¿ç»­è¯­éŸ³ç›‘å¬"""
        print("ğŸ”„ å¼€å§‹ç›‘å¬å”¤é†’è¯...")
        
        while self.running:
            try:
                if self.is_listening_for_wake:
                    # ç›‘å¬å”¤é†’è¯æ¨¡å¼
                    self.listen_for_wake_word()
                elif self.is_in_conversation:
                    # å¯¹è¯æ¨¡å¼
                    self.listen_for_conversation()
                else:
                    time.sleep(0.1)
                    
            except Exception as e:
                print(f"âŒ è¯­éŸ³ç›‘å¬é”™è¯¯: {str(e)}")
                time.sleep(1)
    
    def listen_for_wake_word(self):
        """ç›‘å¬å”¤é†’è¯"""
        # å¦‚æœæœºå™¨äººæ­£åœ¨è¯´è¯æˆ–æ’­æ”¾éŸ³é¢‘ï¼Œä¸ç›‘å¬å”¤é†’è¯
        if self.is_speaking or self.audio_processing:
            time.sleep(0.2)
            return
            
        audio_data, audio_level = self.record_chunk(duration=1)
        
        if audio_data is not None and audio_level > 0.03:  # æé«˜é˜ˆå€¼ï¼Œå‡å°‘å™ªéŸ³å¹²æ‰°
            text = self.recognize_audio(audio_data)
            
            if text and self.wake_word in text:
                print(f"ğŸ¯ æ£€æµ‹åˆ°å”¤é†’è¯: '{text}'")
                self.handle_wake_up()
        
        time.sleep(0.1)
    
    def handle_wake_up(self):
        """å¤„ç†å”¤é†’"""
        print("ğŸ‘‹ æœºå™¨äººè¢«å”¤é†’ï¼Œå‡†å¤‡å›å¤...")
        
        # åˆ‡æ¢çŠ¶æ€
        self.is_listening_for_wake = False
        self.is_in_conversation = False
        self.is_speaking = True  # æ ‡è®°æ­£åœ¨è¯´è¯
        
        # ç”Ÿæˆæ¬¢è¿è¯­éŸ³
        welcome_text = "ä½ å¥½å‘€ï¼Œæœ‰ä»€ä¹ˆé—®é¢˜å¯ä»¥é—®æˆ‘ã€‚"
        self.tts_queue.put({
            'text': welcome_text,
            'callback': self.on_welcome_complete
        })
    
    def on_welcome_complete(self, segments):
        """æ¬¢è¿è¯­éŸ³æ’­æ”¾å®Œæˆå›è°ƒ"""
        print("âœ… æ¬¢è¿è¯­éŸ³æ’­æ”¾å®Œæˆï¼Œå¼€å§‹ç›‘å¬é—®é¢˜...")
        
        # åˆ‡æ¢åˆ°å¯¹è¯æ¨¡å¼
        self.is_speaking = False  # åœæ­¢è¯´è¯çŠ¶æ€
        self.is_in_conversation = True
        self.conversation_buffer.clear()
        self.silence_count = 0
        self.last_speech_time = 0
        self.has_meaningful_content = False
    
    def listen_for_conversation(self):
        """ç›‘å¬å¯¹è¯å†…å®¹"""
        continuous_audio = []
        speech_detected = False
        
        while self.is_in_conversation and self.running:
            # å¦‚æœæ­£åœ¨æ’­æ”¾éŸ³é¢‘ï¼Œæš‚åœç›‘å¬
            if self.is_speaking or self.audio_processing:
                time.sleep(0.2)
                continue
                
            audio_data, audio_level = self.record_chunk(duration=1)
            
            if audio_data is not None:
                if audio_level > 0.03:  # æé«˜é˜ˆå€¼ï¼Œå‡å°‘å™ªéŸ³å’Œå›éŸ³å¹²æ‰°
                    continuous_audio.append(audio_data)
                    speech_detected = True
                    self.silence_count = 0
                    self.last_speech_time = time.time()
                    print("ğŸµ", end="", flush=True)
                    
                else:
                    # é™é»˜å¤„ç†
                    if speech_detected and len(continuous_audio) > 1:
                        # å¤„ç†ç´¯ç§¯çš„éŸ³é¢‘
                        combined_audio = np.concatenate(continuous_audio)
                        text = self.recognize_audio(combined_audio)
                        
                        if text:
                            print(f"\nğŸ—£ï¸  '{text}'")
                            self.conversation_buffer.append({
                                "text": text,
                                "timestamp": time.time()
                            })
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ„ä¹‰çš„å†…å®¹
                            if self.is_meaningful_content(text):
                                self.has_meaningful_content = True
                                print("âœ¨ æ£€æµ‹åˆ°å®Œæ•´é—®é¢˜")
                        
                        continuous_audio = []
                        speech_detected = False
                    
                    self.silence_count += 1
                    print(".", end="", flush=True)
                    
                    # æ£€æŸ¥æ˜¯å¦ç»“æŸå¯¹è¯
                    if self.should_end_conversation():
                        self.end_conversation()
                        break
            
            time.sleep(0.1)
    
    def should_end_conversation(self):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»“æŸå¯¹è¯"""
        current_time = time.time()
        
        # å¦‚æœå·²ç»æœ‰æœ‰æ„ä¹‰çš„å†…å®¹ï¼Œ1ç§’é™é»˜å°±ç»“æŸï¼ˆæé«˜å®æ—¶æ€§ï¼‰
        if self.has_meaningful_content and self.silence_count >= 1:
            print(f"\nğŸ¯ å·²è·å¾—å®Œæ•´å†…å®¹ï¼Œ{self.silence_count}ç§’é™é»˜åç»“æŸå¯¹è¯")
            return True
        
        # æ²¡æœ‰å†…å®¹æ—¶ï¼Œç­‰å¾…2ç§’ï¼ˆå‡å°‘ç­‰å¾…æ—¶é—´ï¼‰
        if not self.has_meaningful_content and self.silence_count >= 2:
            print(f"\nğŸ”‡ æ£€æµ‹åˆ°è¿ç»­{self.silence_count}æ¬¡é™é»˜ï¼Œå‡†å¤‡ç»“æŸå¯¹è¯...")
            return True
        
        # æ£€æŸ¥ç»“æŸè¯
        if len(self.conversation_buffer) > 0:
            recent_text = " ".join([item["text"] for item in list(self.conversation_buffer)[-2:]])
            end_phrases = ["è°¢è°¢", "å†è§", "å¥½çš„", "çŸ¥é“äº†", "æ˜ç™½äº†", "å®Œæ¯•", "ç»“æŸ"]
            
            for phrase in end_phrases:
                if phrase in recent_text:
                    print(f"\nğŸ”š æ£€æµ‹åˆ°ç»“æŸè¯ '{phrase}'")
                    return True
        
        # è¶…æ—¶ä¿æŠ¤ï¼ˆå‡å°‘åˆ°8ç§’ï¼‰
        if self.last_speech_time > 0 and (current_time - self.last_speech_time) > 8:
            print("\nâ° è¶…æ—¶å¼ºåˆ¶ç»“æŸå¯¹è¯")
            return True
        
        return False
    
    def end_conversation(self):
        """ç»“æŸå¯¹è¯"""
        # å…ˆåˆ‡æ¢çŠ¶æ€ï¼Œé˜²æ­¢é‡å¤è§¦å‘
        self.is_in_conversation = False
        
        print("\n" + "-" * 40)
        print("ğŸ”š å¯¹è¯ç»“æŸ")
        
        # æ”¶é›†å®Œæ•´é—®é¢˜
        if self.conversation_buffer:
            complete_question = " ".join([item["text"] for item in self.conversation_buffer])
            print(f"ğŸ“ å®Œæ•´é—®é¢˜: '{complete_question}'")
            
            # æ¸…ç©ºå¯¹è¯ç¼“å†²åŒºï¼Œé˜²æ­¢é‡å¤å¤„ç†
            self.conversation_buffer.clear()
            
            # å°†é—®é¢˜å‘é€ç»™LLMå¤„ç†
            self.llm_queue.put({
                'question': complete_question,
                'callback': self.on_answer_generated
            })
        else:
            print("ğŸ˜… æ²¡æœ‰æ£€æµ‹åˆ°å®Œæ•´é—®é¢˜ï¼Œè¿”å›ç›‘å¬çŠ¶æ€")
            self.return_to_wake_listening()
    
    def on_answer_generated(self, answer):
        """LLMå›ç­”ç”Ÿæˆå®Œæˆå›è°ƒ"""
        print(f"\nğŸ’¬ AIå›ç­”: {answer}")
        
        # æ ‡è®°å¼€å§‹è¯´è¯
        self.is_speaking = True
        
        # å°†å›ç­”å‘é€ç»™TTS
        self.tts_queue.put({
            'text': answer,
            'callback': self.on_answer_speech_complete
        })
    
    def on_answer_speech_complete(self, segments):
        """å›ç­”è¯­éŸ³æ’­æ”¾å®Œæˆå›è°ƒ"""
        print("âœ… å›ç­”æ’­æ”¾å®Œæˆï¼Œè¿”å›ç›‘å¬çŠ¶æ€")
        self.return_to_wake_listening()
    
    def return_to_wake_listening(self):
        """è¿”å›å”¤é†’è¯ç›‘å¬çŠ¶æ€"""
        self.is_listening_for_wake = True
        self.is_in_conversation = False
        self.is_speaking = False  # åœæ­¢è¯´è¯çŠ¶æ€
        print("ğŸ”„ é‡æ–°ç›‘å¬å”¤é†’è¯ä¸­...")
    
    def cleanup_temp_audio_files(self):
        """æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶"""
        for temp_file in self.temp_audio_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {temp_file} - {e}")
        self.temp_audio_files.clear()
    
    def record_chunk(self, duration=1):
        """å½•åˆ¶éŸ³é¢‘ç‰‡æ®µ"""
        temp_file = f"/tmp/voice_chunk_{int(time.time() * 1000)}.wav"
        
        try:
            cmd = [
                "arecord", 
                "-D", "sysdefault",
                "-f", "S16_LE", 
                "-r", "16000",
                "-c", "1",
                "-d", str(duration),
                temp_file
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=duration + 1)
            
            if result.returncode != 0:
                return None, 0
                
            with wave.open(temp_file, 'rb') as f:
                frames = f.readframes(f.getnframes())
                audio_data = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32768.0
            
            audio_level = np.abs(audio_data).max()
            return audio_data, audio_level
            
        except Exception as e:
            return None, 0
        finally:
            if os.path.exists(temp_file):
                os.remove(temp_file)
    
    def recognize_audio(self, audio_data):
        """è¯†åˆ«éŸ³é¢‘"""
        try:
            if len(audio_data) < 16000:  # å°‘äº1ç§’
                return ""
            
            audio_max = np.abs(audio_data).max()
            if audio_max < 0.01:
                return ""
            
            # éŸ³é¢‘å½’ä¸€åŒ–
            if audio_max > 0:
                if audio_max < 0.1:
                    audio_data = audio_data / audio_max * 0.1
                elif audio_max > 0.8:
                    audio_data = audio_data / audio_max * 0.8
            
            # åˆ›å»ºæ–°çš„è¯†åˆ«å™¨å®ä¾‹
            model_dir = "/home/seeed/Local_Chat_Bot/audio/sherpa-ncnn/models"
            recognizer = sherpa_ncnn.Recognizer(
                tokens=f"{model_dir}/tokens.txt",
                encoder_param=f"{model_dir}/encoder_jit_trace-pnnx.ncnn.param",
                encoder_bin=f"{model_dir}/encoder_jit_trace-pnnx.ncnn.bin",
                decoder_param=f"{model_dir}/decoder_jit_trace-pnnx.ncnn.param",
                decoder_bin=f"{model_dir}/decoder_jit_trace-pnnx.ncnn.bin",
                joiner_param=f"{model_dir}/joiner_jit_trace-pnnx.ncnn.param",
                joiner_bin=f"{model_dir}/joiner_jit_trace-pnnx.ncnn.bin",
                num_threads=4,
            )
            
            recognizer.accept_waveform(16000, audio_data)
            silence = np.zeros(int(0.8 * 16000), dtype=np.float32)
            recognizer.accept_waveform(16000, silence)
            recognizer.input_finished()
            
            result = recognizer.text.strip()
            return result
            
        except Exception as e:
            return ""
    
    def is_meaningful_content(self, text):
        """åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ„ä¹‰çš„å®Œæ•´å†…å®¹"""
        question_words = ["ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å“ªé‡Œ", "å“ªä¸ª", "å‡ ", "å¤šå°‘", "è°", "å—", "å‘¢", "?", "ï¼Ÿ"]
        for word in question_words:
            if word in text:
                return True
        
        if len(text.strip()) >= 3:
            return True
            
        return False
    
    def llm_worker(self):
        """LLMå¤„ç†å·¥ä½œçº¿ç¨‹"""
        last_processed_question = ""  # è®°å½•ä¸Šæ¬¡å¤„ç†çš„é—®é¢˜ï¼Œé¿å…é‡å¤
        
        while self.running:
            try:
                if self.llm_queue.empty():
                    time.sleep(0.1)
                    continue
                
                task = self.llm_queue.get(timeout=1)
                self.llm_processing = True
                
                question = task['question']
                callback = task['callback']
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤é—®é¢˜
                if question == last_processed_question:
                    print(f"âš ï¸ [LLMçº¿ç¨‹] è·³è¿‡é‡å¤é—®é¢˜: '{question[:30]}...'")
                    self.llm_processing = False
                    self.llm_queue.task_done()
                    continue
                
                print(f"ğŸ¤– [LLMçº¿ç¨‹] å¼€å§‹å¤„ç†é—®é¢˜: '{question[:30]}...'")
                start_time = time.time()
                
                answer = self.generate_answer(question)
                last_processed_question = question  # è®°å½•å·²å¤„ç†çš„é—®é¢˜
                
                process_time = time.time() - start_time
                print(f"âœ… [LLMçº¿ç¨‹] å›ç­”ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {process_time:.2f}ç§’")
                
                if callback:
                    callback(answer)
                
                self.llm_processing = False
                self.llm_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ [LLMçº¿ç¨‹] å¤„ç†é”™è¯¯: {str(e)}")
                self.llm_processing = False
                time.sleep(1)
    
    def generate_answer(self, question):
        """ç”Ÿæˆå›ç­”"""
        try:
            # æœç´¢çŸ¥è¯†åº“
            search_results = self.search_knowledge_base(question, top_k=3)
            
            if not search_results:
                return "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            for result in search_results:
                title = result['title']
                content = result['content']
                if content.startswith('[Introduction] '):
                    content = content[16:]
                if len(content) > 200:
                    content = content[:200] + "..."
                context_parts.append(f"æ–‡æ¡£: {title}\nå†…å®¹: {content}")
            
            context = "\n\n".join(context_parts)
            
            prompt = f"""è¯·åŸºäºä»¥ä¸‹èµ„æ–™ï¼Œç”¨ç®€æ´çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

é‡è¦è¦æ±‚ï¼š
1. å¿…é¡»ç”¨ä¸­æ–‡å›ç­”
2. å›ç­”æ§åˆ¶åœ¨100-150å­—ä¹‹é—´
3. è¯­è¨€ç®€æ´æ˜äº†ï¼Œä¾¿äºè¯­éŸ³æ’­æ”¾
4. ä¸¥æ ¼åŸºäºæä¾›çš„èµ„æ–™å›ç­”
5. ä»‹ç»äº§å“æ—¶è¯´"æˆ‘ä»¬çš„xxxäº§å“"

ç›¸å…³èµ„æ–™:
{context}

ç”¨æˆ·é—®é¢˜: {question}

è¯·ç®€æ´å›ç­”:"""
            
            response = ollama.chat(
                model='qwen2.5:3b',
                messages=[
                    {'role': 'system', 'content': 'ç”¨ä¸­æ–‡ç®€æ´å›ç­”ï¼ŒåŸºäºèµ„æ–™ï¼Œé€‚åˆè¯­éŸ³æ’­æ”¾ã€‚'},
                    {'role': 'user', 'content': prompt}
                ],
                options={
                    'temperature': 0.7,
                    'top_p': 0.9,
                    'num_predict': 200,
                }
            )
            
            answer = response['message']['content'].strip()
            
            # é™åˆ¶é•¿åº¦
            if len(answer) > 150:
                sentences = answer.split('ã€‚')
                result = ""
                for sentence in sentences:
                    if len(result + sentence + 'ã€‚') <= 150:
                        result += sentence + 'ã€‚'
                    else:
                        break
                answer = result
            
            return answer
            
        except Exception as e:
            print(f"âŒ å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}")
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›ç­”ã€‚"
    
    def search_knowledge_base(self, query, top_k=10):
        """æœç´¢çŸ¥è¯†åº“"""
        try:
            query_embedding = self.generate_embedding(query)
            if query_embedding is None:
                return []
            
            query_embedding = query_embedding.reshape(1, -1).astype(np.float32)
            scores, indices = self.faiss_index.search(query_embedding, top_k)
            
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.faiss_metadata):
                    metadata = self.faiss_metadata[idx]
                    page_data = self.wiki_pages[idx]
                    
                    results.append({
                        'rank': i + 1,
                        'score': float(score),
                        'title': metadata['title'],
                        'url': metadata['url'],
                        'content': page_data['content'],
                    })
            
            return results
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    def generate_embedding(self, text):
        """ç”Ÿæˆembedding"""
        if not text or not text.strip():
            return None
        
        try:
            response = ollama.embeddings(model=self.embedding_model, prompt=text)
            embedding = np.array(response["embedding"], dtype=np.float32)
            
            norm = np.linalg.norm(embedding)
            if norm > 0:
                embedding = embedding / norm
            
            return embedding
            
        except Exception as e:
            print(f"âŒ Embeddingç”Ÿæˆå¤±è´¥: {str(e)}")
            return None
    
    def tts_worker(self):
        """TTSå¤„ç†å·¥ä½œçº¿ç¨‹"""
        while self.running:
            try:
                if self.tts_queue.empty():
                    time.sleep(0.1)
                    continue
                
                task = self.tts_queue.get(timeout=1)
                self.tts_processing = True
                
                text = task['text']
                callback = task['callback']
                
                if not self.tts_available or not text.strip():
                    self.tts_processing = False
                    self.tts_queue.task_done()
                    continue
                
                print(f"ğŸ¤ [TTSçº¿ç¨‹] å¼€å§‹ç”Ÿæˆè¯­éŸ³: '{text[:30]}...'")
                
                # æ¸…ç†ä¹‹å‰çš„ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
                self.cleanup_temp_audio_files()
                
                start_time = time.time()
                
                # ä½¿ç”¨ä¸­æ–‡TTS
                selected_voice = self.voice_zf_tensor if self.voice_zf_tensor is not None else 'af_heart'
                
                audio_segments = []
                generator = self.tts_pipeline_zh(text, voice=selected_voice)
                
                # æ”¶é›†æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
                for gs, ps, audio in generator:
                    if isinstance(audio, torch.Tensor):
                        audio = audio.detach().cpu().numpy()
                    
                    if audio.dtype != np.float32:
                        audio = audio.astype(np.float32)
                    
                    audio_segments.append(audio)
                
                # åˆå¹¶æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µä¸ºä¸€ä¸ªå®Œæ•´çš„éŸ³é¢‘ï¼Œé¿å…åˆ†æ®µæ’­æ”¾å¯¼è‡´çš„å›éŸ³
                if audio_segments:
                    combined_audio = np.concatenate(audio_segments)
                    
                    # éŸ³é¢‘åå¤„ç†
                    max_val = np.max(np.abs(combined_audio))
                    if max_val > 1.0:
                        combined_audio = combined_audio / max_val
                    
                    # è½¬æ¢ä¸ºint16æ ¼å¼
                    audio_int16 = (combined_audio * 32767).astype(np.int16)
                    
                    process_time = time.time() - start_time
                    print(f"âœ… [TTSçº¿ç¨‹] è¯­éŸ³ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {process_time:.2f}ç§’ï¼ŒéŸ³é¢‘é•¿åº¦: {len(audio_int16)/24000:.1f}ç§’")
                    
                    # ä¿å­˜ä¸ºWAVæ–‡ä»¶
                    temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                    temp_filename = temp_file.name
                    temp_file.close()
                    
                    # å†™å…¥WAVæ–‡ä»¶
                    with wave.open(temp_filename, 'wb') as wav_file:
                        wav_file.setnchannels(1)  # å•å£°é“
                        wav_file.setsampwidth(2)  # 16ä½
                        wav_file.setframerate(24000)  # é‡‡æ ·ç‡
                        wav_file.writeframes(audio_int16.tobytes())
                    
                    # æ·»åŠ åˆ°ä¸´æ—¶æ–‡ä»¶åˆ—è¡¨
                    self.temp_audio_files.append(temp_filename)
                    
                    # æ¸…ç©ºéŸ³é¢‘é˜Ÿåˆ—ï¼Œé˜²æ­¢é‡å æ’­æ”¾
                    while not self.audio_queue.empty():
                        try:
                            self.audio_queue.get_nowait()
                        except queue.Empty:
                            break
                    
                    # æ·»åŠ æ–‡ä»¶è·¯å¾„åˆ°æ’­æ”¾é˜Ÿåˆ—
                    self.audio_queue.put(temp_filename)
                else:
                    print("âš ï¸ [TTSçº¿ç¨‹] æ²¡æœ‰ç”ŸæˆéŸ³é¢‘å†…å®¹")
                
                if callback:
                    callback(1)  # ç°åœ¨åªæœ‰ä¸€ä¸ªåˆå¹¶çš„éŸ³é¢‘ç‰‡æ®µ
                
                self.tts_processing = False
                self.tts_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ [TTSçº¿ç¨‹] å¤„ç†é”™è¯¯: {str(e)}")
                self.tts_processing = False
                time.sleep(1)
    
    def audio_worker(self):
        """éŸ³é¢‘æ’­æ”¾å·¥ä½œçº¿ç¨‹ - ä½¿ç”¨ç³»ç»Ÿæ’­æ”¾å™¨"""
        while self.running:
            try:
                if self.audio_queue.empty():
                    time.sleep(0.01)
                    continue
                
                audio_filename = self.audio_queue.get(timeout=0.1)
                self.audio_processing = True
                
                try:
                    if os.path.exists(audio_filename):
                        print(f"ğŸ”Š [éŸ³é¢‘çº¿ç¨‹] æ’­æ”¾éŸ³é¢‘æ–‡ä»¶")
                        
                        # ä½¿ç”¨aplayæ’­æ”¾éŸ³é¢‘æ–‡ä»¶ï¼Œé™ä½éŸ³é‡é¿å…å›éŸ³
                        start_time = time.time()
                        result = subprocess.run(['aplay', '-q', audio_filename], 
                                              capture_output=True, text=True, 
                                              timeout=30)  # 30ç§’è¶…æ—¶ï¼Œ-qå®‰é™æ¨¡å¼
                        
                        play_time = time.time() - start_time
                        
                        if result.returncode == 0:
                            print(f"âœ… [éŸ³é¢‘çº¿ç¨‹] éŸ³é¢‘æ’­æ”¾å®Œæˆï¼Œæ’­æ”¾æ—¶é—´: {play_time:.1f}ç§’")
                            
                            # æ’­æ”¾å®Œæˆåç­‰å¾…2ç§’ï¼Œè®©æ‰¬å£°å™¨å£°éŸ³å®Œå…¨æ¶ˆå¤±ï¼Œé¿å…è¢«éº¦å…‹é£æ•è·
                            print("â±ï¸ [éŸ³é¢‘çº¿ç¨‹] ç­‰å¾…å›éŸ³æ¶ˆæ•£...")
                            time.sleep(2.0)
                            print("âœ… [éŸ³é¢‘çº¿ç¨‹] é™éŸ³ç¼“å†²å®Œæˆ")
                        else:
                            print(f"âŒ [éŸ³é¢‘çº¿ç¨‹] æ’­æ”¾å¤±è´¥: {result.stderr}")
                            
                        # æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
                        try:
                            os.remove(audio_filename)
                            if audio_filename in self.temp_audio_files:
                                self.temp_audio_files.remove(audio_filename)
                        except:
                            pass
                            
                    else:
                        print(f"âŒ [éŸ³é¢‘çº¿ç¨‹] éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_filename}")
                    
                except subprocess.TimeoutExpired:
                    print("âš ï¸ [éŸ³é¢‘çº¿ç¨‹] æ’­æ”¾è¶…æ—¶")
                except Exception as e:
                    print(f"âŒ [éŸ³é¢‘çº¿ç¨‹] æ’­æ”¾é”™è¯¯: {str(e)}")
                
                self.audio_processing = False
                self.audio_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ [éŸ³é¢‘çº¿ç¨‹] å¤„ç†é”™è¯¯: {str(e)}")
                self.audio_processing = False
                time.sleep(0.1)
    
    def show_system_info(self):
        """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯"""
        print(f"\nğŸ“Š ç³»ç»Ÿä¿¡æ¯:")
        print(f"   çŸ¥è¯†åº“é¡µé¢: {len(self.wiki_pages)}")
        print(f"   è¯­éŸ³è¯†åˆ«: {'å¯ç”¨' if self.voice_recognizer else 'ä¸å¯ç”¨'}")
        print(f"   TTSç³»ç»Ÿ: {'å¯ç”¨' if self.tts_available else 'ä¸å¯ç”¨'}")
        print(f"   å”¤é†’è¯: '{self.wake_word}'")
        print(f"   éŸ³é¢‘æ’­æ”¾: ç³»ç»Ÿæ’­æ”¾å™¨ (é˜²å›éŸ³æ¨¡å¼)")
        print(f"   å·¥ä½œçº¿ç¨‹: è¯­éŸ³={'è¿è¡Œä¸­' if self.voice_thread and self.voice_thread.is_alive() else 'æœªå¯åŠ¨'}")
        print(f"            LLM={'è¿è¡Œä¸­' if self.llm_thread and self.llm_thread.is_alive() else 'æœªå¯åŠ¨'}")
        print(f"            TTS={'è¿è¡Œä¸­' if self.tts_thread and self.tts_thread.is_alive() else 'æœªå¯åŠ¨'}")
        print(f"            éŸ³é¢‘={'è¿è¡Œä¸­' if self.audio_thread and self.audio_thread.is_alive() else 'æœªå¯åŠ¨'}")
        print(f"\nğŸ’¡ é˜²å›éŸ³å»ºè®®:")
        print(f"   - ä½¿ç”¨è€³æœºå¯å®Œå…¨é¿å…å›éŸ³é—®é¢˜")
        print(f"   - é™ä½æ‰¬å£°å™¨éŸ³é‡")
        print(f"   - ä¿æŒéº¦å…‹é£ä¸æ‰¬å£°å™¨è·ç¦»")
        print(f"   - å½“å‰å·²å¯ç”¨éŸ³é¢‘æš‚åœç›‘å¬åŠŸèƒ½")
    
    def run(self):
        """è¿è¡Œè¯­éŸ³å¯¹è¯åŠ©æ‰‹"""
        print("ğŸ¤– Seeed Studio é›†æˆè¯­éŸ³å¯¹è¯åŠ©æ‰‹ (é˜²å›éŸ³ç‰ˆ)")
        print("=" * 60)
        print("æ¬¢è¿ä½¿ç”¨æˆ‘ä»¬çš„æ™ºèƒ½è¯­éŸ³é—®ç­”ç³»ç»Ÿï¼")
        print("ğŸ¯ åŠŸèƒ½: å”¤é†’è¯æ£€æµ‹ â†’ è¯­éŸ³é—®ç­” â†’ æ™ºèƒ½å›å¤")
        print(f"ğŸ”‘ å”¤é†’è¯: '{self.wake_word}'")
        print("ğŸ’¡ è¯´è¯æµç¨‹: è¯´å”¤é†’è¯ â†’ ç­‰å¾…å›å¤ â†’ æé—® â†’ å¬å›ç­”")
        print("ğŸ§ æœ€ä½³ä½“éªŒ: å»ºè®®ä½¿ç”¨è€³æœºé¿å…å›éŸ³å¹²æ‰°")
        print("ğŸ”‡ é˜²å›éŸ³: æ’­æ”¾æ—¶è‡ªåŠ¨æš‚åœç›‘å¬ + 2ç§’é™éŸ³ç¼“å†²")
        print("=" * 60)
        
        try:
            while True:
                # æ˜¾ç¤ºçŠ¶æ€
                if self.is_listening_for_wake:
                    status = "ğŸ”„ ç›‘å¬å”¤é†’è¯ä¸­..."
                elif self.is_in_conversation:
                    status = "ğŸ’¬ å¯¹è¯è¿›è¡Œä¸­..."
                else:
                    status = "ğŸ¤– ç³»ç»Ÿå¤„ç†ä¸­..."
                
                print(f"\r{status}", end="", flush=True)
                time.sleep(1)
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
        finally:
            self.running = False
            print("ğŸ”„ æ­£åœ¨åœæ­¢å·¥ä½œçº¿ç¨‹...")
            
            # æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
            self.cleanup_temp_audio_files()
            
            # ç­‰å¾…çº¿ç¨‹ç»“æŸ
            for thread in [self.voice_thread, self.llm_thread, self.tts_thread, self.audio_thread]:
                if thread and thread.is_alive():
                    thread.join(timeout=2)
            
            print("âœ… è¯­éŸ³å¯¹è¯åŠ©æ‰‹å·²åœæ­¢")


def main():
    """ä¸»å‡½æ•°"""
    try:
        assistant = IntegratedVoiceAssistant()
        assistant.run()
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {str(e)}")
        print("è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶å’Œä¾èµ–é¡¹")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
