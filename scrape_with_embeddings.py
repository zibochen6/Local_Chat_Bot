#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Seeed Wiki ä¼˜åŒ–çˆ¬è™« - çˆ¬å–æ—¶ç”Ÿæˆå¹¶ä¿å­˜ Embedding å‘é‡
æ”¯æŒä¸­è‹±æ–‡å†…å®¹çˆ¬å–ï¼Œå¢é‡æ›´æ–°ï¼Œå®šæ—¶æ£€æµ‹æ–°é¡µé¢
ä½¿ç”¨ Ollama nomic-embed-text æ¨¡å‹ + FAISS ç´¢å¼•ï¼ŒåŠ é€Ÿå¯åŠ¨å’Œæ£€ç´¢é€Ÿåº¦
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
from urllib.parse import urljoin, urlparse
from collections import deque
import re
import numpy as np
import faiss
import pickle
import ollama
import hashlib
from datetime import datetime, timedelta
import threading
import schedule

class OptimizedWikiScraper:
    def __init__(self, base_url="https://wiki.seeedstudio.com"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.visited_urls = set()
        self.all_content = []
        self.url_queue = deque()
        self.max_depth = 4  # å‡å°‘æ·±åº¦ï¼Œä¸“æ³¨äºä¸»è¦é¡µé¢
        
        # æ•°æ®æ–‡ä»¶è·¯å¾„
        self.data_dir = "./data_base"
        self.db_file = f"{self.data_dir}/seeed_wiki_embeddings_db.json"
        self.faiss_index_file = f"{self.data_dir}/faiss_index.bin"
        self.faiss_metadata_file = f"{self.data_dir}/faiss_metadata.pkl"
        self.url_hash_file = f"{self.data_dir}/url_hashes.json"
        self.last_update_file = f"{self.data_dir}/last_update.json"
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(self.data_dir, exist_ok=True)
        
        # åŠ è½½å·²å­˜åœ¨çš„æ•°æ®
        self.load_existing_data()
        
        # åˆå§‹åŒ– Ollama Embedding æ¨¡å‹
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– Ollama Embedding æ¨¡å‹...")
        try:
            # æ£€æŸ¥ Ollama æœåŠ¡
            self.check_ollama_service()
            
            # ä½¿ç”¨ nomic-embed-text æ¨¡å‹
            self.embedding_model = "nomic-embed-text"
            
            # æµ‹è¯•æ¨¡å‹
            test_embedding = self.generate_embedding("test")
            if test_embedding is not None:
                self.dimension = len(test_embedding)
                print(f"âœ… Ollama Embedding æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {self.dimension} ç»´")
                print(f"   æ¨¡å‹åç§°: {self.embedding_model}")
            else:
                raise Exception("æ¨¡å‹æµ‹è¯•å¤±è´¥")
            
        except Exception as e:
            print(f"âŒ Ollama Embedding æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            print("è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œå¹¶å·²å®‰è£… nomic-embed-text æ¨¡å‹")
            print("å®‰è£…å‘½ä»¤: ollama pull nomic-embed-text")
            raise
        
        # åˆå§‹åŒ– FAISS ç´¢å¼•
        if os.path.exists(self.faiss_index_file) and self.faiss_vectors:
            # åŠ è½½ç°æœ‰ç´¢å¼•
            self.faiss_index = faiss.read_index(self.faiss_index_file)
            print(f"âœ… å·²åŠ è½½ç°æœ‰ FAISS ç´¢å¼•: {self.faiss_index.ntotal} ä¸ªå‘é‡")
        else:
            # åˆ›å»ºæ–°ç´¢å¼•
            self.faiss_index = faiss.IndexFlatIP(self.dimension)
            print("âœ… åˆ›å»ºæ–°çš„ FAISS ç´¢å¼•")
        
        print("âœ… FAISS ç´¢å¼•åˆå§‹åŒ–å®Œæˆ")
    
    def load_existing_data(self):
        """åŠ è½½å·²å­˜åœ¨çš„æ•°æ®"""
        # åŠ è½½æ•°æ®åº“
        if os.path.exists(self.db_file):
            try:
                with open(self.db_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.all_content = data.get('pages', [])
                    print(f"ğŸ“‚ å·²åŠ è½½ {len(self.all_content)} ä¸ªç°æœ‰é¡µé¢")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ•°æ®åº“å¤±è´¥: {str(e)}")
                self.all_content = []
        
        # åŠ è½½å‘é‡å…ƒæ•°æ®
        if os.path.exists(self.faiss_metadata_file):
            try:
                with open(self.faiss_metadata_file, 'rb') as f:
                    self.faiss_metadata = pickle.load(f)
                    self.faiss_vectors = [None] * len(self.faiss_metadata)  # å ä½ç¬¦
                    print(f"ğŸ“Š å·²åŠ è½½ {len(self.faiss_metadata)} ä¸ªå‘é‡å…ƒæ•°æ®")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å‘é‡å…ƒæ•°æ®å¤±è´¥: {str(e)}")
                self.faiss_metadata = []
                self.faiss_vectors = []
        
        # åŠ è½½URLå“ˆå¸Œå€¼
        self.url_hashes = {}
        if os.path.exists(self.url_hash_file):
            try:
                with open(self.url_hash_file, 'r', encoding='utf-8') as f:
                    self.url_hashes = json.load(f)
                    print(f"ğŸ”— å·²åŠ è½½ {len(self.url_hashes)} ä¸ªURLå“ˆå¸Œå€¼")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½URLå“ˆå¸Œå€¼å¤±è´¥: {str(e)}")
                self.url_hashes = {}
    
    def save_url_hashes(self):
        """ä¿å­˜URLå“ˆå¸Œå€¼"""
        with open(self.url_hash_file, 'w', encoding='utf-8') as f:
            json.dump(self.url_hashes, f, ensure_ascii=False, indent=2)
    
    def update_last_update_time(self):
        """æ›´æ–°æœ€åæ›´æ–°æ—¶é—´"""
        update_info = {
            'last_update': datetime.now().isoformat(),
            'total_pages': len(self.all_content),
            'total_vectors': len(self.faiss_vectors)
        }
        with open(self.last_update_file, 'w', encoding='utf-8') as f:
            json.dump(update_info, f, ensure_ascii=False, indent=2)
    
    def should_update(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆ24å°æ—¶æ£€æŸ¥ä¸€æ¬¡ï¼‰"""
        if not os.path.exists(self.last_update_file):
            return True
        
        try:
            with open(self.last_update_file, 'r', encoding='utf-8') as f:
                last_update_info = json.load(f)
                last_update_str = last_update_info.get('last_update')
                if last_update_str:
                    last_update = datetime.fromisoformat(last_update_str)
                    time_diff = datetime.now() - last_update
                    return time_diff.total_seconds() >= 24 * 3600  # 24å°æ—¶
        except Exception as e:
            print(f"âš ï¸ æ£€æŸ¥æ›´æ–°æ—¶é—´å¤±è´¥: {str(e)}")
        
        return True
    
    def check_ollama_service(self):
        """æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€"""
        try:
            # å°è¯•è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
            models = ollama.list()
            print(f"âœ… Ollama æœåŠ¡æ­£å¸¸ï¼Œå¯ç”¨æ¨¡å‹: {len(models['models'])} ä¸ª")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ nomic-embed-text æ¨¡å‹
            model_names = [model['name'] for model in models['models']]
            if 'nomic-embed-text' not in model_names:
                print("âš ï¸  æœªæ‰¾åˆ° nomic-embed-text æ¨¡å‹ï¼Œæ­£åœ¨å®‰è£…...")
                ollama.pull('nomic-embed-text')
                print("âœ… nomic-embed-text æ¨¡å‹å®‰è£…å®Œæˆ")
            else:
                print("âœ… nomic-embed-text æ¨¡å‹å·²å®‰è£…")
                
        except Exception as e:
            print(f"âŒ Ollama æœåŠ¡æ£€æŸ¥å¤±è´¥: {str(e)}")
            raise
    
    def is_valid_wiki_url(self, url):
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ Wiki URL - æ”¯æŒä¸­è‹±æ–‡é¡µé¢"""
        if not url or 'wiki.seeedstudio.com' not in url:
            return False
        
        # æ’é™¤æ–‡ä»¶ç±»å‹
        exclude_patterns = [
            r'\.(pdf|doc|docx|xls|xlsx|zip|rar|jpg|jpeg|png|gif|svg|ico|css|js)$',
            r'#.*$', r'\?.*$', r'/api/', r'/admin/',
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        return True
    
    def get_page_hash(self, url, content):
        """è·å–é¡µé¢å†…å®¹çš„å“ˆå¸Œå€¼"""
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
        return content_hash
    
    def is_page_updated(self, url, content):
        """æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰æ›´æ–°"""
        current_hash = self.get_page_hash(url, content)
        old_hash = self.url_hashes.get(url)
        
        if old_hash != current_hash:
            self.url_hashes[url] = current_hash
            return True
        
        return False
    
    def normalize_url(self, url):
        """æ ‡å‡†åŒ– URL"""
        if not url.startswith('http'):
            url = urljoin(self.base_url, url)
        url = url.split('#')[0].split('?')[0]
        if not re.search(r'\.[a-zA-Z0-9]+$', url) and not url.endswith('/'):
            url += '/'
        return url
    
    def extract_links_from_page(self, soup, current_url):
        """ä»é¡µé¢ä¸­æå–æ‰€æœ‰æœ‰æ•ˆçš„é“¾æ¥"""
        links = set()
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href:
                full_url = self.normalize_url(href)
                if self.is_valid_wiki_url(full_url):
                    links.add(full_url)
        return links
    
    def extract_page_content(self, soup, url):
        """æå–é¡µé¢å†…å®¹ - æ”¯æŒä¸­è‹±æ–‡"""
        title = soup.find('title')
        title_text = title.get_text().strip() if title else "No Title"
        
        content = ""
        main_content = (
            soup.find('main') or 
            soup.find('article') or 
            soup.find('div', class_='content') or 
            soup.find('div', class_='main') or
            soup.find('div', class_='theme-doc-markdown') or
            soup.find('div', class_='markdown') or
            soup.find('div', {'role': 'main'})
        )
        
        if main_content:
            # ç§»é™¤è„šæœ¬ã€æ ·å¼ã€å¯¼èˆªç­‰å…ƒç´ 
            for element in main_content(["script", "style", "nav", "header", "footer"]):
                element.decompose()
            
            # è·å–å†…å®¹ï¼ˆä¸­è‹±æ–‡éƒ½è·å–ï¼‰
            paragraphs = main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            
            if paragraphs:
                intro_content = []
                char_count = 0
                max_chars = 800  # å¢åŠ å­—ç¬¦æ•°é™åˆ¶
                
                for p in paragraphs[:8]:  # æœ€å¤šå–å‰8ä¸ªæ®µè½
                    p_text = p.get_text().strip()
                    if p_text and len(p_text) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                        intro_content.append(p_text)
                        char_count += len(p_text)
                        
                        # å¦‚æœå·²ç»è¾¾åˆ°å­—ç¬¦é™åˆ¶ï¼Œåœæ­¢æ”¶é›†
                        if char_count >= max_chars:
                            break
                
                content = ' '.join(intro_content)
                
                # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œå°è¯•è·å–æ›´å¤šå†…å®¹
                if len(content) < 200:
                    full_text = main_content.get_text(separator=' ', strip=True)
                    content = full_text[:1000]
                    
                    # åœ¨å¥å·å¤„æˆªæ–­
                    if '.' in content:
                        last_period = content.rfind('.')
                        if last_period > 600:
                            content = content[:last_period + 1]
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ®µè½æ ‡ç­¾ï¼Œè·å–å‰1000å­—ç¬¦
                full_text = main_content.get_text(separator=' ', strip=True)
                content = full_text[:1000]
                
                # åœ¨å¥å·å¤„æˆªæ–­
                if '.' in content:
                    last_period = content.rfind('.')
                    if last_period > 600:
                        content = content[:last_period + 1]
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼Œè·å– body å†…å®¹çš„å‰éƒ¨åˆ†
            body = soup.find('body')
            if body:
                for element in body(["script", "style", "nav", "header", "footer", "aside"]):
                    element.decompose()
                
                full_text = body.get_text(separator=' ', strip=True)
                content = full_text[:800]  # é™åˆ¶ä¸º800å­—ç¬¦
                
                # åœ¨å¥å·å¤„æˆªæ–­
                if '.' in content:
                    last_period = content.rfind('.')
                    if last_period > 400:
                        content = content[:last_period + 1]
        
        # æ¸…ç†å†…å®¹
        content = re.sub(r'\s+', ' ', content).strip()
        
        # æ·»åŠ å†…å®¹æ‘˜è¦æ ‡è®°
        if content and len(content) > 0:
            # æ£€æµ‹è¯­è¨€
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
            english_chars = len(re.findall(r'[a-zA-Z]', content))
            
            if chinese_chars > english_chars:
                content = f"[ä¸­æ–‡ä»‹ç»] {content}"
            else:
                content = f"[English Introduction] {content}"
            
            # å¦‚æœå†…å®¹è¢«æˆªæ–­ï¼Œæ·»åŠ çœç•¥å·
            if len(content) > 900:
                content = content[:900] + "..."
        
        return title_text, content
    
    def generate_embedding(self, text):
        """ä½¿ç”¨ Ollama ç”Ÿæˆæ–‡æœ¬çš„ embedding å‘é‡"""
        try:
            # ä½¿ç”¨ Ollama nomic-embed-text æ¨¡å‹ç”Ÿæˆ embedding
            response = ollama.embeddings(model=self.embedding_model, prompt=text)
            embedding = response["embedding"]
            
            # è½¬æ¢ä¸º numpy æ•°ç»„
            embedding = np.array(embedding, dtype=np.float32)
            
            # å½’ä¸€åŒ–å‘é‡ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ï¼‰
            embedding = embedding / np.linalg.norm(embedding)
            
            return embedding
        except Exception as e:
            print(f"âŒ Embedding ç”Ÿæˆå¤±è´¥: {str(e)}")
            return None
    
    def scrape_page(self, url, depth=0):
        """çˆ¬å–å•ä¸ªé¡µé¢å¹¶ç”Ÿæˆ embedding"""
        if url in self.visited_urls or depth > self.max_depth:
            return set()
        
        print(f"æ­£åœ¨çˆ¬å– (æ·±åº¦ {depth}): {url}")
        self.visited_urls.add(url)
        
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            title, content = self.extract_page_content(soup, url)
            
            # å†…å®¹è¿‡æ»¤
            if content and len(content) > 50:
                # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰æ›´æ–°
                if not self.is_page_updated(url, content):
                    print(f"  â­ï¸ é¡µé¢æ— æ›´æ–°ï¼Œè·³è¿‡: {title}")
                    return set()
                
                # ç”Ÿæˆ embedding
                print(f"  ğŸ” ç”Ÿæˆ Embedding...")
                embedding = self.generate_embedding(content)
                
                if embedding is not None:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥é¡µé¢
                    existing_page = None
                    for i, page in enumerate(self.all_content):
                        if page['url'] == url:
                            existing_page = i
                            break
                    
                    # ä¿å­˜é¡µé¢æ•°æ®
                    page_data = {
                        'url': url,
                        'title': title,
                        'content': content,
                        'depth': depth,
                        'content_length': len(content),
                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                        'language': 'ä¸­æ–‡' if '[ä¸­æ–‡ä»‹ç»]' in content else 'English'
                    }
                    
                    if existing_page is not None:
                        # æ›´æ–°ç°æœ‰é¡µé¢
                        self.all_content[existing_page] = page_data
                        print(f"  ğŸ”„ å·²æ›´æ–°: {title}")
                    else:
                        # æ·»åŠ æ–°é¡µé¢
                        self.all_content.append(page_data)
                        print(f"  âœ… å·²ä¿å­˜: {title}")
                    
                    # æ›´æ–°å‘é‡æ•°æ®
                    if existing_page is not None:
                        # æ›´æ–°ç°æœ‰å‘é‡
                        self.faiss_vectors[existing_page] = embedding
                        self.faiss_metadata[existing_page] = {
                            'title': title,
                            'url': url,
                            'content_length': len(content),
                            'timestamp': page_data['timestamp'],
                            'language': page_data['language']
                        }
                    else:
                        # æ·»åŠ æ–°å‘é‡
                        self.faiss_vectors.append(embedding)
                        self.faiss_metadata.append({
                            'title': title,
                            'url': url,
                            'content_length': len(content),
                            'timestamp': page_data['timestamp'],
                            'language': page_data['language']
                        })
                    
                    print(f"    å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    print(f"    Embedding ç»´åº¦: {embedding.shape}")
                    print(f"    è¯­è¨€: {page_data['language']}")
                else:
                    print(f"  âš  Embedding ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡: {title}")
            else:
                print(f"  âš  å†…å®¹è¿‡çŸ­ ({len(content) if content else 0} å­—ç¬¦)ï¼Œè·³è¿‡: {title}")
            
            # æå–é¡µé¢ä¸­çš„é“¾æ¥
            new_links = self.extract_links_from_page(soup, url)
            for link in new_links:
                if link not in self.visited_urls:
                    self.url_queue.append((link, depth + 1))
            
            return new_links
            
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {str(e)}")
            return set()
    
    def discover_initial_links(self):
        """å‘ç°åˆå§‹é“¾æ¥é›†åˆ"""
        print("æ­£åœ¨å‘ç°åˆå§‹é“¾æ¥...")
        
        initial_urls = [
            self.base_url,
            f"{self.base_url}/Getting_Started/",
            f"{self.base_url}/sitemap.xml",
        ]
        
        discovered_links = set()
        
        for url in initial_urls:
            try:
                response = self.session.get(url, timeout=15)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    links = self.extract_links_from_page(soup, url)
                    discovered_links.update(links)
                    print(f"ä» {url} å‘ç° {len(links)} ä¸ªé“¾æ¥")
            except Exception as e:
                print(f"æ— æ³•è®¿é—® {url}: {str(e)}")
        
        # æ·»åŠ ä¸­è‹±æ–‡ç›®å½•
        known_directories = [
            # è‹±æ–‡ç›®å½•
            "/SensorandSensing/",
            "/Networking/", 
            "/EdgeComputing/",
            "/Cloud/",
            "/TechnologyTopics/",
            "/Contributions/",
            "/WeeklyWiki/",
            "/XIAO/",
            "/Grove/",
            "/SenseCAP/",
            "/reComputer/",
            "/WioTerminal/",
            "/Odyssey/",
            "/RaspberryPi/",
            "/NVIDIAJetson/",
            "/ESPDevices/",
            "/BeagleBone/",
            "/Arduino/",
            "/Microbit/",
            # ä¸­æ–‡ç›®å½•
            "/zh/",
            "/zh-cn/",
            "/zh/SensorandSensing/",
            "/zh/Networking/",
            "/zh/EdgeComputing/",
            "/zh/Cloud/",
            "/zh/TechnologyTopics/",
            "/zh/Contributions/",
            "/zh/WeeklyWiki/",
            "/zh/XIAO/",
            "/zh/Grove/",
            "/zh/SenseCAP/",
            "/zh/reComputer/",
            "/zh/WioTerminal/",
            "/zh/Odyssey/",
            "/zh/RaspberryPi/",
            "/zh/NVIDIAJetson/",
            "/zh/ESPDevices/",
            "/zh/BeagleBone/",
            "/zh/Arduino/",
            "/zh/Microbit/"
        ]
        
        for directory in known_directories:
            full_url = urljoin(self.base_url, directory)
            if self.is_valid_wiki_url(full_url):
                discovered_links.add(full_url)
        
        print(f"æ€»å…±å‘ç° {len(discovered_links)} ä¸ªåˆå§‹é“¾æ¥")
        return discovered_links
    
    def build_faiss_index(self):
        """æ„å»º FAISS ç´¢å¼•"""
        if not self.faiss_vectors:
            print("âš ï¸ æ²¡æœ‰å‘é‡æ•°æ®ï¼Œè·³è¿‡ç´¢å¼•æ„å»º")
            return False
        
        print(f"\nğŸ”§ æ„å»º FAISS ç´¢å¼•...")
        print(f"   å‘é‡æ•°é‡: {len(self.faiss_vectors)}")
        print(f"   å‘é‡ç»´åº¦: {self.dimension}")
        
        try:
            # è¿‡æ»¤æ‰Noneå€¼
            valid_vectors = [v for v in self.faiss_vectors if v is not None]
            if not valid_vectors:
                print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„å‘é‡æ•°æ®")
                return False
            
            # å°†å‘é‡è½¬æ¢ä¸º numpy æ•°ç»„
            vectors_array = np.array(valid_vectors, dtype=np.float32)
            
            # åˆ›å»ºæ–°ç´¢å¼•
            self.faiss_index = faiss.IndexFlatIP(self.dimension)
            self.faiss_index.add(vectors_array)
            
            print(f"âœ… FAISS ç´¢å¼•æ„å»ºå®Œæˆ")
            print(f"   ç´¢å¼•å¤§å°: {self.faiss_index.ntotal}")
            
            return True
            
        except Exception as e:
            print(f"âŒ FAISS ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}")
            return False
    
    def save_embeddings_and_index(self):
        """ä¿å­˜ embedding å‘é‡å’Œ FAISS ç´¢å¼•"""
        print(f"\nğŸ’¾ ä¿å­˜ Embedding å’Œç´¢å¼•...")
        
        # ä¿å­˜é¡µé¢æ•°æ®
        output_file = f"{self.data_dir}/seeed_wiki_embeddings.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.all_content, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ é¡µé¢æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        
        # ä¿å­˜ FAISS ç´¢å¼•
        faiss.write_index(self.faiss_index, self.faiss_index_file)
        print(f"ğŸ” FAISS ç´¢å¼•å·²ä¿å­˜åˆ°: {self.faiss_index_file}")
        
        # ä¿å­˜å‘é‡å…ƒæ•°æ®
        with open(self.faiss_metadata_file, 'wb') as f:
            pickle.dump(self.faiss_metadata, f)
        print(f"ğŸ“Š å‘é‡å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {self.faiss_metadata_file}")
        
        # ä¿å­˜URLå“ˆå¸Œå€¼
        self.save_url_hashes()
        print(f"ğŸ”— URLå“ˆå¸Œå€¼å·²ä¿å­˜åˆ°: {self.url_hash_file}")
        
        # æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
        self.update_last_update_time()
        print(f"â° æœ€åæ›´æ–°æ—¶é—´å·²æ›´æ–°")
        
        # ä¿å­˜ä¸ºæ•°æ®åº“æ ¼å¼
        db_data = {
            'metadata': {
                'total_pages': len(self.all_content),
                'total_vectors': len([v for v in self.faiss_vectors if v is not None]),
                'vector_dimension': self.dimension,
                'crawl_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'base_url': self.base_url,
                'max_depth': self.max_depth,
                'content_type': 'ä¸­è‹±æ–‡é¡µé¢ä»‹ç»æ‘˜è¦',
                'embedding_model': 'nomic-embed-text',
                'index_type': 'FAISS_IndexFlatIP',
                'languages': ['ä¸­æ–‡', 'English'],
                'last_update': datetime.now().isoformat()
            },
            'pages': self.all_content
        }
        
        with open(self.db_file, 'w', encoding='utf-8') as f:
            json.dump(db_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ—„ï¸ æ•°æ®åº“æ ¼å¼å·²ä¿å­˜åˆ°: {self.db_file}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_chars = sum(page.get('content_length', 0) for page in self.all_content)
        avg_chars = total_chars / len(self.all_content) if self.all_content else 0
        
        # è¯­è¨€ç»Ÿè®¡
        chinese_pages = len([p for p in self.all_content if p.get('language') == 'ä¸­æ–‡'])
        english_pages = len([p for p in self.all_content if p.get('language') == 'English'])
        
        print(f"\nğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        print(f"   æ€»é¡µé¢æ•°: {len(self.all_content)}")
        print(f"   ä¸­æ–‡é¡µé¢: {chinese_pages}")
        print(f"   è‹±æ–‡é¡µé¢: {english_pages}")
        print(f"   æ€»å‘é‡æ•°: {len([v for v in self.faiss_vectors if v is not None])}")
        print(f"   æ€»å­—ç¬¦æ•°: {total_chars:,}")
        print(f"   å¹³å‡å­—ç¬¦æ•°: {avg_chars:.1f}")
        print(f"   å‘é‡ç»´åº¦: {self.dimension}")
        
        print(f"\nâœ… æ‰€æœ‰æ•°æ®å·²ä¿å­˜å®Œæˆï¼")
        print(f"ğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨ä¼˜åŒ–ç‰ˆé—®ç­”ç³»ç»Ÿäº†")
        print(f"ğŸš€ å¯åŠ¨å‘½ä»¤: python optimized_qa.py")
    
    def run_incremental_update(self):
        """è¿è¡Œå¢é‡æ›´æ–°"""
        print("ğŸ”„ å¼€å§‹å¢é‡æ›´æ–° Seeed Studio Wiki")
        print(f"åŸºç¡€ URL: {self.base_url}")
        print(f"æœ€å¤§æ·±åº¦: {self.max_depth}")
        print(f"Embedding æ¨¡å‹: {self.embedding_model}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        if not self.should_update():
            print("â° è·ç¦»ä¸Šæ¬¡æ›´æ–°æœªæ»¡24å°æ—¶ï¼Œè·³è¿‡æ›´æ–°")
            return
        
        # å‘ç°åˆå§‹é“¾æ¥
        initial_links = self.discover_initial_links()
        
        # å°†åˆå§‹é“¾æ¥æ·»åŠ åˆ°é˜Ÿåˆ—
        for link in initial_links:
            self.url_queue.append((link, 0))
        
        # å¼€å§‹çˆ¬å–
        processed_count = 0
        new_pages_count = 0
        updated_pages_count = 0
        
        while self.url_queue:
            url, depth = self.url_queue.popleft()
            
            if url in self.visited_urls:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°é¡µé¢æˆ–éœ€è¦æ›´æ–°
            existing_page = None
            for page in self.all_content:
                if page['url'] == url:
                    existing_page = page
                    break
            
            if existing_page:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                try:
                    response = self.session.head(url, timeout=10)
                    if response.status_code == 200:
                        # è·å–é¡µé¢å†…å®¹æ£€æŸ¥æ˜¯å¦æœ‰æ›´æ–°
                        full_response = self.session.get(url, timeout=15)
                        soup = BeautifulSoup(full_response.content, 'html.parser')
                        title, content = self.extract_page_content(soup, url)
                        
                        if self.is_page_updated(url, content):
                            # é¡µé¢æœ‰æ›´æ–°ï¼Œé‡æ–°çˆ¬å–
                            self.scrape_page(url, depth)
                            updated_pages_count += 1
                        else:
                            print(f"â­ï¸ é¡µé¢æ— æ›´æ–°ï¼Œè·³è¿‡: {existing_page['title']}")
                            self.visited_urls.add(url)
                except Exception as e:
                    print(f"âš ï¸ æ£€æŸ¥é¡µé¢æ›´æ–°å¤±è´¥: {str(e)}")
                    self.visited_urls.add(url)
            else:
                # æ–°é¡µé¢ï¼Œçˆ¬å–
                self.scrape_page(url, depth)
                new_pages_count += 1
            
            processed_count += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            if processed_count % 10 == 0:
                print(f"\nğŸ“Š è¿›åº¦: å·²å¤„ç† {processed_count} ä¸ªé¡µé¢ï¼Œé˜Ÿåˆ—ä¸­è¿˜æœ‰ {len(self.url_queue)} ä¸ª")
                print(f"ğŸ“ æ–°é¡µé¢: {new_pages_count}ï¼Œæ›´æ–°é¡µé¢: {updated_pages_count}")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)
        
        # æ„å»º FAISS ç´¢å¼•
        if self.build_faiss_index():
            # ä¿å­˜æ‰€æœ‰æ•°æ®
            self.save_embeddings_and_index()
        
        print(f"\nğŸ‰ å¢é‡æ›´æ–°å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - æ€»å…±è®¿é—®: {len(self.visited_urls)} ä¸ªé¡µé¢")
        print(f"  - æ–°é¡µé¢: {new_pages_count} ä¸ª")
        print(f"  - æ›´æ–°é¡µé¢: {updated_pages_count} ä¸ª")
        print(f"  - æ€»é¡µé¢æ•°: {len(self.all_content)} ä¸ª")
    
    def run_full_crawl(self):
        """è¿è¡Œå®Œæ•´çˆ¬å–"""
        print("ğŸš€ å¼€å§‹å®Œæ•´çˆ¬å– Seeed Studio Wiki (ä¸­è‹±æ–‡é¡µé¢ä»‹ç» + Embedding)")
        print(f"åŸºç¡€ URL: {self.base_url}")
        print(f"æœ€å¤§æ·±åº¦: {self.max_depth}")
        print(f"Embedding æ¨¡å‹: {self.embedding_model}")
        print(f"å‘é‡ç´¢å¼•: FAISS ç´¢å¼•")
        
        # æ¸…ç©ºç°æœ‰æ•°æ®
        self.all_content = []
        self.faiss_vectors = []
        self.faiss_metadata = []
        self.url_hashes = {}
        self.visited_urls = set()
        
        # å‘ç°åˆå§‹é“¾æ¥
        initial_links = self.discover_initial_links()
        
        # å°†åˆå§‹é“¾æ¥æ·»åŠ åˆ°é˜Ÿåˆ—
        for link in initial_links:
            self.url_queue.append((link, 0))
        
        # å¼€å§‹çˆ¬å–
        processed_count = 0
        while self.url_queue:
            url, depth = self.url_queue.popleft()
            
            if url in self.visited_urls:
                continue
            
            # çˆ¬å–é¡µé¢
            self.scrape_page(url, depth)
            processed_count += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            if processed_count % 10 == 0:
                print(f"\nğŸ“Š è¿›åº¦: å·²å¤„ç† {processed_count} ä¸ªé¡µé¢ï¼Œé˜Ÿåˆ—ä¸­è¿˜æœ‰ {len(self.url_queue)} ä¸ª")
                print(f"ğŸ“ å·²ä¿å­˜ {len(self.all_content)} ä¸ªé¡µé¢")
                print(f"ğŸ” å·²ç”Ÿæˆ {len(self.faiss_vectors)} ä¸ªå‘é‡")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)
        
        # æ„å»º FAISS ç´¢å¼•
        if self.build_faiss_index():
            # ä¿å­˜æ‰€æœ‰æ•°æ®
            self.save_embeddings_and_index()
        
        print(f"\nğŸ‰ å®Œæ•´çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - æ€»å…±è®¿é—®: {len(self.visited_urls)} ä¸ªé¡µé¢")
        print(f"  - æˆåŠŸä¿å­˜: {len(self.all_content)} ä¸ªé¡µé¢")
        print(f"  - ç”Ÿæˆå‘é‡: {len(self.faiss_vectors)} ä¸ª")
        print(f"  - æœ€å¤§æ·±åº¦: {max([page['depth'] for page in self.all_content]) if self.all_content else 0}")
        
        return self.all_content
    
    def run_continuous_monitor(self):
        """æŒç»­è¿è¡Œç›‘æ§æ¨¡å¼ - æ£€æŸ¥æ–°é¡µé¢å¹¶å®šæ—¶æ›´æ–°"""
        print("ğŸ”„ å¯åŠ¨æŒç»­ç›‘æ§æ¨¡å¼")
        print("ğŸ“Š åŠŸèƒ½è¯´æ˜:")
        print("   - å®æ—¶æ£€æŸ¥æ–°é¡µé¢å¹¶æ›´æ–°åˆ°æœ¬åœ°")
        print("   - æ¯å¤©å‡Œæ™¨12ç‚¹è‡ªåŠ¨è¿›è¡Œå®Œæ•´æ•°æ®åº“æ›´æ–°")
        print("   - æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
        
        def daily_update_job():
            print(f"\nâ° æ‰§è¡Œæ¯æ—¥å®šæ—¶æ›´æ–°ä»»åŠ¡ - {datetime.now()}")
            try:
                print("ğŸ”„ å¼€å§‹æ¯æ—¥å®Œæ•´æ›´æ–°...")
                self.run_incremental_update()
                print("âœ… æ¯æ—¥æ›´æ–°å®Œæˆ")
            except Exception as e:
                print(f"âŒ æ¯æ—¥æ›´æ–°å¤±è´¥: {str(e)}")
        
        def continuous_check_job():
            """æŒç»­æ£€æŸ¥æ–°é¡µé¢çš„ä»»åŠ¡"""
            try:
                print(f"\nğŸ” æ‰§è¡ŒæŒç»­æ£€æŸ¥ä»»åŠ¡ - {datetime.now()}")
                self.run_quick_check()
            except Exception as e:
                print(f"âŒ æŒç»­æ£€æŸ¥å¤±è´¥: {str(e)}")
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every().day.at("00:00").do(daily_update_job)  # æ¯å¤©å‡Œæ™¨12ç‚¹
        schedule.every(30).minutes.do(continuous_check_job)    # æ¯30åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        
        print("â° å®šæ—¶ä»»åŠ¡è®¾ç½®:")
        print("   - æ¯æ—¥å‡Œæ™¨ 00:00: å®Œæ•´æ•°æ®åº“æ›´æ–°")
        print("   - æ¯ 30 åˆ†é’Ÿ: å¿«é€Ÿæ£€æŸ¥æ–°é¡µé¢")
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡å®šæ—¶ä»»åŠ¡
        except KeyboardInterrupt:
            print("\nâ¹ï¸ æŒç»­ç›‘æ§å·²åœæ­¢")
    
    def run_quick_check(self):
        """å¿«é€Ÿæ£€æŸ¥æ–°é¡µé¢ï¼ˆä¸è¿›è¡Œæ·±åº¦çˆ¬å–ï¼‰"""
        print("ğŸ” å¿«é€Ÿæ£€æŸ¥æ–°é¡µé¢...")
        
        # è·å–ä¸»é¡µé¢çš„é“¾æ¥
        try:
            response = self.session.get(self.base_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                new_links = self.extract_links_from_page(soup, self.base_url)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°é¡µé¢
                new_pages_found = 0
                for link in new_links:
                    if link not in self.url_hashes:
                        # å‘ç°æ–°é¡µé¢ï¼Œç«‹å³çˆ¬å–
                        print(f"ğŸ†• å‘ç°æ–°é¡µé¢: {link}")
                        try:
                            self.scrape_page(link, 0)
                            new_pages_found += 1
                        except Exception as e:
                            print(f"âš ï¸ çˆ¬å–æ–°é¡µé¢å¤±è´¥: {str(e)}")
                
                if new_pages_found > 0:
                    print(f"âœ… å¿«é€Ÿæ£€æŸ¥å®Œæˆï¼Œå‘ç°å¹¶çˆ¬å–äº† {new_pages_found} ä¸ªæ–°é¡µé¢")
                    # ä¿å­˜æ›´æ–°
                    self.save_embeddings_and_index()
                else:
                    print("âœ… å¿«é€Ÿæ£€æŸ¥å®Œæˆï¼Œæ²¡æœ‰å‘ç°æ–°é¡µé¢")
            else:
                print(f"âš ï¸ æ— æ³•è®¿é—®ä¸»é¡µ: {response.status_code}")
        except Exception as e:
            print(f"âŒ å¿«é€Ÿæ£€æŸ¥å¤±è´¥: {str(e)}")
    
    def schedule_daily_update(self):
        """è®¾ç½®æ¯æ—¥å®šæ—¶æ›´æ–°ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
        def daily_update_job():
            print(f"\nâ° æ‰§è¡Œå®šæ—¶æ›´æ–°ä»»åŠ¡ - {datetime.now()}")
            try:
                self.run_incremental_update()
            except Exception as e:
                print(f"âŒ å®šæ—¶æ›´æ–°å¤±è´¥: {str(e)}")
        
        # æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œæ›´æ–°
        schedule.every().day.at("02:00").do(daily_update_job)
        
        print("â° å·²è®¾ç½®æ¯æ—¥å‡Œæ™¨2ç‚¹è‡ªåŠ¨æ›´æ–°")
        print("ğŸ”„ å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢")
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        except KeyboardInterrupt:
            print("\nâ¹ï¸ å®šæ—¶ä»»åŠ¡å·²åœæ­¢")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Seeed Wiki çˆ¬è™«')
    parser.add_argument('--mode', choices=['full', 'incremental', 'schedule', 'monitor'], 
                       default='incremental', help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶å®Œæ•´çˆ¬å–')
    parser.add_argument('--check-interval', type=int, default=30, 
                       help='ç›‘æ§æ¨¡å¼ä¸‹çš„æ£€æŸ¥é—´éš”ï¼ˆåˆ†é’Ÿï¼‰')
    
    args = parser.parse_args()
    
    scraper = OptimizedWikiScraper()
    
    try:
        if args.mode == 'full' or args.force:
            scraper.run_full_crawl()
        elif args.mode == 'schedule':
            scraper.schedule_daily_update()
        elif args.mode == 'monitor':
            scraper.run_continuous_monitor()
        else:
            scraper.run_incremental_update()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        if scraper.all_content:
            scraper.save_embeddings_and_index()
    except Exception as e:
        print(f"\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        if scraper.all_content:
            scraper.save_embeddings_and_index()

if __name__ == "__main__":
    main()
